{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 23: Advanced Retrieval Implementation\n",
    "\n",
    "In this notebook, we'll build upon our basic RAG system from Day 22 by implementing advanced retrieval techniques to improve the quality of the context provided to the LLM. \n",
    "\n",
    "## Overview\n",
    "\n",
    "We will cover:\n",
    "1.  **Setup**: Prepare our data and baseline retrieval system.\n",
    "2.  **Evaluating Retrieval**: Implement and calculate metrics like Recall@k and Mean Reciprocal Rank (MRR).\n",
    "3.  **Reranking**: Use a more powerful Cross-Encoder model to rerank initial search results for higher precision.\n",
    "4.  **Hybrid Search**: Combine traditional keyword search (BM25) with semantic search for more robust retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's install the necessary libraries and set up our initial data and retrieval system from Day 22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers faiss-cpu rank_bm25 numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# --- Data and Baseline System from Day 22 ---\n",
    "document = (\n",
    "    \"The planet Zoltar is a marvel of the Andromeda galaxy, known for its twin suns, Helios Prime and Helios Beta, which create a perpetual twilight. \"\n",
    "    \"The surface of Zoltar is covered in crystalline forests that hum with a low-frequency energy. This energy is harnessed by the native Zoltarians, a species of sentient, silicon-based lifeforms. \"\n",
    "    \"Zoltarians communicate through a complex series of light patterns, a language known as 'Luminar'. Their society is structured around the 'Great Crystal', a massive geological formation at the planet's north pole that is believed to be the source of all life. \"\n",
    "    \"The Zoltarian diet consists of absorbing geothermal energy from volcanic vents scattered across the planet. They reproduce asexually, budding off smaller versions of themselves once every 'Great Cycle', which corresponds to 50 Earth years. \"\n",
    "    \"Zoltar's atmosphere is composed mainly of nitrogen and argon, and is unbreathable for carbon-based lifeforms. The planet has a strong magnetic field, which protects it from the intense solar winds of its twin suns. The average temperature on Zoltar is a cool -30 degrees Celsius.\"\n",
    ")\n",
    "\n",
    "def chunk_text(text, chunk_size=30, overlap=5):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "text_chunks = chunk_text(document)\n",
    "bi_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "chunk_embeddings = bi_encoder.encode(text_chunks, convert_to_tensor=False)\n",
    "\n",
    "d = chunk_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(chunk_embeddings.astype('float32'))\n",
    "\n",
    "print(f\"Created {len(text_chunks)} chunks and indexed them in FAISS.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluating Retrieval\n",
    "\n",
    "To measure our improvements, we first need an evaluation set and functions for our metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small evaluation dataset (query, index_of_correct_chunk)\n",
    "eval_dataset = [\n",
    "    {\"query\": \"How do Zoltarians talk to each other?\", \"relevant_chunk_idx\": 2},\n",
    "    {\"query\": \"What is the Zoltarian diet?\", \"relevant_chunk_idx\": 3},\n",
    "    {\"query\": \"What is the weather like on Zoltar?\", \"relevant_chunk_idx\": 4},\n",
    "    {\"query\": \"What are the suns of Zoltar called?\", \"relevant_chunk_idx\": 0}\n",
    "]\n",
    "\n",
    "def evaluate_retrieval(retriever_fn, eval_dataset, k=5):\n",
    "    reciprocal_ranks = []\n",
    "    recall_at_k = 0\n",
    "    \n",
    "    for item in eval_dataset:\n",
    "        query = item[\"query\"]\n",
    "        true_idx = item[\"relevant_chunk_idx\"]\n",
    "        \n",
    "        retrieved_indices = retriever_fn(query, k=k)\n",
    "        \n",
    "        # Check for recall\n",
    "        if true_idx in retrieved_indices:\n",
    "            recall_at_k += 1\n",
    "            # Find rank for MRR\n",
    "            rank = retrieved_indices.index(true_idx) + 1\n",
    "            reciprocal_ranks.append(1 / rank)\n",
    "        else:\n",
    "            reciprocal_ranks.append(0)\n",
    "            \n",
    "    mrr = np.mean(reciprocal_ranks)\n",
    "    recall = recall_at_k / len(eval_dataset)\n",
    "    return {\"mrr\": mrr, f\"recall@{k}\": recall}\n",
    "\n",
    "# --- Baseline Retriever Function ---\n",
    "def baseline_retriever(query, k=5):\n",
    "    query_embedding = bi_encoder.encode([query])\n",
    "    _, indices = index.search(query_embedding.astype('float32'), k)\n",
    "    return indices[0].tolist()\n",
    "\n",
    "# Evaluate the baseline system\n",
    "baseline_metrics = evaluate_retrieval(baseline_retriever, eval_dataset)\n",
    "print(f\"Baseline Metrics: {baseline_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reranking with a Cross-Encoder\n",
    "\n",
    "Now, let's add a second stage to our retrieval. We'll use a fast bi-encoder to get initial candidates and a powerful cross-encoder to rerank them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a cross-encoder model\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "def reranked_retriever(query, k=5, candidates_k=10):\n",
    "    # 1. Get initial candidates from the fast retriever\n",
    "    candidate_indices = baseline_retriever(query, k=candidates_k)\n",
    "    \n",
    "    # 2. Create pairs of (query, chunk) for the cross-encoder\n",
    "    pairs = [(query, text_chunks[i]) for i in candidate_indices]\n",
    "    \n",
    "    # 3. Score the pairs with the cross-encoder\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # 4. Combine indices and scores, then sort\n",
    "    scored_indices = list(zip(candidate_indices, scores))\n",
    "    scored_indices.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # 5. Return the top k reranked indices\n",
    "    reranked_indices = [idx for idx, score in scored_indices[:k]]\n",
    "    return reranked_indices\n",
    "\n",
    "# Evaluate the reranked system\n",
    "reranked_metrics = evaluate_retrieval(reranked_retriever, eval_dataset)\n",
    "print(f\"Reranked Metrics: {reranked_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hybrid Search (BM25 + Dense)\n",
    "\n",
    "Let's implement hybrid search by combining our dense vector search with a traditional keyword search algorithm, BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set up the BM25 index\n",
    "tokenized_chunks = [chunk.split(\" \") for chunk in text_chunks]\n",
    "bm25 = BM25Okapi(tokenized_chunks)\n",
    "\n",
    "def hybrid_retriever(query, k=5):\n",
    "    # Get BM25 (keyword) results\n",
    "    tokenized_query = query.split(\" \")\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    bm25_top_indices = np.argsort(bm25_scores)[::-1][:k]\n",
    "    \n",
    "    # Get dense (semantic) results\n",
    "    dense_top_indices = baseline_retriever(query, k=k)\n",
    "    \n",
    "    # 2. Fuse the results using Reciprocal Rank Fusion (RRF)\n",
    "    # RRF is simple and effective: the score of a document is the sum of its reciprocal ranks.\n",
    "    rrf_scores = {i: 0 for i in range(len(text_chunks))}\n",
    "    \n",
    "    for rank, idx in enumerate(bm25_top_indices):\n",
    "        rrf_scores[idx] += 1 / (rank + 1) # +1 to avoid division by zero\n",
    "    for rank, idx in enumerate(dense_top_indices):\n",
    "        rrf_scores[idx] += 1 / (rank + 1)\n",
    "        \n",
    "    # 3. Sort documents by their fused RRF score\n",
    "    sorted_docs = sorted(rrf_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    hybrid_indices = [doc[0] for doc in sorted_docs[:k]]\n",
    "    return hybrid_indices\n",
    "\n",
    "# Evaluate the hybrid system\n",
    "hybrid_metrics = evaluate_retrieval(hybrid_retriever, eval_dataset)\n",
    "print(f\"Hybrid Search Metrics: {hybrid_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison and Conclusion\n",
    "\n",
    "Let's compare the performance of our three retrieval strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for comparison\n",
    "metrics_df = pd.DataFrame([baseline_metrics, reranked_metrics, hybrid_metrics], \n",
    "                          index=['Baseline', 'Reranked', 'Hybrid'])\n",
    "\n",
    "print(\"Comparison of Retrieval Metrics:\")\n",
    "display(metrics_df)\n",
    "\n",
    "# Plot the results\n",
    "metrics_df.plot(kind='bar', figsize=(10, 6), rot=0)\n",
    "plt.title('Retrieval Performance Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAs we can see, both reranking and hybrid search can significantly improve retrieval performance over a simple baseline. \"\n",
    "      \"Reranking excels at precision (improving MRR), while hybrid search improves robustness (improving recall). \"\n",
    "      \"In a production system, you might even combine them: use hybrid search for initial retrieval and then a reranker on top.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}