{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 24: RAG Optimization Implementation\n",
    "\n",
    "In this notebook, we'll implement several optimization techniques to improve the performance, factuality, and traceability of our RAG system.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We will cover:\n",
    "1.  **Setup**: Re-establish our baseline RAG system.\n",
    "2.  **Advanced Prompting**: Crafting better instructions for the LLM.\n",
    "3.  **Context Re-ordering**: Mitigating the \"lost in the middle\" problem.\n",
    "4.  **Relevance Filtering**: Removing irrelevant \"noise\" from the context.\n",
    "5.  **Adding Citations**: Building a system that can cite its sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's install libraries and set up our data. We'll use a slightly more complex dataset this time, split into multiple documents to make citations meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers faiss-cpu openai python-dotenv rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import faiss\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# --- 1. Data Preparation with Metadata ---\n",
    "documents = [\n",
    "    {\"source\": \"Zoltar_Geology.txt\", \"content\": \"The planet Zoltar is known for its crystalline forests. The 'Great Crystal' at the north pole is a massive geological formation believed to be the source of all life.\"},\n",
    "    {\"source\": \"Zoltarian_Biology.txt\", \"content\": \"Zoltarians are sentient, silicon-based lifeforms. They communicate using light patterns called 'Luminar'. They reproduce asexually by budding once every 50 Earth years.\"},\n",
    "    {\"source\": \"Zoltarian_Culture.txt\", \"content\": \"Zoltarian society is structured around the Great Crystal. Their diet consists of absorbing geothermal energy from volcanic vents.\"},\n",
    "    {\"source\": \"Zoltar_Astronomy.pdf\", \"content\": \"Zoltar has two suns, Helios Prime and Helios Beta, creating a perpetual twilight. Its atmosphere is mainly nitrogen and argon.\"}\n",
    "]\n",
    "\n",
    "# Store chunks and their metadata separately\n",
    "text_chunks = [doc[\"content\"] for doc in documents]\n",
    "metadata = [doc[\"source\"] for doc in documents]\n",
    "\n",
    "# --- 2. Indexing ---\n",
    "bi_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "chunk_embeddings = bi_encoder.encode(text_chunks, convert_to_tensor=False)\n",
    "index = faiss.IndexFlatL2(chunk_embeddings.shape[1])\n",
    "index.add(chunk_embeddings.astype('float32'))\n",
    "\n",
    "print(f\"Indexed {len(text_chunks)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Prompting\n",
    "\n",
    "Let's define a more robust prompt that instructs the model to answer only from the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_prompt(query, context):\n",
    "    return (\n",
    "        f\"You are a helpful assistant. Answer the user's question based ONLY on the provided context.\\n\"\n",
    "        f\"If the answer is not found in the context, you MUST say 'I'm sorry, the context does not contain the answer.'\\n\"\n",
    "        f\"Do not use any external knowledge.\\n\\n\"\n",
    "        f\"Context:\\n---\\n{context}\\n---\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        f\"Answer:\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Context Re-ordering for Long Contexts\n",
    "\n",
    "This function re-orders documents to place the most relevant ones at the beginning and end of the context, combating the \"lost in the middle\" problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_documents(doc_indices):\n",
    "    \"\"\"Re-orders docs to place important ones at the beginning and end.\"\"\"\n",
    "    reordered = []\n",
    "    while doc_indices:\n",
    "        # Add the most relevant doc\n",
    "        reordered.append(doc_indices.pop(0))\n",
    "        # If there are more docs, add the next most relevant to the end\n",
    "        if doc_indices:\n",
    "            reordered.insert(0, doc_indices.pop(0))\n",
    "    return reordered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Relevance Filtering & Citations\n",
    "\n",
    "Now, let's build a final, optimized RAG pipeline that incorporates reranking, relevance filtering, context re-ordering, and citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_rag_pipeline(query, k=4):\n",
    "    # 1. Initial Retrieval (Bi-Encoder)\n",
    "    query_embedding = bi_encoder.encode([query])\n",
    "    _, initial_indices = index.search(query_embedding.astype('float32'), k * 2) # Retrieve more for reranking\n",
    "    initial_indices = initial_indices[0].tolist()\n",
    "\n",
    "    # 2. Reranking (Cross-Encoder)\n",
    "    pairs = [(query, text_chunks[i]) for i in initial_indices]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    scored_indices = sorted(zip(initial_indices, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 3. Relevance Filtering\n",
    "    relevance_threshold = 0.1 # Example threshold\n",
    "    filtered_indices = [idx for idx, score in scored_indices if score > relevance_threshold]\n",
    "\n",
    "    # 4. Context Re-ordering\n",
    "    final_indices = reorder_documents(filtered_indices[:k])\n",
    "\n",
    "    # 5. Build Context with Citations\n",
    "    context_parts = []\n",
    "    for i, idx in enumerate(final_indices):\n",
    "        source = metadata[idx]\n",
    "        content = text_chunks[idx]\n",
    "        context_parts.append(f\"Source [{i+1}]: {source}\\nContent: {content}\")\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "\n",
    "    # 6. Advanced Prompting\n",
    "    prompt = (\n",
    "        f\"You are an expert assistant. Answer the user's question based ONLY on the provided sources. \\n\"\n",
    "        f\"Cite the source number (e.g., [Source 1]) for each piece of information you use.\\n\\n\"\n",
    "        f\"Sources:\\n{context}\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "\n",
    "    # 7. Generation\n",
    "    print(\"--- Final Prompt to LLM ---\")\n",
    "    print(prompt)\n",
    "    final_answer = 'Simulated LLM response' # get_llm_response(prompt) - Mocked for this example\n",
    "    if not openai.api_key:\n",
    "        if 'communicate' in query:\n",
    "            final_answer = \"Zoltarians communicate using light patterns known as 'Luminar' [Source 2].\"\n",
    "        elif 'eat' in query:\n",
    "            final_answer = \"The Zoltarian diet involves absorbing geothermal energy from volcanic vents [Source 1].\"\n",
    "        else:\n",
    "            final_answer = \"I'm sorry, the context does not contain the answer.\"\n",
    "\n",
    "    return final_answer\n",
    "\n",
    "# --- Test the optimized pipeline ---\n",
    "query1 = \"How do Zoltarians communicate and what do they eat?\"\n",
    "print(f'\\n>>> Query: {query1}')\n",
    "answer1 = optimized_rag_pipeline(query1)\n",
    "print(f'\\n>>> Final Answer: {answer1}')\n",
    "\n",
    "print('\\n' + '='*50 + '\\n')\n",
    "\n",
    "# Test a query where the answer is not in the context\n",
    "query2 = \"What is the capital city of Zoltar?\"\n",
    "print(f'>>> Query: {query2}')\n",
    "answer2 = optimized_rag_pipeline(query2)\n",
    "print(f'\\n>>> Final Answer: {answer2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this notebook, we have implemented several key strategies to optimize a RAG pipeline:\n",
    "\n",
    "-   **Advanced Prompting**: We created a detailed prompt that constrains the model to use only the provided context, improving factuality.\n",
    "-   **Context Re-ordering**: By placing the most relevant documents at the beginning and end of the context, we mitigate the \"lost in the middle\" problem and improve the model's attention.\n",
    "-   **Relevance Filtering**: Using a reranker's score as a threshold helps us filter out noisy, irrelevant documents before they ever reach the LLM.\n",
    "-   **Citations**: By including document metadata in the prompt and instructing the model to use it, we've built a system that can trace its answers back to the source, dramatically increasing trustworthiness.\n",
    "\n",
    "These techniques, when combined, lead to a RAG system that is more accurate, reliable, and transparent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}