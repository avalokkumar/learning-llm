{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 22: Basic RAG Implementation\n",
    "\n",
    "In this notebook, we'll build a simple Retrieval-Augmented Generation (RAG) pipeline from scratch. This will help solidify the concepts of chunking, embeddings, and vector stores.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We will cover the following steps:\n",
    "1.  **Setup**: Install necessary libraries.\n",
    "2.  **Document Loading & Chunking**: Prepare our knowledge base.\n",
    "3.  **Embedding**: Convert text chunks into vector embeddings.\n",
    "4.  **Indexing**: Store the embeddings in a local vector store (FAISS).\n",
    "5.  **Retrieval & Generation**: Build the pipeline to answer questions using our knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, we need to install the required libraries. We'll use `sentence-transformers` for creating embeddings and `faiss-cpu` for our local vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers faiss-cpu openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables for OpenAI API key\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Helper function for LLM calls\n",
    "def get_llm_response(prompt):\n",
    "    if not openai.api_key:\n",
    "        print(\"OpenAI API key not found. Returning a placeholder response.\")\n",
    "        return f\"Based on the provided context, the answer to your question is... [Simulated Response]\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document Loading & Chunking\n",
    "\n",
    "For simplicity, we'll use a single string as our document. We'll then write a function to split it into smaller, overlapping chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our knowledge base: A simple text about the fictional planet 'Zoltar'\n",
    "document = (\n",
    "    \"The planet Zoltar is a marvel of the Andromeda galaxy, known for its twin suns, Helios Prime and Helios Beta, which create a perpetual twilight. \"\n",
    "    \"The surface of Zoltar is covered in crystalline forests that hum with a low-frequency energy. This energy is harnessed by the native Zoltarians, a species of sentient, silicon-based lifeforms. \"\n",
    "    \"Zoltarians communicate through a complex series of light patterns, a language known as 'Luminar'. Their society is structured around the 'Great Crystal', a massive geological formation at the planet's north pole that is believed to be the source of all life. \"\n",
    "    \"The Zoltarian diet consists of absorbing geothermal energy from volcanic vents scattered across the planet. They reproduce asexually, budding off smaller versions of themselves once every 'Great Cycle', which corresponds to 50 Earth years. \"\n",
    "    \"Zoltar's atmosphere is composed mainly of nitrogen and argon, and is unbreathable for carbon-based lifeforms. The planet has a strong magnetic field, which protects it from the intense solar winds of its twin suns. The average temperature on Zoltar is a cool -30 degrees Celsius.\"\n",
    ")\n",
    "\n",
    "def chunk_text(text, chunk_size=150, overlap=30):\n",
    "    \"\"\"Splits text into overlapping chunks.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# Create chunks\n",
    "text_chunks = chunk_text(document)\n",
    "\n",
    "print(f\"Created {len(text_chunks)} chunks.\")\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    print(f\"--- Chunk {i+1} ---\")\n",
    "    print(chunk + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedding\n",
    "\n",
    "Next, we'll use a sentence-transformer model to convert our text chunks into numerical vectors (embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for each chunk\n",
    "chunk_embeddings = embedding_model.encode(text_chunks, convert_to_tensor=False)\n",
    "\n",
    "print(f\"Shape of the embedding matrix: {chunk_embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {chunk_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Indexing\n",
    "\n",
    "Now, we'll store these embeddings in a FAISS index for efficient similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dimension of the embeddings\n",
    "d = chunk_embeddings.shape[1]\n",
    "\n",
    "# Create a FAISS index\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Add the chunk embeddings to the index\n",
    "index.add(chunk_embeddings.astype('float32'))\n",
    "\n",
    "print(f\"FAISS index created with {index.ntotal} vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Retrieval & Generation\n",
    "\n",
    "Finally, let's build the core RAG logic. We'll take a user query, find relevant chunks, and use an LLM to generate an answer based on that context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(query, k=2):\n",
    "    \"\"\"The main RAG pipeline function.\"\"\"\n",
    "    # 1. Embed the user query\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    \n",
    "    # 2. Retrieve relevant chunks from the vector store\n",
    "    distances, indices = index.search(query_embedding.astype('float32'), k)\n",
    "    \n",
    "    # Get the actual text chunks\n",
    "    retrieved_chunks = [text_chunks[i] for i in indices[0]]\n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "    \n",
    "    print(f\"--- Retrieved Context ---\")\n",
    "    print(context + '\\n')\n",
    "    \n",
    "    # 3. Build the augmented prompt\n",
    "    prompt = (\n",
    "        f\"Based on the following context, please answer the question.\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {query}\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "    \n",
    "    # 4. Generate the final answer using the LLM\n",
    "    final_answer = get_llm_response(prompt)\n",
    "    return final_answer\n",
    "\n",
    "# --- Let's test our RAG system! ---\n",
    "query1 = \"How do Zoltarians communicate?\"\n",
    "print(f'\\n>>> Query: {query1}')\n",
    "answer1 = answer_question(query1)\n",
    "print(f'\\n>>> Final Answer: {answer1}')\n",
    "\n",
    "print('\\n' + '='*50 + '\\n')\n",
    "\n",
    "query2 = \"What do Zoltarians eat?\"\n",
    "print(f'>>> Query: {query2}')\n",
    "answer2 = answer_question(query2)\n",
    "print(f'\\n>>> Final Answer: {answer2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "Congratulations! You've built a complete, albeit simple, RAG pipeline from scratch. We have successfully:\n",
    "\n",
    "- Loaded and chunked a source document.\n",
    "- Generated vector embeddings for each chunk.\n",
    "- Stored and indexed these embeddings in a FAISS vector store.\n",
    "- Implemented a retrieval-and-generation loop to answer questions based on the document.\n",
    "\n",
    "This forms the foundation for more complex RAG systems. In the upcoming days, we'll explore how to improve each component of this pipeline, from enhancing retrieval quality with rerankers to optimizing the final prompt for the LLM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}