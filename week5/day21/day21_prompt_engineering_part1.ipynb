{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 21: Prompt Engineering Patterns - Part 1\n",
    "\n",
    "In this notebook, we'll implement and experiment with zero-shot and few-shot prompting techniques. These are fundamental prompt engineering patterns that can significantly improve language model performance without requiring model fine-tuning.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll cover:\n",
    "1. Setting up the environment\n",
    "2. Zero-shot prompting implementation\n",
    "3. Few-shot prompting implementation\n",
    "4. Comparing the effectiveness of different approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "\n",
    "First, let's install and import the necessary libraries. We'll use OpenAI's API for this demonstration, but the concepts apply to any language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai python-dotenv requests matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (API keys)\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# If you don't have an API key, you can use this function to simulate API calls\n",
    "def simulate_llm_call(prompt, model=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "    \"\"\"Simulate a call to a language model API for demonstration purposes.\"\"\"\n",
    "    print(\"\\n--- Prompt ---\\n\")\n",
    "    print(prompt)\n",
    "    print(\"\\n--- [Simulated LLM Response] ---\\n\")\n",
    "    \n",
    "    # Simulate different responses based on the prompt\n",
    "    if \"sentiment\" in prompt.lower():\n",
    "        if \"amazing\" in prompt.lower() or \"love\" in prompt.lower():\n",
    "            return \"positive\"\n",
    "        elif \"terrible\" in prompt.lower() or \"hate\" in prompt.lower():\n",
    "            return \"negative\"\n",
    "        else:\n",
    "            return \"neutral\"\n",
    "    elif \"classify\" in prompt.lower():\n",
    "        if \"animal\" in prompt.lower():\n",
    "            return \"animal\"\n",
    "        elif \"technology\" in prompt.lower() or \"computer\" in prompt.lower():\n",
    "            return \"technology\"\n",
    "        else:\n",
    "            return \"other\"\n",
    "    else:\n",
    "        return \"This is a simulated response. Please provide your OpenAI API key for actual responses.\"\n",
    "\n",
    "# Function to call OpenAI API\n",
    "def call_llm(prompt, model=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "    \"\"\"Call the language model API with the given prompt.\"\"\"\n",
    "    try:\n",
    "        if not openai.api_key or openai.api_key == \"your-api-key-here\":\n",
    "            return simulate_llm_call(prompt, model, temperature)\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling LLM API: {e}\")\n",
    "        return simulate_llm_call(prompt, model, temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Zero-Shot Prompting\n",
    "\n",
    "Zero-shot prompting involves asking the model to perform a task without providing any examples. The model relies entirely on its pre-training knowledge and the instructions in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_prompt(task_description, input_data, output_format=\"\"):\n",
    "    \"\"\"Create a zero-shot prompt with clear instructions.\"\"\"\n",
    "    prompt = f\"{task_description}\\n\\n{input_data}\"\n",
    "    \n",
    "    if output_format:\n",
    "        prompt += f\"\\n\\n{output_format}\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Zero-Shot Sentiment Analysis\n",
    "\n",
    "Let's implement a zero-shot prompt for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example texts for sentiment analysis\n",
    "sentiment_texts = [\n",
    "    \"I love this new restaurant! The food was amazing and the service was excellent.\",\n",
    "    \"This movie was terrible. I hated every minute of it.\",\n",
    "    \"The weather is okay today, nothing special.\",\n",
    "    \"I'm so excited about my vacation next week!\",\n",
    "    \"The new software update has some interesting features.\"\n",
    "]\n",
    "\n",
    "# Create and test zero-shot sentiment analysis prompts\n",
    "for text in sentiment_texts:\n",
    "    task_description = \"Classify the sentiment of the following text as positive, negative, or neutral.\"\n",
    "    input_data = f\"Text: {text}\"\n",
    "    output_format = \"Sentiment:\"\n",
    "    \n",
    "    prompt = zero_shot_prompt(task_description, input_data, output_format)\n",
    "    response = call_llm(prompt, temperature=0.1)  # Low temperature for consistent results\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Zero-Shot Text Classification\n",
    "\n",
    "Now let's try a different task: classifying text into categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example texts for classification\n",
    "classification_texts = [\n",
    "    \"The new iPhone 13 has an improved camera and longer battery life.\",\n",
    "    \"Elephants are the largest land animals and can live up to 70 years.\",\n",
    "    \"The Great Barrier Reef is the world's largest coral reef system.\",\n",
    "    \"Python is a popular programming language known for its simplicity and readability.\",\n",
    "    \"The Eiffel Tower was completed in 1889 and stands at 324 meters tall.\"\n",
    "]\n",
    "\n",
    "# Categories\n",
    "categories = [\"Technology\", \"Animals\", \"Nature\", \"History\"]\n",
    "\n",
    "# Create and test zero-shot classification prompts\n",
    "for text in classification_texts:\n",
    "    task_description = f\"Classify the following text into one of these categories: {', '.join(categories)}.\"\n",
    "    input_data = f\"Text: {text}\"\n",
    "    output_format = \"Category:\"\n",
    "    \n",
    "    prompt = zero_shot_prompt(task_description, input_data, output_format)\n",
    "    response = call_llm(prompt, temperature=0.1)\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Category: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Zero-Shot Question Answering\n",
    "\n",
    "Let's try zero-shot prompting for question answering tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example context and questions\n",
    "context = \"\"\"\n",
    "The James Webb Space Telescope (JWST) is a space telescope designed primarily to conduct infrared astronomy. \n",
    "The U.S. National Aeronautics and Space Administration (NASA) led development of the telescope in collaboration \n",
    "with the European Space Agency (ESA) and the Canadian Space Agency (CSA). The telescope is named after James E. Webb, \n",
    "who was the administrator of NASA from 1961 to 1968 during the Mercury, Gemini, and Apollo programs.\n",
    "\n",
    "The telescope was launched on 25 December 2021 on an Ariane 5 rocket from Kourou, French Guiana, and arrived at \n",
    "the Sunâ€“Earth L2 Lagrange point in January 2022. The first JWST image was released to the public on July 11, 2022.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"When was the James Webb Space Telescope launched?\",\n",
    "    \"Who is the telescope named after?\",\n",
    "    \"Which space agencies collaborated on the JWST?\",\n",
    "    \"Where is the JWST located?\"\n",
    "]\n",
    "\n",
    "# Create and test zero-shot QA prompts\n",
    "for question in questions:\n",
    "    task_description = \"Answer the question based on the given context.\"\n",
    "    input_data = f\"Context: {context}\\n\\nQuestion: {question}\"\n",
    "    output_format = \"Answer:\"\n",
    "    \n",
    "    prompt = zero_shot_prompt(task_description, input_data, output_format)\n",
    "    response = call_llm(prompt)\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Few-Shot Prompting\n",
    "\n",
    "Few-shot prompting provides the model with a few examples of the desired input-output pattern before presenting the actual task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_prompt(task_description, examples, input_data, output_format=\"\"):\n",
    "    \"\"\"Create a few-shot prompt with examples.\"\"\"\n",
    "    prompt = f\"{task_description}\\n\\n\"\n",
    "    \n",
    "    # Add examples\n",
    "    for example in examples:\n",
    "        prompt += f\"{example['input']}\\n{example['output']}\\n\\n\"\n",
    "    \n",
    "    # Add the actual input\n",
    "    prompt += f\"{input_data}\"\n",
    "    \n",
    "    if output_format:\n",
    "        prompt += f\"\\n{output_format}\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Few-Shot Sentiment Analysis\n",
    "\n",
    "Let's implement few-shot prompting for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples for sentiment analysis\n",
    "sentiment_examples = [\n",
    "    {\n",
    "        \"input\": \"Text: This movie was terrible. I hated every minute of it.\",\n",
    "        \"output\": \"Sentiment: negative\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Text: The weather is okay today, nothing special.\",\n",
    "        \"output\": \"Sentiment: neutral\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Text: I'm so excited about my vacation next week!\",\n",
    "        \"output\": \"Sentiment: positive\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# New texts for sentiment analysis\n",
    "new_sentiment_texts = [\n",
    "    \"The customer service was absolutely appalling.\",\n",
    "    \"I might go to the park later if it doesn't rain.\",\n",
    "    \"The concert last night was the best I've ever been to!\"\n",
    "]\n",
    "\n",
    "# Create and test few-shot sentiment analysis prompts\n",
    "for text in new_sentiment_texts:\n",
    "    task_description = \"Classify the sentiment of the following texts as positive, negative, or neutral.\"\n",
    "    input_data = f\"Text: {text}\"\n",
    "    output_format = \"Sentiment:\"\n",
    "    \n",
    "    prompt = few_shot_prompt(task_description, sentiment_examples, input_data, output_format)\n",
    "    response = call_llm(prompt, temperature=0.1)\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Few-Shot Text Classification\n",
    "\n",
    "Now let's try few-shot prompting for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples for classification\n",
    "classification_examples = [\n",
    "    {\n",
    "        \"input\": \"Text: The new iPhone 13 has an improved camera and longer battery life.\",\n",
    "        \"output\": \"Category: Technology\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Text: Elephants are the largest land animals and can live up to 70 years.\",\n",
    "        \"output\": \"Category: Animals\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Text: The Great Barrier Reef is the world's largest coral reef system.\",\n",
    "        \"output\": \"Category: Nature\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# New texts for classification\n",
    "new_classification_texts = [\n",
    "    \"The Roman Empire reached its peak territorial expansion under Emperor Trajan.\",\n",
    "    \"Quantum computers use qubits instead of traditional binary bits.\",\n",
    "    \"Dolphins are highly intelligent marine mammals known for their playful behavior.\",\n",
    "    \"The Amazon Rainforest produces about 20% of the world's oxygen.\"\n",
    "]\n",
    "\n",
    "# Create and test few-shot classification prompts\n",
    "for text in new_classification_texts:\n",
    "    task_description = \"Classify the following texts into one of these categories: Technology, Animals, Nature, History.\"\n",
    "    input_data = f\"Text: {text}\"\n",
    "    output_format = \"Category:\"\n",
    "    \n",
    "    prompt = few_shot_prompt(task_description, classification_examples, input_data, output_format)\n",
    "    response = call_llm(prompt, temperature=0.1)\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Category: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Few-Shot Named Entity Recognition\n",
    "\n",
    "Let's implement few-shot prompting for a more complex task: named entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples for named entity recognition\n",
    "ner_examples = [\n",
    "    {\n",
    "        \"input\": \"Text: Apple is looking at buying U.K. startup for $1 billion.\",\n",
    "        \"output\": \"Entities: [\\n  {\\\"entity\\\": \\\"Apple\\\", \\\"type\\\": \\\"ORG\\\"},\\n  {\\\"entity\\\": \\\"U.K.\\\", \\\"type\\\": \\\"GPE\\\"},\\n  {\\\"entity\\\": \\\"$1 billion\\\", \\\"type\\\": \\\"MONEY\\\"}\\n]\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Text: Microsoft was founded by Bill Gates and Paul Allen in April 1975.\",\n",
    "        \"output\": \"Entities: [\\n  {\\\"entity\\\": \\\"Microsoft\\\", \\\"type\\\": \\\"ORG\\\"},\\n  {\\\"entity\\\": \\\"Bill Gates\\\", \\\"type\\\": \\\"PERSON\\\"},\\n  {\\\"entity\\\": \\\"Paul Allen\\\", \\\"type\\\": \\\"PERSON\\\"},\\n  {\\\"entity\\\": \\\"April 1975\\\", \\\"type\\\": \\\"DATE\\\"}\\n]\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# New texts for NER\n",
    "new_ner_texts = [\n",
    "    \"Amazon announced a new office in Seattle that will employ 2,000 people.\",\n",
    "    \"The Golden State Warriors won the NBA championship in June 2022.\",\n",
    "    \"Tesla's CEO Elon Musk visited Berlin last Thursday.\"\n",
    "]\n",
    "\n",
    "# Create and test few-shot NER prompts\n",
    "for text in new_ner_texts:\n",
    "    task_description = \"Extract named entities from the text and classify them as PERSON, ORG (organization), GPE (geopolitical entity), DATE, or MONEY.\"\n",
    "    input_data = f\"Text: {text}\"\n",
    "    output_format = \"Entities:\"\n",
    "    \n",
    "    prompt = few_shot_prompt(task_description, ner_examples, input_data, output_format)\n",
    "    response = call_llm(prompt)\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Entities: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing Zero-Shot and Few-Shot Performance\n",
    "\n",
    "Let's compare the performance of zero-shot and few-shot prompting on a sentiment analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset with ground truth labels\n",
    "test_data = [\n",
    "    {\"text\": \"This product exceeded all my expectations!\", \"sentiment\": \"positive\"},\n",
    "    {\"text\": \"The service was slow and the staff was rude.\", \"sentiment\": \"negative\"},\n",
    "    {\"text\": \"It was an average experience, nothing special.\", \"sentiment\": \"neutral\"},\n",
    "    {\"text\": \"I wouldn't recommend this to anyone.\", \"sentiment\": \"negative\"},\n",
    "    {\"text\": \"The price is reasonable for what you get.\", \"sentiment\": \"neutral\"},\n",
    "    {\"text\": \"Best purchase I've made all year!\", \"sentiment\": \"positive\"}\n",
    "]\n",
    "\n",
    "# Function to evaluate accuracy\n",
    "def evaluate_accuracy(predictions, ground_truth):\n",
    "    correct = sum(1 for p, g in zip(predictions, ground_truth) if p.lower().strip() == g.lower())\n",
    "    return correct / len(ground_truth) if ground_truth else 0\n",
    "\n",
    "# Run zero-shot and few-shot prompting on test data\n",
    "zero_shot_results = []\n",
    "few_shot_results = []\n",
    "ground_truth = [item[\"sentiment\"] for item in test_data]\n",
    "\n",
    "for item in test_data:\n",
    "    text = item[\"text\"]\n",
    "    \n",
    "    # Zero-shot\n",
    "    zero_shot_prompt_text = zero_shot_prompt(\n",
    "        \"Classify the sentiment of the following text as positive, negative, or neutral.\",\n",
    "        f\"Text: {text}\",\n",
    "        \"Sentiment:\"\n",
    "    )\n",
    "    zero_shot_response = call_llm(zero_shot_prompt_text, temperature=0.1)\n",
    "    zero_shot_results.append(zero_shot_response)\n",
    "    \n",
    "    # Few-shot\n",
    "    few_shot_prompt_text = few_shot_prompt(\n",
    "        \"Classify the sentiment of the following texts as positive, negative, or neutral.\",\n",
    "        sentiment_examples,\n",
    "        f\"Text: {text}\",\n",
    "        \"Sentiment:\"\n",
    "    )\n",
    "    few_shot_response = call_llm(few_shot_prompt_text, temperature=0.1)\n",
    "    few_shot_results.append(few_shot_response)\n",
    "    \n",
    "    # Add a small delay to avoid rate limits\n",
    "    time.sleep(1)\n",
    "\n",
    "# Calculate accuracy\n",
    "zero_shot_accuracy = evaluate_accuracy(zero_shot_results, ground_truth)\n",
    "few_shot_accuracy = evaluate_accuracy(few_shot_results, ground_truth)\n",
    "\n",
    "print(f\"Zero-shot accuracy: {zero_shot_accuracy:.2f}\")\n",
    "print(f\"Few-shot accuracy: {few_shot_accuracy:.2f}\")\n",
    "\n",
    "# Create a comparison table\n",
    "comparison_data = []\n",
    "for i, item in enumerate(test_data):\n",
    "    comparison_data.append({\n",
    "        \"Text\": item[\"text\"],\n",
    "        \"Ground Truth\": item[\"sentiment\"],\n",
    "        \"Zero-Shot\": zero_shot_results[i],\n",
    "        \"Few-Shot\": few_shot_results[i]\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "display(comparison_df)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar([\"Zero-Shot\", \"Few-Shot\"], [zero_shot_accuracy, few_shot_accuracy], color=[\"blue\", \"green\"])\n",
    "plt.ylim(0, 1.1)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Comparison of Zero-Shot vs. Few-Shot Prompting\")\n",
    "for i, v in enumerate([zero_shot_accuracy, few_shot_accuracy]):\n",
    "    plt.text(i, v + 0.05, f\"{v:.2f}\", ha=\"center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this notebook, we've explored and implemented zero-shot and few-shot prompting techniques. Here are the key takeaways:\n",
    "\n",
    "1. **Zero-shot prompting** works well for simple, well-defined tasks where the model has sufficient pre-training knowledge.\n",
    "\n",
    "2. **Few-shot prompting** generally improves performance by providing examples of the desired input-output pattern, especially for complex or ambiguous tasks.\n",
    "\n",
    "3. **Prompt structure matters**: Clear instructions, well-formatted examples, and explicit output formats can significantly improve results.\n",
    "\n",
    "In the next part, we'll explore more advanced prompting techniques including chain-of-thought reasoning and self-consistency methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
