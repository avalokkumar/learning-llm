{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 21: Prompt Engineering Patterns - Part 2\n",
    "\n",
    "In this notebook, we'll implement and experiment with Chain-of-Thought (CoT) reasoning and Self-consistency methods. These advanced prompting techniques can significantly improve language model performance on complex reasoning tasks.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll cover:\n",
    "1. Setting up the environment\n",
    "2. Chain-of-Thought (CoT) reasoning implementation\n",
    "3. Self-consistency methods implementation\n",
    "4. Comparing the effectiveness of different approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "\n",
    "First, let's install and import the necessary libraries. We'll use OpenAI's API for this demonstration, but the concepts apply to any language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai python-dotenv requests matplotlib pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (API keys)\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# If you don't have an API key, you can use this function to simulate API calls\n",
    "def simulate_llm_call(prompt, model=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "    \"\"\"Simulate a call to a language model API for demonstration purposes.\"\"\"\n",
    "    print(\"\\n--- Prompt ---\\n\")\n",
    "    print(prompt)\n",
    "    print(\"\\n--- [Simulated LLM Response] ---\\n\")\n",
    "    \n",
    "    # Simulate different responses based on the prompt\n",
    "    if \"step by step\" in prompt.lower():\n",
    "        if \"math problem\" in prompt.lower() or \"calculate\" in prompt.lower():\n",
    "            return \"Let me solve this step by step:\\n1. First, I'll identify the key values\\n2. Then I'll set up the equation\\n3. Next, I'll solve for the unknown\\n4. Finally, I'll verify my answer\\n\\nThe answer is 42.\"\n",
    "        else:\n",
    "            return \"I'll think through this step by step and provide a detailed answer.\"\n",
    "    else:\n",
    "        return \"This is a simulated response. Please provide your OpenAI API key for actual responses.\"\n",
    "\n",
    "# Function to call OpenAI API\n",
    "def call_llm(prompt, model=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "    \"\"\"Call the language model API with the given prompt.\"\"\"\n",
    "    try:\n",
    "        if not openai.api_key or openai.api_key == \"your-api-key-here\":\n",
    "            return simulate_llm_call(prompt, model, temperature)\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling LLM API: {e}\")\n",
    "        return simulate_llm_call(prompt, model, temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chain-of-Thought (CoT) Reasoning\n",
    "\n",
    "Chain-of-Thought (CoT) prompting encourages the model to break down complex problems into step-by-step reasoning processes, similar to showing your work in mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_prompt(problem):\n",
    "    \"\"\"Create a standard prompt without CoT.\"\"\"\n",
    "    return f\"Problem: {problem}\\n\\nAnswer:\"\n",
    "\n",
    "def zero_shot_cot_prompt(problem):\n",
    "    \"\"\"Create a zero-shot CoT prompt.\"\"\"\n",
    "    return f\"Problem: {problem}\\n\\nLet's think step by step:\"\n",
    "\n",
    "def few_shot_cot_prompt(problem, examples):\n",
    "    \"\"\"Create a few-shot CoT prompt with examples.\"\"\"\n",
    "    prompt = \"Solve the following problems by thinking step by step.\\n\\n\"\n",
    "    \n",
    "    # Add examples\n",
    "    for example in examples:\n",
    "        prompt += f\"Problem: {example['problem']}\\n\\n{example['solution']}\\n\\n\"\n",
    "    \n",
    "    # Add the actual problem\n",
    "    prompt += f\"Problem: {problem}\\n\\nLet's think step by step:\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Math Word Problems\n",
    "\n",
    "Let's test CoT reasoning on math word problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example math word problems\n",
    "math_problems = [\n",
    "    \"A store has 24 apples. They sell 1/3 of them in the morning and 1/4 of the remaining apples in the afternoon. How many apples are left?\",\n",
    "    \"If a train travels 60 miles per hour for 2.5 hours, how far does it travel?\",\n",
    "    \"A recipe calls for 2 cups of flour to make 12 cookies. How much flour is needed for 18 cookies?\",\n",
    "    \"A car uses 1 gallon of gas to travel 25 miles. How many gallons are needed for a 150-mile trip?\"\n",
    "]\n",
    "\n",
    "# CoT examples for few-shot prompting\n",
    "cot_examples = [\n",
    "    {\n",
    "        \"problem\": \"John has 5 marbles. He buys 2 more bags of marbles, with 7 marbles in each bag. How many marbles does John have now?\",\n",
    "        \"solution\": \"Let's think step by step:\\n1. John starts with 5 marbles.\\n2. He buys 2 bags of marbles, with 7 marbles in each bag.\\n3. The number of marbles in the bags is 2 × 7 = 14 marbles.\\n4. Now John has 5 + 14 = 19 marbles.\\n\\nAnswer: 19 marbles\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"Sarah has $30. She spends 2/3 of her money on a book. How much money does she have left?\",\n",
    "        \"solution\": \"Let's think step by step:\\n1. Sarah starts with $30.\\n2. She spends 2/3 of her money on a book.\\n3. The amount she spends is 2/3 × $30 = $20.\\n4. The amount she has left is $30 - $20 = $10.\\n\\nAnswer: $10\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test standard prompting vs. zero-shot CoT vs. few-shot CoT\n",
    "for problem in math_problems:\n",
    "    print(f\"Problem: {problem}\\n\")\n",
    "    \n",
    "    # Standard prompting\n",
    "    standard_prompt_text = standard_prompt(problem)\n",
    "    standard_response = call_llm(standard_prompt_text)\n",
    "    print(\"Standard Response:\")\n",
    "    print(standard_response)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # Zero-shot CoT\n",
    "    zero_shot_cot_prompt_text = zero_shot_cot_prompt(problem)\n",
    "    zero_shot_cot_response = call_llm(zero_shot_cot_prompt_text)\n",
    "    print(\"Zero-Shot CoT Response:\")\n",
    "    print(zero_shot_cot_response)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # Few-shot CoT\n",
    "    few_shot_cot_prompt_text = few_shot_cot_prompt(problem, cot_examples)\n",
    "    few_shot_cot_response = call_llm(few_shot_cot_prompt_text)\n",
    "    print(\"Few-Shot CoT Response:\")\n",
    "    print(few_shot_cot_response)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Logical Reasoning Problems\n",
    "\n",
    "Let's test CoT reasoning on logical reasoning problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example logical reasoning problems\n",
    "logic_problems = [\n",
    "    \"If all A are B, and some B are C, can we conclude that some A are C?\",\n",
    "    \"If it's raining, then the ground is wet. The ground is wet. Does that mean it's raining?\",\n",
    "    \"All lions are mammals. Some mammals are carnivores. Are all lions carnivores?\",\n",
    "    \"If I have a red marble, a blue marble, and a green marble in a bag, and I pick two marbles without replacement, what is the probability that I pick the red marble and then the blue marble?\"\n",
    "]\n",
    "\n",
    "# CoT examples for logical reasoning\n",
    "logic_cot_examples = [\n",
    "    {\n",
    "        \"problem\": \"If all dogs are animals, and all animals need food, do all dogs need food?\",\n",
    "        \"solution\": \"Let's think step by step:\\n1. We know that all dogs are animals.\\n2. We also know that all animals need food.\\n3. Since all dogs are animals, and all animals need food, it follows that all dogs need food.\\n\\nAnswer: Yes, all dogs need food.\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"If no fish are mammals, and all whales are mammals, are any whales fish?\",\n",
    "        \"solution\": \"Let's think step by step:\\n1. We know that no fish are mammals.\\n2. We also know that all whales are mammals.\\n3. If all whales are mammals, and no fish are mammals, then whales cannot be fish.\\n\\nAnswer: No, no whales are fish.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test standard prompting vs. zero-shot CoT vs. few-shot CoT\n",
    "for problem in logic_problems:\n",
    "    print(f\"Problem: {problem}\\n\")\n",
    "    \n",
    "    # Standard prompting\n",
    "    standard_prompt_text = standard_prompt(problem)\n",
    "    standard_response = call_llm(standard_prompt_text)\n",
    "    print(\"Standard Response:\")\n",
    "    print(standard_response)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # Zero-shot CoT\n",
    "    zero_shot_cot_prompt_text = zero_shot_cot_prompt(problem)\n",
    "    zero_shot_cot_response = call_llm(zero_shot_cot_prompt_text)\n",
    "    print(\"Zero-Shot CoT Response:\")\n",
    "    print(zero_shot_cot_response)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # Few-shot CoT\n",
    "    few_shot_cot_prompt_text = few_shot_cot_prompt(problem, logic_cot_examples)\n",
    "    few_shot_cot_response = call_llm(few_shot_cot_prompt_text)\n",
    "    print(\"Few-Shot CoT Response:\")\n",
    "    print(few_shot_cot_response)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Self-Consistency Methods\n",
    "\n",
    "Self-consistency involves generating multiple reasoning paths for the same problem and selecting the most frequent answer. This helps reduce errors from individual reasoning mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_answer(response):\n",
    "    \"\"\"Extract the final answer from a CoT response.\"\"\"\n",
    "    # Look for patterns like \"Answer: X\" or \"Therefore, X\" at the end\n",
    "    answer_patterns = [\n",
    "        r\"Answer:\\s*(.+)$\",\n",
    "        r\"Therefore,\\s*(.+)$\",\n",
    "        r\"Thus,\\s*(.+)$\",\n",
    "        r\"So,\\s*(.+)$\",\n",
    "        r\"In conclusion,\\s*(.+)$\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in answer_patterns:\n",
    "        match = re.search(pattern, response, re.IGNORECASE | re.MULTILINE)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    \n",
    "    # If no pattern matches, return the last sentence\n",
    "    sentences = response.split('.')\n",
    "    if sentences:\n",
    "        return sentences[-1].strip()\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "def self_consistency(problem, num_samples=5, temperature=0.7):\n",
    "    \"\"\"Implement self-consistency by generating multiple reasoning paths.\"\"\"\n",
    "    responses = []\n",
    "    answers = []\n",
    "    \n",
    "    # Generate multiple responses using zero-shot CoT\n",
    "    prompt = zero_shot_cot_prompt(problem)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        response = call_llm(prompt, temperature=temperature)\n",
    "        responses.append(response)\n",
    "        \n",
    "        # Extract the final answer\n",
    "        answer = extract_final_answer(response)\n",
    "        answers.append(answer)\n",
    "        \n",
    "        # Add a small delay to avoid rate limits\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Find the most common answer\n",
    "    answer_counts = Counter(answers)\n",
    "    most_common_answer = answer_counts.most_common(1)[0][0]\n",
    "    confidence = answer_counts[most_common_answer] / num_samples\n",
    "    \n",
    "    return {\n",
    "        \"responses\": responses,\n",
    "        \"answers\": answers,\n",
    "        \"most_common_answer\": most_common_answer,\n",
    "        \"confidence\": confidence\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Self-Consistency for Math Problems\n",
    "\n",
    "Let's apply self-consistency to a challenging math problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A challenging math problem\n",
    "challenging_math_problem = \"If the probability of rain on Saturday is 60% and the probability of rain on Sunday is 70%, what is the probability of rain on both days? Assume the events are independent.\"\n",
    "\n",
    "# Apply self-consistency\n",
    "sc_results = self_consistency(challenging_math_problem, num_samples=3, temperature=0.7)\n",
    "\n",
    "# Display results\n",
    "print(f\"Problem: {challenging_math_problem}\\n\")\n",
    "print(f\"Most common answer: {sc_results['most_common_answer']}\")\n",
    "print(f\"Confidence: {sc_results['confidence']:.2f} ({sc_results['answers'].count(sc_results['most_common_answer'])}/{len(sc_results['answers'])})\\n\")\n",
    "\n",
    "print(\"Individual reasoning paths:\")\n",
    "for i, (response, answer) in enumerate(zip(sc_results['responses'], sc_results['answers'])):\n",
    "    print(f\"\\nAttempt {i+1}:\")\n",
    "    print(response)\n",
    "    print(f\"Extracted answer: {answer}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Self-Consistency with Different Temperatures\n",
    "\n",
    "Let's explore how temperature affects self-consistency results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A problem with potential for different reasoning paths\n",
    "probability_problem = \"A bag contains 3 red marbles, 4 blue marbles, and 5 green marbles. If you draw 2 marbles without replacement, what is the probability of drawing a red marble followed by a blue marble?\"\n",
    "\n",
    "# Test different temperatures\n",
    "temperatures = [0.1, 0.5, 0.9]\n",
    "temperature_results = {}\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nTesting temperature = {temp}\")\n",
    "    sc_results = self_consistency(probability_problem, num_samples=3, temperature=temp)\n",
    "    temperature_results[temp] = sc_results\n",
    "    \n",
    "    print(f\"Most common answer: {sc_results['most_common_answer']}\")\n",
    "    print(f\"Confidence: {sc_results['confidence']:.2f} ({sc_results['answers'].count(sc_results['most_common_answer'])}/{len(sc_results['answers'])})\")\n",
    "    print(f\"All answers: {sc_results['answers']}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot confidence vs. temperature\n",
    "confidences = [temperature_results[temp]['confidence'] for temp in temperatures]\n",
    "plt.plot(temperatures, confidences, marker='o', linestyle='-', linewidth=2)\n",
    "\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Confidence (Agreement Ratio)')\n",
    "plt.title('Effect of Temperature on Self-Consistency Confidence')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing Standard, CoT, and Self-Consistency Approaches\n",
    "\n",
    "Let's compare the performance of standard prompting, CoT reasoning, and self-consistency on a set of challenging problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test problems with known answers\n",
    "test_problems = [\n",
    "    {\n",
    "        \"problem\": \"A store has 24 apples. They sell 1/3 of them in the morning and 1/4 of the remaining apples in the afternoon. How many apples are left?\",\n",
    "        \"answer\": \"12\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"If a train travels 60 miles per hour for 2.5 hours, how far does it travel?\",\n",
    "        \"answer\": \"150 miles\"\n",
    "    },\n",
    "    {\n",
    "        \"problem\": \"If the probability of rain on Saturday is 60% and the probability of rain on Sunday is 70%, what is the probability of rain on both days? Assume the events are independent.\",\n",
    "        \"answer\": \"42%\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to check if an answer is correct\n",
    "def is_correct(predicted, actual):\n",
    "    \"\"\"Check if the predicted answer matches the actual answer.\"\"\"\n",
    "    # Normalize answers for comparison\n",
    "    predicted = re.sub(r'[^0-9.%]', '', predicted.lower())\n",
    "    actual = re.sub(r'[^0-9.%]', '', actual.lower())\n",
    "    return predicted == actual\n",
    "\n",
    "# Compare approaches\n",
    "results = []\n",
    "\n",
    "for problem_data in test_problems:\n",
    "    problem = problem_data[\"problem\"]\n",
    "    correct_answer = problem_data[\"answer\"]\n",
    "    \n",
    "    print(f\"Problem: {problem}\")\n",
    "    print(f\"Correct answer: {correct_answer}\\n\")\n",
    "    \n",
    "    # Standard approach\n",
    "    standard_response = call_llm(standard_prompt(problem))\n",
    "    standard_answer = extract_final_answer(standard_response)\n",
    "    standard_correct = is_correct(standard_answer, correct_answer)\n",
    "    \n",
    "    # CoT approach\n",
    "    cot_response = call_llm(zero_shot_cot_prompt(problem))\n",
    "    cot_answer = extract_final_answer(cot_response)\n",
    "    cot_correct = is_correct(cot_answer, correct_answer)\n",
    "    \n",
    "    # Self-consistency approach (simplified to 3 samples for demonstration)\n",
    "    sc_results = self_consistency(problem, num_samples=3)\n",
    "    sc_answer = sc_results[\"most_common_answer\"]\n",
    "    sc_correct = is_correct(sc_answer, correct_answer)\n",
    "    \n",
    "    print(f\"Standard answer: {standard_answer} (Correct: {standard_correct})\")\n",
    "    print(f\"CoT answer: {cot_answer} (Correct: {cot_correct})\")\n",
    "    print(f\"Self-consistency answer: {sc_answer} (Correct: {sc_correct})\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    results.append({\n",
    "        \"problem\": problem,\n",
    "        \"standard_correct\": standard_correct,\n",
    "        \"cot_correct\": cot_correct,\n",
    "        \"sc_correct\": sc_correct\n",
    "    })\n",
    "\n",
    "# Calculate accuracy for each approach\n",
    "standard_accuracy = sum(r[\"standard_correct\"] for r in results) / len(results)\n",
    "cot_accuracy = sum(r[\"cot_correct\"] for r in results) / len(results)\n",
    "sc_accuracy = sum(r[\"sc_correct\"] for r in results) / len(results)\n",
    "\n",
    "print(f\"Standard accuracy: {standard_accuracy:.2f}\")\n",
    "print(f\"CoT accuracy: {cot_accuracy:.2f}\")\n",
    "print(f\"Self-consistency accuracy: {sc_accuracy:.2f}\")\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "methods = [\"Standard\", \"Chain-of-Thought\", \"Self-Consistency\"]\n",
    "accuracies = [standard_accuracy, cot_accuracy, sc_accuracy]\n",
    "\n",
    "plt.bar(methods, accuracies, color=[\"blue\", \"green\", \"orange\"])\n",
    "plt.ylim(0, 1.1)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Comparison of Prompting Methods\")\n",
    "\n",
    "for i, v in enumerate(accuracies):\n",
    "    plt.text(i, v + 0.05, f\"{v:.2f}\", ha=\"center\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this notebook, we've explored and implemented Chain-of-Thought (CoT) reasoning and Self-consistency methods. Here are the key takeaways:\n",
    "\n",
    "1. **Chain-of-Thought (CoT) reasoning** significantly improves performance on complex reasoning tasks by breaking down problems into step-by-step solutions.\n",
    "\n",
    "2. **Zero-shot CoT** can be implemented simply by adding \"Let's think step by step\" to prompts, making it a very efficient technique.\n",
    "\n",
    "3. **Few-shot CoT** provides even better guidance by showing examples of step-by-step reasoning, which helps the model understand the expected reasoning pattern.\n",
    "\n",
    "4. **Self-consistency** further improves reliability by generating multiple reasoning paths and selecting the most consistent answer, reducing the impact of individual reasoning errors.\n",
    "\n",
    "5. **Temperature settings** affect the diversity of reasoning paths in self-consistency, with lower temperatures generally leading to more consistent answers but potentially missing creative solutions.\n",
    "\n",
    "These advanced prompting techniques are particularly valuable for complex reasoning tasks like math problems, logical reasoning, and multi-step decision-making processes.\n",
    "\n",
    "In the next part, we'll explore function calling and tool use with JSON schemas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
