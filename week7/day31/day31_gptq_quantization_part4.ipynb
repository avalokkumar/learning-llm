{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 31: Advanced Quantization with GPTQ - Part 4\n",
    "\n",
    "In this notebook, we'll explore GPTQ (Generative Pre-trained Transformer Quantization), an advanced technique for quantizing large language models with minimal quality degradation.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Understanding GPTQ\n",
    "2. Setup and dependencies\n",
    "3. Implementing GPTQ quantization\n",
    "4. Evaluating GPTQ model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding GPTQ\n",
    "\n",
    "GPTQ (Generative Pre-trained Transformer Quantization) is a one-shot weight quantization method specifically designed for large language models. It uses a layer-by-layer approach with error redistribution to minimize the impact of quantization on model quality.\n",
    "\n",
    "Key features of GPTQ:\n",
    "1. Quantizes one layer at a time, preserving the overall model structure\n",
    "2. Uses second-order information (Hessian matrix) to minimize quantization error\n",
    "3. Redistributes quantization errors to unquantized weights\n",
    "4. Achieves high compression rates (INT4) with minimal quality degradation\n",
    "5. No need for retraining or fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Dependencies\n",
    "\n",
    "We'll use the `optimum` library with the `auto-gptq` backend, which provides an implementation of GPTQ for Hugging Face models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers accelerate optimum auto-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Print PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "Let's define some helper functions to measure inference time and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to measure inference time\n",
    "def measure_inference_time(model, tokenizer, prompt, num_runs=5):\n",
    "    \"\"\"Measure average inference time over multiple runs\"\"\"\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Warm-up run\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(**inputs, max_length=50)\n",
    "    \n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            _ = model.generate(**inputs, max_length=50)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / num_runs\n",
    "    return avg_time\n",
    "\n",
    "# Function to get GPU memory usage\n",
    "def get_gpu_memory():\n",
    "    \"\"\"Get current GPU memory usage in MB\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / (1024 * 1024)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing GPTQ Quantization\n",
    "\n",
    "We'll use the `optimum` library with `auto-gptq` to quantize a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "try:\n",
    "    from optimum.gptq import GPTQQuantizer, load_quantized_model\n",
    "    from transformers import AutoModelForCausalLM\n",
    "    print(\"GPTQ libraries imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing GPTQ libraries: {e}\")\n",
    "    print(\"Please make sure optimum and auto-gptq are installed correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model name\n",
    "model_name = \"facebook/opt-350m\"  # Using a smaller model for demonstration\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record initial GPU memory\n",
    "initial_memory = get_gpu_memory()\n",
    "print(f\"Initial GPU memory usage: {initial_memory:.2f} MB\")\n",
    "\n",
    "# Load model for GPTQ quantization\n",
    "try:\n",
    "    print(\"Loading model for GPTQ quantization...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    print(\"Model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Creating a Calibration Dataset\n",
    "\n",
    "GPTQ requires a calibration dataset to compute the quantization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small calibration dataset\n",
    "calibration_data = [\n",
    "    \"Artificial intelligence has revolutionized many industries.\",\n",
    "    \"The future of technology depends on sustainable innovation.\",\n",
    "    \"Machine learning models require large amounts of data for training.\",\n",
    "    \"Climate change presents significant challenges for our planet.\",\n",
    "    \"Quantum computing may solve problems that are currently intractable.\"\n",
    "]\n",
    "\n",
    "# Tokenize the calibration data\n",
    "tokenized_data = [tokenizer(text, return_tensors=\"pt\").input_ids.to(device) for text in calibration_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Quantizing the Model with GPTQ\n",
    "\n",
    "Now we'll quantize the model using the GPTQ algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = \"./gptq-model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize the GPTQ quantizer\n",
    "try:\n",
    "    print(\"Initializing GPTQ quantizer...\")\n",
    "    quantizer = GPTQQuantizer(\n",
    "        bits=4,  # 4-bit quantization\n",
    "        dataset=tokenized_data,  # Calibration dataset\n",
    "        block_name_to_quantize=\"model.decoder.layers\",  # Target blocks to quantize\n",
    "        model_seqlen=2048  # Maximum sequence length\n",
    "    )\n",
    "    print(\"GPTQ quantizer initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing quantizer: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize the model with GPTQ\n",
    "try:\n",
    "    print(\"Quantizing model with GPTQ...\")\n",
    "    quantized_model = quantizer.quantize_model(model, tokenizer)\n",
    "    \n",
    "    # Save the quantized model\n",
    "    quantized_model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model quantized and saved to {output_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during quantization: {e}\")\n",
    "    print(\"Skipping quantization step due to error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Loading the Quantized Model\n",
    "\n",
    "Now let's load the quantized model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory before loading the quantized model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Reset memory baseline\n",
    "initial_memory = get_gpu_memory()\n",
    "print(f\"GPU memory after cleanup: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the quantized model\n",
    "try:\n",
    "    print(\"Loading GPTQ quantized model...\")\n",
    "    gptq_model = load_quantized_model(output_dir, device_map=\"auto\")\n",
    "    print(\"GPTQ quantized model loaded successfully\")\n",
    "    \n",
    "    # Calculate memory usage\n",
    "    gptq_memory = get_gpu_memory() - initial_memory\n",
    "    print(f\"GPTQ model GPU memory usage: {gptq_memory:.2f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading quantized model: {e}\")\n",
    "    print(\"Falling back to standard model for comparison...\")\n",
    "    gptq_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "    gptq_memory = get_gpu_memory() - initial_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating GPTQ Model Performance\n",
    "\n",
    "Let's evaluate the performance of our GPTQ quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test prompt\n",
    "prompt = \"Artificial intelligence will transform the future by\"\n",
    "\n",
    "# Measure inference time\n",
    "gptq_time = measure_inference_time(gptq_model, tokenizer, prompt)\n",
    "print(f\"GPTQ model average inference time: {gptq_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with the GPTQ model\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(gptq_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = gptq_model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "# Decode the generated text\n",
    "gptq_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated text with GPTQ model:\")\n",
    "print(gptq_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing with FP16 Model\n",
    "\n",
    "Let's load an FP16 model for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory before loading the FP16 model\n",
    "del gptq_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Reset memory baseline\n",
    "initial_memory = get_gpu_memory()\n",
    "print(f\"GPU memory after cleanup: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FP16 model\n",
    "print(\"Loading model in FP16...\")\n",
    "fp16_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "# Calculate memory usage\n",
    "fp16_memory = get_gpu_memory() - initial_memory\n",
    "print(f\"FP16 model GPU memory usage: {fp16_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure FP16 inference time\n",
    "fp16_time = measure_inference_time(fp16_model, tokenizer, prompt)\n",
    "print(f\"FP16 average inference time: {fp16_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with the FP16 model\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(fp16_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = fp16_model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "# Decode the generated text\n",
    "fp16_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated text with FP16 model:\")\n",
    "print(fp16_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing Results\n",
    "\n",
    "Let's compile and visualize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results\n",
    "results = {\n",
    "    \"Model\": [\"FP16\", \"GPTQ (4-bit)\"],\n",
    "    \"Memory Usage (MB)\": [fp16_memory, gptq_memory],\n",
    "    \"Inference Time (s)\": [fp16_time, gptq_time],\n",
    "    \"Memory Reduction\": [\"1.0x\", f\"{fp16_memory/gptq_memory:.2f}x\"],\n",
    "    \"Speed Improvement\": [\"1.0x\", f\"{fp16_time/gptq_time:.2f}x\"]\n",
    "}\n",
    "\n",
    "# Display results as a table\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot memory usage comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(results[\"Model\"], results[\"Memory Usage (MB)\"], color=[\"blue\", \"green\"])\n",
    "plt.title(\"Memory Usage Comparison\")\n",
    "plt.ylabel(\"Memory (MB)\")\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Plot inference time comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(results[\"Model\"], results[\"Inference Time (s)\"], color=[\"blue\", \"green\"])\n",
    "plt.title(\"Inference Time Comparison\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Text Quality Comparison\n",
    "\n",
    "Let's compare the quality of text generated by both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print both generated texts for comparison\n",
    "print(\"FP16 model output:\")\n",
    "print(fp16_text)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "print(\"GPTQ model output:\")\n",
    "print(gptq_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. GPTQ vs. Other Quantization Methods\n",
    "\n",
    "Let's compare GPTQ with other quantization methods we've explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table (theoretical values based on literature)\n",
    "comparison = {\n",
    "    \"Method\": [\"FP16\", \"INT8\", \"INT4 (Naive)\", \"GPTQ (INT4)\", \"AWQ (INT4)\"],\n",
    "    \"Bits per Weight\": [16, 8, 4, 4, 4],\n",
    "    \"Memory Reduction\": [\"1.0x\", \"2.0x\", \"4.0x\", \"4.0x\", \"4.0x\"],\n",
    "    \"Quality Preservation\": [\"Excellent\", \"Very Good\", \"Fair\", \"Good\", \"Very Good\"],\n",
    "    \"Complexity\": [\"Low\", \"Low\", \"Low\", \"Medium\", \"Medium\"],\n",
    "    \"Key Advantage\": [\n",
    "        \"Full precision\", \n",
    "        \"Good balance\", \n",
    "        \"Maximum compression\", \n",
    "        \"Error redistribution\", \n",
    "        \"Activation-aware\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display comparison as a table\n",
    "comparison_df = pd.DataFrame(comparison)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored GPTQ (Generative Pre-trained Transformer Quantization), an advanced technique for quantizing large language models to 4-bit precision while maintaining quality. We've seen that:\n",
    "\n",
    "1. GPTQ can significantly reduce model memory usage compared to FP16 models\n",
    "2. The inference speed may improve depending on hardware support for INT4 operations\n",
    "3. The quality of text generation can be preserved even with extreme quantization\n",
    "\n",
    "Key advantages of GPTQ:\n",
    "- Layer-by-layer quantization with error redistribution\n",
    "- Uses second-order information to minimize quantization error\n",
    "- Better quality preservation compared to naive INT4 quantization\n",
    "- No need for retraining or fine-tuning\n",
    "\n",
    "GPTQ is particularly useful for deploying large language models on resource-constrained hardware while maintaining acceptable quality. When compared to other methods like AWQ, GPTQ has its own strengths and may perform better on certain models or tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
