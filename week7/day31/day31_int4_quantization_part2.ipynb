{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 31: INT4 Quantization - Part 2\n",
    "\n",
    "In this notebook, we'll explore INT4 quantization for large language models. INT4 quantization reduces precision even further than INT8, offering greater memory savings but with potential quality trade-offs.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Setup and dependencies\n",
    "2. Loading a pre-trained model\n",
    "3. INT4 quantization with bitsandbytes\n",
    "4. Measuring model size and memory usage\n",
    "5. Comparing inference latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers datasets evaluate accelerate bitsandbytes psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import psutil\n",
    "import gc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Print PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n",
    "\n",
    "Let's define some helper functions to measure memory usage and model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to measure memory usage\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    memory_mb = memory_info.rss / (1024 * 1024)  # Convert to MB\n",
    "    return memory_mb\n",
    "\n",
    "# Function to count parameters\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count the number of parameters in a model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Function to calculate model size in MB\n",
    "def calculate_model_size(model, bits_per_param):\n",
    "    \"\"\"Calculate model size in MB based on parameter count and bits per parameter\"\"\"\n",
    "    num_params = count_parameters(model)\n",
    "    size_bytes = num_params * bits_per_param / 8  # Convert bits to bytes\n",
    "    size_mb = size_bytes / (1024 * 1024)  # Convert to MB\n",
    "    return size_mb\n",
    "\n",
    "# Function to measure inference time\n",
    "def measure_inference_time(model, tokenizer, prompt, num_runs=5):\n",
    "    \"\"\"Measure average inference time over multiple runs\"\"\"\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Warm-up run\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(**inputs, max_length=50)\n",
    "    \n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            _ = model.generate(**inputs, max_length=50)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / num_runs\n",
    "    return avg_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading a Pre-trained Model\n",
    "\n",
    "We'll use a small language model for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model name\n",
    "model_name = \"facebook/opt-350m\"  # Using a smaller model for demonstration\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record initial memory usage\n",
    "initial_memory = get_memory_usage()\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")\n",
    "\n",
    "# Load model in FP16 (half precision) for comparison\n",
    "print(\"Loading model in FP16...\")\n",
    "fp16_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "# Calculate model size and memory usage\n",
    "fp16_params = count_parameters(fp16_model)\n",
    "fp16_size = calculate_model_size(fp16_model, 16)  # 16 bits per parameter\n",
    "fp16_memory = get_memory_usage() - initial_memory\n",
    "\n",
    "print(f\"FP16 model parameters: {fp16_params:,}\")\n",
    "print(f\"FP16 model size: {fp16_size:.2f} MB\")\n",
    "print(f\"FP16 model memory usage: {fp16_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. INT4 Quantization with bitsandbytes\n",
    "\n",
    "We'll use the bitsandbytes library through Hugging Face's integration to load the model in 4-bit precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory before loading the INT4 model\n",
    "del fp16_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Reset memory baseline\n",
    "initial_memory = get_memory_usage()\n",
    "print(f\"Memory after cleanup: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                # Load model in 4-bit precision\n",
    "    bnb_4bit_use_double_quant=True,   # Use double quantization for 4-bit\n",
    "    bnb_4bit_quant_type=\"nf4\",        # Use NF4 format (normalized float 4)\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Compute in FP16\n",
    ")\n",
    "\n",
    "# Load model with INT4 quantization\n",
    "print(\"Loading model with INT4 quantization...\")\n",
    "int4_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Calculate INT4 model size and memory usage\n",
    "int4_params = count_parameters(int4_model)\n",
    "int4_size = calculate_model_size(int4_model, 4)  # 4 bits per parameter\n",
    "int4_memory = get_memory_usage() - initial_memory\n",
    "\n",
    "print(f\"INT4 model parameters: {int4_params:,}\")\n",
    "print(f\"INT4 model size: {int4_size:.2f} MB\")\n",
    "print(f\"INT4 model memory usage: {int4_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loading INT8 Model for Comparison\n",
    "\n",
    "Let's also load an INT8 model to compare with our INT4 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory before loading the INT8 model\n",
    "del int4_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Reset memory baseline\n",
    "initial_memory = get_memory_usage()\n",
    "print(f\"Memory after cleanup: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure 8-bit quantization\n",
    "bnb_config_8bit = BitsAndBytesConfig(\n",
    "    load_in_8bit=True  # Load model in 8-bit precision\n",
    ")\n",
    "\n",
    "# Load model with INT8 quantization\n",
    "print(\"Loading model with INT8 quantization...\")\n",
    "int8_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config_8bit,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Calculate INT8 model size and memory usage\n",
    "int8_params = count_parameters(int8_model)\n",
    "int8_size = calculate_model_size(int8_model, 8)  # 8 bits per parameter\n",
    "int8_memory = get_memory_usage() - initial_memory\n",
    "\n",
    "print(f\"INT8 model parameters: {int8_params:,}\")\n",
    "print(f\"INT8 model size: {int8_size:.2f} MB\")\n",
    "print(f\"INT8 model memory usage: {int8_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reload INT4 Model for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory before reloading the INT4 model\n",
    "del int8_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Reset memory baseline\n",
    "initial_memory = get_memory_usage()\n",
    "print(f\"Memory after cleanup: {initial_memory:.2f} MB\")\n",
    "\n",
    "# Reload INT4 model\n",
    "print(\"Reloading model with INT4 quantization...\")\n",
    "int4_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reload FP16 Model for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory before reloading the FP16 model\n",
    "del int4_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Reset memory baseline\n",
    "initial_memory = get_memory_usage()\n",
    "print(f\"Memory after cleanup: {initial_memory:.2f} MB\")\n",
    "\n",
    "# Reload FP16 model\n",
    "print(\"Reloading model in FP16...\")\n",
    "fp16_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparing Inference Latency\n",
    "\n",
    "Let's measure and compare the inference latency of the different precision models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test prompt\n",
    "prompt = \"Artificial intelligence will transform the future by\"\n",
    "\n",
    "# Measure FP16 inference time\n",
    "fp16_time = measure_inference_time(fp16_model, tokenizer, prompt)\n",
    "print(f\"FP16 average inference time: {fp16_time:.4f} seconds\")\n",
    "\n",
    "# Clear memory\n",
    "del fp16_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load INT8 model\n",
    "int8_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config_8bit,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Measure INT8 inference time\n",
    "int8_time = measure_inference_time(int8_model, tokenizer, prompt)\n",
    "print(f\"INT8 average inference time: {int8_time:.4f} seconds\")\n",
    "\n",
    "# Clear memory\n",
    "del int8_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load INT4 model\n",
    "int4_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Measure INT4 inference time\n",
    "int4_time = measure_inference_time(int4_model, tokenizer, prompt)\n",
    "print(f\"INT4 average inference time: {int4_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparing Text Generation Quality\n",
    "\n",
    "Let's compare the quality of text generation across different precision models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with the INT4 model\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(int4_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = int4_model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "# Decode the generated text\n",
    "int4_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated text with INT4 model:\")\n",
    "print(int4_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparing Results\n",
    "\n",
    "Let's compile and visualize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results\n",
    "results = {\n",
    "    \"Precision\": [\"FP16\", \"INT8\", \"INT4\"],\n",
    "    \"Model Size (MB)\": [fp16_size, int8_size, int4_size],\n",
    "    \"Memory Usage (MB)\": [fp16_memory, int8_memory, int4_memory],\n",
    "    \"Inference Time (s)\": [fp16_time, int8_time, int4_time],\n",
    "    \"Size Reduction\": [\"1.0x\", f\"{fp16_size/int8_size:.2f}x\", f\"{fp16_size/int4_size:.2f}x\"],\n",
    "    \"Memory Reduction\": [\"1.0x\", f\"{fp16_memory/int8_memory:.2f}x\", f\"{fp16_memory/int4_memory:.2f}x\"],\n",
    "    \"Speed Improvement\": [\"1.0x\", f\"{fp16_time/int8_time:.2f}x\", f\"{fp16_time/int4_time:.2f}x\"]\n",
    "}\n",
    "\n",
    "# Display results as a table\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot model size comparison\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(results[\"Precision\"], results[\"Model Size (MB)\"], color=[\"blue\", \"green\", \"red\"])\n",
    "plt.title(\"Model Size Comparison\")\n",
    "plt.ylabel(\"Size (MB)\")\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Plot memory usage comparison\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(results[\"Precision\"], results[\"Memory Usage (MB)\"], color=[\"blue\", \"green\", \"red\"])\n",
    "plt.title(\"Memory Usage Comparison\")\n",
    "plt.ylabel(\"Memory (MB)\")\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Plot inference time comparison\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(results[\"Precision\"], results[\"Inference Time (s)\"], color=[\"blue\", \"green\", \"red\"])\n",
    "plt.title(\"Inference Time Comparison\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored INT4 quantization using the bitsandbytes library and compared it with FP16 and INT8 models. We've seen that:\n",
    "\n",
    "1. INT4 quantization provides significant memory savings compared to FP16 and INT8 models\n",
    "2. The inference speed improvements can vary depending on hardware support for INT4 operations\n",
    "3. The quality of text generation may be affected, but often remains acceptable for many use cases\n",
    "\n",
    "Key takeaways:\n",
    "- INT4 quantization can reduce model size by approximately 4x compared to FP16\n",
    "- The memory usage reduction can enable running larger models on limited hardware\n",
    "- The trade-off between quality and efficiency needs to be evaluated for each specific use case\n",
    "\n",
    "In the next part, we'll explore more advanced quantization techniques like AWQ and GPTQ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
