{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 31: Advanced Quantization with AWQ - Part 3\n",
    "\n",
    "In this notebook, we'll explore Activation-aware Weight Quantization (AWQ), an advanced technique for quantizing large language models with minimal quality degradation.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Understanding AWQ\n",
    "2. Setup and dependencies\n",
    "3. Implementing AWQ quantization\n",
    "4. Evaluating AWQ model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding AWQ\n",
    "\n",
    "Activation-aware Weight Quantization (AWQ) is an advanced quantization technique that preserves the weights that have the most impact on activations. The key insight of AWQ is that not all weights in a model are equally important - some weights have a much larger impact on the model's outputs than others.\n",
    "\n",
    "AWQ works by:\n",
    "1. Identifying which weight channels are most important for activations\n",
    "2. Applying per-channel scaling to preserve these important weights\n",
    "3. Quantizing the model to INT4 precision\n",
    "\n",
    "This approach allows for extreme compression (4-bit quantization) while maintaining model quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Dependencies\n",
    "\n",
    "We'll use the AutoAWQ library, which provides an implementation of AWQ for Hugging Face models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers accelerate autoawq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Print PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "Let's define some helper functions to measure inference time and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to measure inference time\n",
    "def measure_inference_time(model, tokenizer, prompt, num_runs=5):\n",
    "    \"\"\"Measure average inference time over multiple runs\"\"\"\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Warm-up run\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(**inputs, max_length=50)\n",
    "    \n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            _ = model.generate(**inputs, max_length=50)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / num_runs\n",
    "    return avg_time\n",
    "\n",
    "# Function to get GPU memory usage\n",
    "def get_gpu_memory():\n",
    "    \"\"\"Get current GPU memory usage in MB\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / (1024 * 1024)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing AWQ Quantization\n",
    "\n",
    "We'll use the AutoAWQ library to quantize a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import AutoAWQ\n",
    "try:\n",
    "    from awq import AutoAWQForCausalLM\n",
    "    print(\"AutoAWQ imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"Failed to import AutoAWQ. Please make sure it's installed correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model name\n",
    "model_name = \"facebook/opt-350m\"  # Using a smaller model for demonstration\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record initial GPU memory\n",
    "initial_memory = get_gpu_memory()\n",
    "print(f\"Initial GPU memory usage: {initial_memory:.2f} MB\")\n",
    "\n",
    "# Load model for AWQ quantization\n",
    "try:\n",
    "    # Load model with AutoAWQ\n",
    "    print(\"Loading model for AWQ quantization...\")\n",
    "    awq_model = AutoAWQForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(\"Model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Falling back to standard Hugging Face model loading...\")\n",
    "    from transformers import AutoModelForCausalLM\n",
    "    awq_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Quantizing the Model with AWQ\n",
    "\n",
    "Now we'll quantize the model using the AWQ algorithm. This involves two steps:\n",
    "1. Calibrating the model on a small dataset to identify important weights\n",
    "2. Quantizing the model to 4 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small calibration dataset\n",
    "calibration_data = [\n",
    "    \"Artificial intelligence has revolutionized many industries.\",\n",
    "    \"The future of technology depends on sustainable innovation.\",\n",
    "    \"Machine learning models require large amounts of data for training.\",\n",
    "    \"Climate change presents significant challenges for our planet.\",\n",
    "    \"Quantum computing may solve problems that are currently intractable.\"\n",
    "]\n",
    "\n",
    "# Define quantization parameters\n",
    "quant_config = {\n",
    "    \"zero_point\": True,  # Use zero-point quantization\n",
    "    \"q_group_size\": 128,  # Group size for quantization\n",
    "    \"w_bit\": 4,  # 4-bit quantization\n",
    "    \"version\": \"GEMM\"  # Use GEMM version for inference\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize the model with AWQ\n",
    "try:\n",
    "    # Create output directory\n",
    "    output_dir = \"./awq-model\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Quantize the model\n",
    "    print(\"Quantizing model with AWQ...\")\n",
    "    awq_model.quantize(\n",
    "        tokenizer=tokenizer,\n",
    "        quant_config=quant_config,\n",
    "        calibration_data=calibration_data,\n",
    "        export_path=output_dir\n",
    "    )\n",
    "    print(f\"Model quantized and saved to {output_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during quantization: {e}\")\n",
    "    print(\"Skipping quantization step due to error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Loading the Quantized Model\n",
    "\n",
    "Now let's load the quantized model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory before loading the quantized model\n",
    "del awq_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Reset memory baseline\n",
    "initial_memory = get_gpu_memory()\n",
    "print(f\"GPU memory after cleanup: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the quantized model\n",
    "try:\n",
    "    print(\"Loading AWQ quantized model...\")\n",
    "    quantized_model = AutoAWQForCausalLM.from_quantized(\n",
    "        \"./awq-model\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(\"AWQ quantized model loaded successfully\")\n",
    "    \n",
    "    # Calculate memory usage\n",
    "    awq_memory = get_gpu_memory() - initial_memory\n",
    "    print(f\"AWQ model GPU memory usage: {awq_memory:.2f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading quantized model: {e}\")\n",
    "    print(\"Falling back to standard model for comparison...\")\n",
    "    from transformers import AutoModelForCausalLM\n",
    "    quantized_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "    awq_memory = get_gpu_memory() - initial_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating AWQ Model Performance\n",
    "\n",
    "Let's evaluate the performance of our AWQ quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test prompt\n",
    "prompt = \"Artificial intelligence will transform the future by\"\n",
    "\n",
    "# Measure inference time\n",
    "awq_time = measure_inference_time(quantized_model, tokenizer, prompt)\n",
    "print(f\"AWQ model average inference time: {awq_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with the AWQ model\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(quantized_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = quantized_model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "# Decode the generated text\n",
    "awq_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated text with AWQ model:\")\n",
    "print(awq_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing with FP16 Model\n",
    "\n",
    "Let's load an FP16 model for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory before loading the FP16 model\n",
    "del quantized_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Reset memory baseline\n",
    "initial_memory = get_gpu_memory()\n",
    "print(f\"GPU memory after cleanup: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FP16 model\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "print(\"Loading model in FP16...\")\n",
    "fp16_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "# Calculate memory usage\n",
    "fp16_memory = get_gpu_memory() - initial_memory\n",
    "print(f\"FP16 model GPU memory usage: {fp16_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure FP16 inference time\n",
    "fp16_time = measure_inference_time(fp16_model, tokenizer, prompt)\n",
    "print(f\"FP16 average inference time: {fp16_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with the FP16 model\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(fp16_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = fp16_model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "# Decode the generated text\n",
    "fp16_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated text with FP16 model:\")\n",
    "print(fp16_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing Results\n",
    "\n",
    "Let's compile and visualize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results\n",
    "results = {\n",
    "    \"Model\": [\"FP16\", \"AWQ (4-bit)\"],\n",
    "    \"Memory Usage (MB)\": [fp16_memory, awq_memory],\n",
    "    \"Inference Time (s)\": [fp16_time, awq_time],\n",
    "    \"Memory Reduction\": [\"1.0x\", f\"{fp16_memory/awq_memory:.2f}x\"],\n",
    "    \"Speed Improvement\": [\"1.0x\", f\"{fp16_time/awq_time:.2f}x\"]\n",
    "}\n",
    "\n",
    "# Display results as a table\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot memory usage comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(results[\"Model\"], results[\"Memory Usage (MB)\"], color=[\"blue\", \"red\"])\n",
    "plt.title(\"Memory Usage Comparison\")\n",
    "plt.ylabel(\"Memory (MB)\")\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Plot inference time comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(results[\"Model\"], results[\"Inference Time (s)\"], color=[\"blue\", \"red\"])\n",
    "plt.title(\"Inference Time Comparison\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Text Quality Comparison\n",
    "\n",
    "Let's compare the quality of text generated by both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print both generated texts for comparison\n",
    "print(\"FP16 model output:\")\n",
    "print(fp16_text)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "print(\"AWQ model output:\")\n",
    "print(awq_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored Activation-aware Weight Quantization (AWQ), an advanced technique for quantizing large language models to 4-bit precision while maintaining quality. We've seen that:\n",
    "\n",
    "1. AWQ can significantly reduce model memory usage compared to FP16 models\n",
    "2. The inference speed may improve depending on hardware support for INT4 operations\n",
    "3. The quality of text generation can be preserved even with extreme quantization\n",
    "\n",
    "Key advantages of AWQ:\n",
    "- More sophisticated than simple quantization, preserving important weights\n",
    "- Better quality preservation compared to naive INT4 quantization\n",
    "- No need for retraining or fine-tuning\n",
    "\n",
    "AWQ is particularly useful for deploying large language models on resource-constrained hardware while maintaining acceptable quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
