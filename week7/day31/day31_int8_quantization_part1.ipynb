{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 31: INT8 Quantization - Part 1\n",
    "\n",
    "In this notebook, we'll explore INT8 quantization for large language models. We'll focus on implementing basic INT8 quantization using PyTorch and measuring its impact on model size, latency, and quality.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Setup and dependencies\n",
    "2. Loading a pre-trained model\n",
    "3. Basic INT8 quantization with PyTorch\n",
    "4. Measuring model size and memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers datasets evaluate accelerate psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import psutil\n",
    "import gc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Print PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading a Pre-trained Model\n",
    "\n",
    "We'll use a small language model for demonstration purposes. In practice, quantization is most beneficial for larger models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model name\n",
    "model_name = \"facebook/opt-350m\"  # Using a smaller model for demonstration\n",
    "\n",
    "# Function to measure memory usage\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    memory_mb = memory_info.rss / (1024 * 1024)  # Convert to MB\n",
    "    return memory_mb\n",
    "\n",
    "# Function to count parameters\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count the number of parameters in a model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Function to calculate model size in MB\n",
    "def calculate_model_size(model, dtype=torch.float32):\n",
    "    \"\"\"Calculate model size in MB based on parameter count and dtype\"\"\"\n",
    "    bytes_per_element = {\n",
    "        torch.float32: 4,\n",
    "        torch.float16: 2,\n",
    "        torch.int8: 1\n",
    "    }\n",
    "    num_params = count_parameters(model)\n",
    "    size_bytes = num_params * bytes_per_element.get(dtype, 4)\n",
    "    size_mb = size_bytes / (1024 * 1024)  # Convert to MB\n",
    "    return size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record initial memory usage\n",
    "initial_memory = get_memory_usage()\n",
    "print(f\"Initial memory usage: {initial_memory:.2f} MB\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "\n",
    "# Load model in FP32 (default precision)\n",
    "print(\"Loading model in FP32...\")\n",
    "fp32_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Calculate model size and memory usage\n",
    "fp32_params = count_parameters(fp32_model)\n",
    "fp32_size = calculate_model_size(fp32_model, torch.float32)\n",
    "fp32_memory = get_memory_usage() - initial_memory\n",
    "\n",
    "print(f\"FP32 model parameters: {fp32_params:,}\")\n",
    "print(f\"FP32 model size: {fp32_size:.2f} MB\")\n",
    "print(f\"FP32 model memory usage: {fp32_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic INT8 Quantization with PyTorch\n",
    "\n",
    "PyTorch provides built-in support for INT8 quantization. We'll use the `torch.quantization` module to quantize our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create a FP16 version for comparison\n",
    "print(\"Converting model to FP16...\")\n",
    "fp16_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "# Calculate FP16 model size and memory usage\n",
    "fp16_params = count_parameters(fp16_model)\n",
    "fp16_size = calculate_model_size(fp16_model, torch.float16)\n",
    "fp16_memory = get_memory_usage() - initial_memory - fp32_memory\n",
    "\n",
    "print(f\"FP16 model parameters: {fp16_params:,}\")\n",
    "print(f\"FP16 model size: {fp16_size:.2f} MB\")\n",
    "print(f\"FP16 model memory usage: {fp16_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory before loading the INT8 model\n",
    "del fp32_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Reset memory baseline\n",
    "initial_memory = get_memory_usage()\n",
    "print(f\"Memory after cleanup: {initial_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with INT8 quantization\n",
    "print(\"Loading model with INT8 quantization...\")\n",
    "int8_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True  # Enable INT8 quantization\n",
    ")\n",
    "\n",
    "# Calculate INT8 model size and memory usage\n",
    "int8_params = count_parameters(int8_model)\n",
    "int8_size = calculate_model_size(int8_model, torch.int8)\n",
    "int8_memory = get_memory_usage() - initial_memory\n",
    "\n",
    "print(f\"INT8 model parameters: {int8_params:,}\")\n",
    "print(f\"INT8 model size: {int8_size:.2f} MB\")\n",
    "print(f\"INT8 model memory usage: {int8_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Measuring Model Size and Memory Usage\n",
    "\n",
    "Let's compare the size and memory usage of the different precision models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results\n",
    "results = {\n",
    "    \"Precision\": [\"FP32\", \"FP16\", \"INT8\"],\n",
    "    \"Parameters\": [fp32_params, fp16_params, int8_params],\n",
    "    \"Model Size (MB)\": [fp32_size, fp16_size, int8_size],\n",
    "    \"Memory Usage (MB)\": [fp32_memory, fp16_memory, int8_memory],\n",
    "    \"Size Reduction\": [\"1.0x\", f\"{fp32_size/fp16_size:.2f}x\", f\"{fp32_size/int8_size:.2f}x\"],\n",
    "    \"Memory Reduction\": [\"1.0x\", f\"{fp32_memory/fp16_memory:.2f}x\", f\"{fp32_memory/int8_memory:.2f}x\"]\n",
    "}\n",
    "\n",
    "# Display results as a table\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot model size comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(results[\"Precision\"], results[\"Model Size (MB)\"], color=[\"blue\", \"green\", \"red\"])\n",
    "plt.title(\"Model Size Comparison\")\n",
    "plt.ylabel(\"Size (MB)\")\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Plot memory usage comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(results[\"Precision\"], results[\"Memory Usage (MB)\"], color=[\"blue\", \"green\", \"red\"])\n",
    "plt.title(\"Memory Usage Comparison\")\n",
    "plt.ylabel(\"Memory (MB)\")\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Basic Inference Test\n",
    "\n",
    "Let's perform a basic inference test to ensure our quantized model is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test prompt\n",
    "prompt = \"Artificial intelligence will transform the future by\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(int8_model.device)\n",
    "\n",
    "# Generate text with the INT8 model\n",
    "with torch.no_grad():\n",
    "    outputs = int8_model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated text with INT8 model:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with the FP16 model for comparison\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(fp16_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = fp16_model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated text with FP16 model:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored basic INT8 quantization using PyTorch's built-in functionality. We've seen that:\n",
    "\n",
    "1. INT8 quantization significantly reduces model size and memory usage compared to FP32 and FP16 models\n",
    "2. The quantized model can still generate coherent text\n",
    "\n",
    "In the next part, we'll explore more advanced quantization techniques and measure the impact on inference latency and quality metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
