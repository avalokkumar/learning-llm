{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 35: Metrics Collection for LLM Services - Part 2\n",
    "\n",
    "Implementing comprehensive metrics collection for LLM services to monitor performance, usage, and system health.\n",
    "\n",
    "## Overview\n",
    "1. Setting up Prometheus metrics\n",
    "2. LLM-specific metrics\n",
    "3. Custom metrics dashboard\n",
    "4. Alerting thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q prometheus-client psutil matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import psutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from prometheus_client import Counter, Histogram, Gauge, Summary, start_http_server\n",
    "from collections import defaultdict, deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up Prometheus Metrics\n",
    "\n",
    "Define comprehensive metrics for LLM services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMMetrics:\n",
    "    \"\"\"Prometheus metrics for LLM services.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Request metrics\n",
    "        self.requests_total = Counter(\n",
    "            'llm_requests_total',\n",
    "            'Total number of LLM requests',\n",
    "            ['tenant_id', 'model_name', 'status']\n",
    "        )\n",
    "        \n",
    "        self.request_duration = Histogram(\n",
    "            'llm_request_duration_seconds',\n",
    "            'Request duration in seconds',\n",
    "            ['tenant_id', 'model_name'],\n",
    "            buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]\n",
    "        )\n",
    "        \n",
    "        # Token metrics\n",
    "        self.tokens_processed = Counter(\n",
    "            'llm_tokens_processed_total',\n",
    "            'Total tokens processed',\n",
    "            ['tenant_id', 'model_name', 'type']  # type: input/output\n",
    "        )\n",
    "        \n",
    "        self.tokens_per_second = Gauge(\n",
    "            'llm_tokens_per_second',\n",
    "            'Current tokens per second throughput',\n",
    "            ['model_name']\n",
    "        )\n",
    "        \n",
    "        # Queue metrics\n",
    "        self.queue_length = Gauge(\n",
    "            'llm_queue_length',\n",
    "            'Current queue length',\n",
    "            ['model_name']\n",
    "        )\n",
    "        \n",
    "        self.queue_wait_time = Histogram(\n",
    "            'llm_queue_wait_seconds',\n",
    "            'Time spent waiting in queue',\n",
    "            ['model_name']\n",
    "        )\n",
    "        \n",
    "        # Cost metrics\n",
    "        self.cost_total = Counter(\n",
    "            'llm_cost_usd_total',\n",
    "            'Total cost in USD',\n",
    "            ['tenant_id', 'model_name']\n",
    "        )\n",
    "        \n",
    "        # System metrics\n",
    "        self.gpu_utilization = Gauge(\n",
    "            'llm_gpu_utilization_percent',\n",
    "            'GPU utilization percentage',\n",
    "            ['gpu_id']\n",
    "        )\n",
    "        \n",
    "        self.memory_usage = Gauge(\n",
    "            'llm_memory_usage_bytes',\n",
    "            'Memory usage in bytes',\n",
    "            ['type']  # type: gpu/cpu\n",
    "        )\n",
    "        \n",
    "        # Error metrics\n",
    "        self.errors_total = Counter(\n",
    "            'llm_errors_total',\n",
    "            'Total number of errors',\n",
    "            ['tenant_id', 'model_name', 'error_type']\n",
    "        )\n",
    "\n",
    "# Initialize metrics\n",
    "metrics = LLMMetrics()\n",
    "print(\"Prometheus metrics initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Instrumented LLM Service\n",
    "\n",
    "Create an LLM service with comprehensive metrics collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstrumentedLLMService:\n",
    "    \"\"\"LLM service with comprehensive metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"gpt-3.5-turbo\"):\n",
    "        self.model_name = model_name\n",
    "        self.metrics = metrics\n",
    "        self.request_queue = deque()\n",
    "        self.active_requests = 0\n",
    "        \n",
    "        # Start system metrics collection\n",
    "        self._start_system_metrics()\n",
    "    \n",
    "    def _start_system_metrics(self):\n",
    "        \"\"\"Start background system metrics collection.\"\"\"\n",
    "        def collect_system_metrics():\n",
    "            while True:\n",
    "                # CPU memory\n",
    "                memory = psutil.virtual_memory()\n",
    "                self.metrics.memory_usage.labels(type='cpu').set(memory.used)\n",
    "                \n",
    "                # Mock GPU metrics (in production, use nvidia-ml-py)\n",
    "                gpu_util = np.random.uniform(40, 95)\n",
    "                gpu_memory = np.random.uniform(2e9, 8e9)  # 2-8GB\n",
    "                \n",
    "                self.metrics.gpu_utilization.labels(gpu_id='0').set(gpu_util)\n",
    "                self.metrics.memory_usage.labels(type='gpu').set(gpu_memory)\n",
    "                \n",
    "                # Queue length\n",
    "                self.metrics.queue_length.labels(model_name=self.model_name).set(len(self.request_queue))\n",
    "                \n",
    "                time.sleep(5)  # Update every 5 seconds\n",
    "        \n",
    "        thread = threading.Thread(target=collect_system_metrics, daemon=True)\n",
    "        thread.start()\n",
    "    \n",
    "    def generate(self, prompt: str, tenant_id: str, max_tokens: int = 100) -> dict:\n",
    "        \"\"\"Generate text with metrics collection.\"\"\"\n",
    "        \n",
    "        # Queue management\n",
    "        queue_start = time.time()\n",
    "        self.request_queue.append({'tenant_id': tenant_id, 'start_time': queue_start})\n",
    "        \n",
    "        # Simulate queue processing\n",
    "        time.sleep(0.1)  # Queue wait time\n",
    "        request_info = self.request_queue.popleft()\n",
    "        \n",
    "        queue_wait = time.time() - queue_start\n",
    "        self.metrics.queue_wait_time.labels(model_name=self.model_name).observe(queue_wait)\n",
    "        \n",
    "        # Start request processing\n",
    "        start_time = time.time()\n",
    "        self.active_requests += 1\n",
    "        \n",
    "        try:\n",
    "            # Input validation\n",
    "            if not prompt.strip():\n",
    "                self.metrics.errors_total.labels(\n",
    "                    tenant_id=tenant_id,\n",
    "                    model_name=self.model_name,\n",
    "                    error_type='validation_error'\n",
    "                ).inc()\n",
    "                raise ValueError(\"Empty prompt\")\n",
    "            \n",
    "            # Token counting\n",
    "            input_tokens = len(prompt.split())\n",
    "            self.metrics.tokens_processed.labels(\n",
    "                tenant_id=tenant_id,\n",
    "                model_name=self.model_name,\n",
    "                type='input'\n",
    "            ).inc(input_tokens)\n",
    "            \n",
    "            # Simulate generation\n",
    "            generation_time = max_tokens * 0.01\n",
    "            time.sleep(generation_time)\n",
    "            \n",
    "            # Generate response\n",
    "            output_tokens = min(max_tokens, np.random.randint(10, max_tokens + 1))\n",
    "            generated_text = f\" Generated response with {output_tokens} tokens.\"\n",
    "            \n",
    "            # Record output tokens\n",
    "            self.metrics.tokens_processed.labels(\n",
    "                tenant_id=tenant_id,\n",
    "                model_name=self.model_name,\n",
    "                type='output'\n",
    "            ).inc(output_tokens)\n",
    "            \n",
    "            # Calculate throughput\n",
    "            total_time = time.time() - start_time\n",
    "            throughput = output_tokens / total_time if total_time > 0 else 0\n",
    "            self.metrics.tokens_per_second.labels(model_name=self.model_name).set(throughput)\n",
    "            \n",
    "            # Calculate cost\n",
    "            total_tokens = input_tokens + output_tokens\n",
    "            cost = total_tokens * 0.001  # $0.001 per token\n",
    "            self.metrics.cost_total.labels(\n",
    "                tenant_id=tenant_id,\n",
    "                model_name=self.model_name\n",
    "            ).inc(cost)\n",
    "            \n",
    "            # Record successful request\n",
    "            self.metrics.requests_total.labels(\n",
    "                tenant_id=tenant_id,\n",
    "                model_name=self.model_name,\n",
    "                status='success'\n",
    "            ).inc()\n",
    "            \n",
    "            # Record duration\n",
    "            duration = time.time() - start_time\n",
    "            self.metrics.request_duration.labels(\n",
    "                tenant_id=tenant_id,\n",
    "                model_name=self.model_name\n",
    "            ).observe(duration)\n",
    "            \n",
    "            return {\n",
    "                'text': generated_text,\n",
    "                'usage': {\n",
    "                    'prompt_tokens': input_tokens,\n",
    "                    'completion_tokens': output_tokens,\n",
    "                    'total_tokens': total_tokens\n",
    "                },\n",
    "                'cost': cost,\n",
    "                'duration': duration\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Record error\n",
    "            self.metrics.requests_total.labels(\n",
    "                tenant_id=tenant_id,\n",
    "                model_name=self.model_name,\n",
    "                status='error'\n",
    "            ).inc()\n",
    "            \n",
    "            self.metrics.errors_total.labels(\n",
    "                tenant_id=tenant_id,\n",
    "                model_name=self.model_name,\n",
    "                error_type=type(e).__name__\n",
    "            ).inc()\n",
    "            \n",
    "            raise\n",
    "        \n",
    "        finally:\n",
    "            self.active_requests -= 1\n",
    "\n",
    "# Initialize instrumented service\n",
    "llm_service = InstrumentedLLMService()\n",
    "print(\"Instrumented LLM service initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing Metrics Collection\n",
    "\n",
    "Generate some test requests to collect metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_load():\n",
    "    \"\"\"Generate test load to collect metrics.\"\"\"\n",
    "    \n",
    "    test_requests = [\n",
    "        {\"prompt\": \"What is AI?\", \"tenant\": \"tenant_1\", \"tokens\": 20},\n",
    "        {\"prompt\": \"Explain ML\", \"tenant\": \"tenant_2\", \"tokens\": 30},\n",
    "        {\"prompt\": \"Deep learning overview\", \"tenant\": \"tenant_1\", \"tokens\": 25},\n",
    "        {\"prompt\": \"\", \"tenant\": \"tenant_3\", \"tokens\": 15},  # Error case\n",
    "        {\"prompt\": \"Neural networks\", \"tenant\": \"tenant_2\", \"tokens\": 40},\n",
    "    ]\n",
    "    \n",
    "    print(\"Generating test load...\")\n",
    "    \n",
    "    for i, req in enumerate(test_requests):\n",
    "        try:\n",
    "            result = llm_service.generate(\n",
    "                prompt=req[\"prompt\"],\n",
    "                tenant_id=req[\"tenant\"],\n",
    "                max_tokens=req[\"tokens\"]\n",
    "            )\n",
    "            print(f\"Request {i+1}: Success - {result['usage']['total_tokens']} tokens\")\n",
    "        except Exception as e:\n",
    "            print(f\"Request {i+1}: Error - {e}\")\n",
    "        \n",
    "        time.sleep(0.5)  # Small delay between requests\n",
    "    \n",
    "    print(\"Test load generation complete\")\n",
    "\n",
    "# Generate test load\n",
    "generate_test_load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metrics Dashboard\n",
    "\n",
    "Create a simple metrics dashboard using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_dashboard():\n",
    "    \"\"\"Create a simple metrics dashboard.\"\"\"\n",
    "    \n",
    "    # Collect current metric values\n",
    "    from prometheus_client import REGISTRY\n",
    "    \n",
    "    # Get metric samples\n",
    "    metric_data = {}\n",
    "    \n",
    "    for metric_family in REGISTRY.collect():\n",
    "        for sample in metric_family.samples:\n",
    "            metric_data[sample.name] = sample.value\n",
    "    \n",
    "    # Create dashboard\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Request counts by status\n",
    "    success_count = sum(v for k, v in metric_data.items() \n",
    "                       if 'llm_requests_total' in k and 'success' in k)\n",
    "    error_count = sum(v for k, v in metric_data.items() \n",
    "                     if 'llm_requests_total' in k and 'error' in k)\n",
    "    \n",
    "    ax1.pie([success_count, error_count], labels=['Success', 'Error'], \n",
    "            autopct='%1.1f%%', colors=['green', 'red'])\n",
    "    ax1.set_title('Request Success Rate')\n",
    "    \n",
    "    # Token processing by type\n",
    "    input_tokens = sum(v for k, v in metric_data.items() \n",
    "                      if 'llm_tokens_processed_total' in k and 'input' in k)\n",
    "    output_tokens = sum(v for k, v in metric_data.items() \n",
    "                       if 'llm_tokens_processed_total' in k and 'output' in k)\n",
    "    \n",
    "    ax2.bar(['Input Tokens', 'Output Tokens'], [input_tokens, output_tokens], \n",
    "            color=['blue', 'orange'])\n",
    "    ax2.set_title('Token Processing')\n",
    "    ax2.set_ylabel('Token Count')\n",
    "    \n",
    "    # System metrics\n",
    "    gpu_util = metric_data.get('llm_gpu_utilization_percent', 0)\n",
    "    queue_len = metric_data.get('llm_queue_length', 0)\n",
    "    \n",
    "    ax3.bar(['GPU Utilization %', 'Queue Length'], [gpu_util, queue_len], \n",
    "            color=['purple', 'brown'])\n",
    "    ax3.set_title('System Metrics')\n",
    "    \n",
    "    # Cost by tenant\n",
    "    tenant_costs = defaultdict(float)\n",
    "    for k, v in metric_data.items():\n",
    "        if 'llm_cost_usd_total' in k:\n",
    "            # Extract tenant from metric name (simplified)\n",
    "            if 'tenant_1' in k:\n",
    "                tenant_costs['tenant_1'] += v\n",
    "            elif 'tenant_2' in k:\n",
    "                tenant_costs['tenant_2'] += v\n",
    "            elif 'tenant_3' in k:\n",
    "                tenant_costs['tenant_3'] += v\n",
    "    \n",
    "    if tenant_costs:\n",
    "        tenants = list(tenant_costs.keys())\n",
    "        costs = list(tenant_costs.values())\n",
    "        ax4.bar(tenants, costs, color=['cyan', 'magenta', 'yellow'])\n",
    "        ax4.set_title('Cost by Tenant')\n",
    "        ax4.set_ylabel('Cost (USD)')\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'No cost data available', \n",
    "                ha='center', va='center', transform=ax4.transAxes)\n",
    "        ax4.set_title('Cost by Tenant')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n=== Metrics Summary ===\")\n",
    "    print(f\"Total requests: {success_count + error_count}\")\n",
    "    print(f\"Success rate: {success_count/(success_count + error_count)*100:.1f}%\")\n",
    "    print(f\"Total tokens processed: {input_tokens + output_tokens}\")\n",
    "    print(f\"GPU utilization: {gpu_util:.1f}%\")\n",
    "    print(f\"Current queue length: {queue_len}\")\n",
    "    print(f\"Total cost: ${sum(tenant_costs.values()):.4f}\")\n",
    "\n",
    "# Create dashboard\n",
    "create_metrics_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Alerting Configuration\n",
    "\n",
    "Define alerting rules for critical metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prometheus alerting rules\n",
    "alerting_rules = \"\"\"\n",
    "# prometheus_alerts.yml\n",
    "groups:\n",
    "  - name: llm_service_alerts\n",
    "    rules:\n",
    "      # High error rate\n",
    "      - alert: HighErrorRate\n",
    "        expr: |\n",
    "          (\n",
    "            rate(llm_requests_total{status=\"error\"}[5m]) /\n",
    "            rate(llm_requests_total[5m])\n",
    "          ) > 0.1\n",
    "        for: 2m\n",
    "        labels:\n",
    "          severity: warning\n",
    "        annotations:\n",
    "          summary: \"High error rate detected\"\n",
    "          description: \"Error rate is {{ $value | humanizePercentage }} for {{ $labels.model_name }}\"\n",
    "      \n",
    "      # High response time\n",
    "      - alert: HighResponseTime\n",
    "        expr: |\n",
    "          histogram_quantile(0.95, rate(llm_request_duration_seconds_bucket[5m])) > 10\n",
    "        for: 5m\n",
    "        labels:\n",
    "          severity: warning\n",
    "        annotations:\n",
    "          summary: \"High response time detected\"\n",
    "          description: \"95th percentile response time is {{ $value }}s for {{ $labels.model_name }}\"\n",
    "      \n",
    "      # Queue length too high\n",
    "      - alert: HighQueueLength\n",
    "        expr: llm_queue_length > 50\n",
    "        for: 1m\n",
    "        labels:\n",
    "          severity: critical\n",
    "        annotations:\n",
    "          summary: \"Queue length is too high\"\n",
    "          description: \"Queue length is {{ $value }} for {{ $labels.model_name }}\"\n",
    "      \n",
    "      # GPU utilization too high\n",
    "      - alert: HighGPUUtilization\n",
    "        expr: llm_gpu_utilization_percent > 95\n",
    "        for: 3m\n",
    "        labels:\n",
    "          severity: warning\n",
    "        annotations:\n",
    "          summary: \"GPU utilization is very high\"\n",
    "          description: \"GPU {{ $labels.gpu_id }} utilization is {{ $value }}%\"\n",
    "      \n",
    "      # Low throughput\n",
    "      - alert: LowThroughput\n",
    "        expr: llm_tokens_per_second < 10\n",
    "        for: 5m\n",
    "        labels:\n",
    "          severity: warning\n",
    "        annotations:\n",
    "          summary: \"Low token generation throughput\"\n",
    "          description: \"Throughput is {{ $value }} tokens/sec for {{ $labels.model_name }}\"\n",
    "      \n",
    "      # Service down\n",
    "      - alert: ServiceDown\n",
    "        expr: up{job=\"llm-service\"} == 0\n",
    "        for: 1m\n",
    "        labels:\n",
    "          severity: critical\n",
    "        annotations:\n",
    "          summary: \"LLM service is down\"\n",
    "          description: \"LLM service {{ $labels.instance }} is not responding\"\n",
    "\"\"\"\n",
    "\n",
    "# Write alerting rules\n",
    "with open(\"prometheus_alerts.yml\", \"w\") as f:\n",
    "    f.write(alerting_rules)\n",
    "\n",
    "print(\"Prometheus alerting rules created: prometheus_alerts.yml\")\n",
    "print(\"\\nAlert Types:\")\n",
    "print(\"- High error rate (>10% for 2 minutes)\")\n",
    "print(\"- High response time (95th percentile >10s)\")\n",
    "print(\"- High queue length (>50 requests)\")\n",
    "print(\"- High GPU utilization (>95% for 3 minutes)\")\n",
    "print(\"- Low throughput (<10 tokens/sec)\")\n",
    "print(\"- Service down (no response for 1 minute)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Comprehensive metrics collection enables:\n",
    "\n",
    "1. **Performance Monitoring**: Track request duration, throughput, and queue metrics\n",
    "2. **Resource Monitoring**: Monitor GPU, memory, and system utilization\n",
    "3. **Business Metrics**: Track costs, usage by tenant, and token consumption\n",
    "4. **Proactive Alerting**: Detect issues before they impact users\n",
    "\n",
    "**Key Metrics for LLM Services**:\n",
    "- Request rate and success rate\n",
    "- Token processing throughput\n",
    "- Queue length and wait times\n",
    "- GPU utilization and memory usage\n",
    "- Cost per tenant and per request\n",
    "\n",
    "Next, we'll explore distributed tracing for request flow analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
