{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 35: Structured Logging for LLM Services - Part 1\n",
    "\n",
    "Structured logging is essential for observability in production LLM systems. This notebook demonstrates how to implement comprehensive logging for LLM services.\n",
    "\n",
    "## Overview\n",
    "1. Setting up structured logging\n",
    "2. LLM-specific log fields\n",
    "3. Log aggregation and analysis\n",
    "4. Performance impact measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q structlog python-json-logger fastapi uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import logging\n",
    "import structlog\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Optional\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up Structured Logging\n",
    "\n",
    "We'll configure structured logging with JSON output for easy parsing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_structured_logging():\n",
    "    \"\"\"Configure structured logging with JSON output.\"\"\"\n",
    "    \n",
    "    # Configure structlog\n",
    "    structlog.configure(\n",
    "        processors=[\n",
    "            structlog.stdlib.filter_by_level,\n",
    "            structlog.stdlib.add_logger_name,\n",
    "            structlog.stdlib.add_log_level,\n",
    "            structlog.stdlib.PositionalArgumentsFormatter(),\n",
    "            structlog.processors.TimeStamper(fmt=\"iso\"),\n",
    "            structlog.processors.StackInfoRenderer(),\n",
    "            structlog.processors.format_exc_info,\n",
    "            structlog.processors.UnicodeDecoder(),\n",
    "            structlog.processors.JSONRenderer()\n",
    "        ],\n",
    "        context_class=dict,\n",
    "        logger_factory=structlog.stdlib.LoggerFactory(),\n",
    "        wrapper_class=structlog.stdlib.BoundLogger,\n",
    "        cache_logger_on_first_use=True,\n",
    "    )\n",
    "    \n",
    "    # Configure standard logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(message)s\",\n",
    "        stream=None,\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    \n",
    "    return structlog.get_logger()\n",
    "\n",
    "# Initialize structured logger\n",
    "logger = setup_structured_logging()\n",
    "print(\"Structured logging configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LLM-Specific Logging Context\n",
    "\n",
    "Create logging context managers for LLM operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMLogContext:\n",
    "    \"\"\"Context manager for LLM request logging.\"\"\"\n",
    "    \n",
    "    def __init__(self, logger, operation: str, **kwargs):\n",
    "        self.logger = logger\n",
    "        self.operation = operation\n",
    "        self.request_id = str(uuid.uuid4())\n",
    "        self.start_time = None\n",
    "        self.context = {\n",
    "            'request_id': self.request_id,\n",
    "            'operation': operation,\n",
    "            **kwargs\n",
    "        }\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.bound_logger = self.logger.bind(**self.context)\n",
    "        self.bound_logger.info(\"Operation started\", **self.context)\n",
    "        return self.bound_logger\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        duration = time.time() - self.start_time\n",
    "        \n",
    "        if exc_type is None:\n",
    "            self.bound_logger.info(\n",
    "                \"Operation completed\",\n",
    "                duration_ms=duration * 1000,\n",
    "                status=\"success\"\n",
    "            )\n",
    "        else:\n",
    "            self.bound_logger.error(\n",
    "                \"Operation failed\",\n",
    "                duration_ms=duration * 1000,\n",
    "                status=\"error\",\n",
    "                error_type=exc_type.__name__,\n",
    "                error_message=str(exc_val)\n",
    "            )\n",
    "\n",
    "@contextmanager\n",
    "def llm_request_context(logger, tenant_id: str, model_name: str, **kwargs):\n",
    "    \"\"\"Context manager for LLM request logging.\"\"\"\n",
    "    with LLMLogContext(\n",
    "        logger, \n",
    "        \"llm_inference\",\n",
    "        tenant_id=tenant_id,\n",
    "        model_name=model_name,\n",
    "        **kwargs\n",
    "    ) as log:\n",
    "        yield log\n",
    "\n",
    "print(\"LLM logging context managers created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mock LLM Service with Logging\n",
    "\n",
    "Create a mock LLM service that demonstrates comprehensive logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggedLLMService:\n",
    "    \"\"\"Mock LLM service with comprehensive logging.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"gpt-3.5-turbo\"):\n",
    "        self.model_name = model_name\n",
    "        self.logger = structlog.get_logger(\"llm_service\")\n",
    "        self.request_count = 0\n",
    "    \n",
    "    def generate(self, prompt: str, tenant_id: str, max_tokens: int = 100, \n",
    "                temperature: float = 0.7) -> Dict[str, Any]:\n",
    "        \"\"\"Generate text with comprehensive logging.\"\"\"\n",
    "        \n",
    "        self.request_count += 1\n",
    "        \n",
    "        with llm_request_context(\n",
    "            self.logger,\n",
    "            tenant_id=tenant_id,\n",
    "            model_name=self.model_name,\n",
    "            input_tokens=len(prompt.split()),\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            request_number=self.request_count\n",
    "        ) as log:\n",
    "            \n",
    "            # Log input validation\n",
    "            if len(prompt.strip()) == 0:\n",
    "                log.warning(\"Empty prompt received\")\n",
    "                raise ValueError(\"Prompt cannot be empty\")\n",
    "            \n",
    "            if max_tokens <= 0:\n",
    "                log.warning(\"Invalid max_tokens\", max_tokens=max_tokens)\n",
    "                raise ValueError(\"max_tokens must be positive\")\n",
    "            \n",
    "            # Log model loading (mock)\n",
    "            log.info(\"Loading model\", model_name=self.model_name)\n",
    "            time.sleep(0.1)  # Simulate model loading\n",
    "            \n",
    "            # Log tokenization\n",
    "            input_tokens = len(prompt.split())\n",
    "            log.info(\"Tokenization complete\", input_token_count=input_tokens)\n",
    "            \n",
    "            # Log inference start\n",
    "            inference_start = time.time()\n",
    "            log.info(\"Starting inference\")\n",
    "            \n",
    "            # Simulate inference\n",
    "            time.sleep(max_tokens * 0.01)  # Simulate generation time\n",
    "            generated_text = f\" This is a generated response with {max_tokens} tokens.\"\n",
    "            output_tokens = len(generated_text.split())\n",
    "            \n",
    "            inference_time = time.time() - inference_start\n",
    "            \n",
    "            # Log inference completion\n",
    "            log.info(\n",
    "                \"Inference complete\",\n",
    "                output_token_count=output_tokens,\n",
    "                total_tokens=input_tokens + output_tokens,\n",
    "                inference_time_ms=inference_time * 1000,\n",
    "                tokens_per_second=output_tokens / inference_time if inference_time > 0 else 0\n",
    "            )\n",
    "            \n",
    "            # Calculate cost (mock)\n",
    "            cost = (input_tokens + output_tokens) * 0.001\n",
    "            log.info(\"Cost calculated\", cost_usd=cost)\n",
    "            \n",
    "            return {\n",
    "                'text': generated_text,\n",
    "                'usage': {\n",
    "                    'prompt_tokens': input_tokens,\n",
    "                    'completion_tokens': output_tokens,\n",
    "                    'total_tokens': input_tokens + output_tokens\n",
    "                },\n",
    "                'cost': cost,\n",
    "                'inference_time': inference_time\n",
    "            }\n",
    "\n",
    "# Initialize service\n",
    "llm_service = LoggedLLMService()\n",
    "print(\"Logged LLM service initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing the Logged Service\n",
    "\n",
    "Let's test our service and see the structured logs in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test successful request\n",
    "print(\"=== Testing Successful Request ===\")\n",
    "try:\n",
    "    result = llm_service.generate(\n",
    "        prompt=\"What is artificial intelligence?\",\n",
    "        tenant_id=\"tenant_123\",\n",
    "        max_tokens=20,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(f\"\\nResult: {result['text']}\")\n",
    "    print(f\"Usage: {result['usage']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test error case\n",
    "print(\"=== Testing Error Case ===\")\n",
    "try:\n",
    "    result = llm_service.generate(\n",
    "        prompt=\"\",  # Empty prompt\n",
    "        tenant_id=\"tenant_456\",\n",
    "        max_tokens=10\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Expected error caught: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Log Analysis and Aggregation\n",
    "\n",
    "Demonstrate how to analyze the structured logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "class LogAnalyzer:\n",
    "    \"\"\"Analyze structured logs for insights.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logs = []\n",
    "        self.original_stdout = sys.stdout\n",
    "        self.log_capture = io.StringIO()\n",
    "    \n",
    "    def start_capture(self):\n",
    "        \"\"\"Start capturing logs.\"\"\"\n",
    "        sys.stdout = self.log_capture\n",
    "    \n",
    "    def stop_capture(self):\n",
    "        \"\"\"Stop capturing logs and parse them.\"\"\"\n",
    "        sys.stdout = self.original_stdout\n",
    "        log_content = self.log_capture.getvalue()\n",
    "        \n",
    "        # Parse JSON logs\n",
    "        for line in log_content.strip().split('\\n'):\n",
    "            if line.strip():\n",
    "                try:\n",
    "                    log_entry = json.loads(line)\n",
    "                    self.logs.append(log_entry)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    \n",
    "    def analyze(self):\n",
    "        \"\"\"Analyze captured logs.\"\"\"\n",
    "        if not self.logs:\n",
    "            print(\"No logs to analyze\")\n",
    "            return\n",
    "        \n",
    "        # Basic statistics\n",
    "        total_logs = len(self.logs)\n",
    "        log_levels = defaultdict(int)\n",
    "        operations = defaultdict(int)\n",
    "        tenants = defaultdict(int)\n",
    "        \n",
    "        # Performance metrics\n",
    "        durations = []\n",
    "        token_counts = []\n",
    "        costs = []\n",
    "        \n",
    "        for log in self.logs:\n",
    "            # Count log levels\n",
    "            level = log.get('level', 'unknown')\n",
    "            log_levels[level] += 1\n",
    "            \n",
    "            # Count operations\n",
    "            operation = log.get('operation')\n",
    "            if operation:\n",
    "                operations[operation] += 1\n",
    "            \n",
    "            # Count tenants\n",
    "            tenant = log.get('tenant_id')\n",
    "            if tenant:\n",
    "                tenants[tenant] += 1\n",
    "            \n",
    "            # Collect performance metrics\n",
    "            if 'duration_ms' in log:\n",
    "                durations.append(log['duration_ms'])\n",
    "            \n",
    "            if 'total_tokens' in log:\n",
    "                token_counts.append(log['total_tokens'])\n",
    "            \n",
    "            if 'cost_usd' in log:\n",
    "                costs.append(log['cost_usd'])\n",
    "        \n",
    "        # Print analysis\n",
    "        print(f\"=== Log Analysis ===\")\n",
    "        print(f\"Total log entries: {total_logs}\")\n",
    "        \n",
    "        print(f\"\\nLog levels:\")\n",
    "        for level, count in log_levels.items():\n",
    "            print(f\"  {level}: {count}\")\n",
    "        \n",
    "        print(f\"\\nOperations:\")\n",
    "        for op, count in operations.items():\n",
    "            print(f\"  {op}: {count}\")\n",
    "        \n",
    "        print(f\"\\nTenants:\")\n",
    "        for tenant, count in tenants.items():\n",
    "            print(f\"  {tenant}: {count}\")\n",
    "        \n",
    "        if durations:\n",
    "            print(f\"\\nPerformance Metrics:\")\n",
    "            print(f\"  Avg duration: {sum(durations)/len(durations):.2f}ms\")\n",
    "            print(f\"  Max duration: {max(durations):.2f}ms\")\n",
    "            print(f\"  Min duration: {min(durations):.2f}ms\")\n",
    "        \n",
    "        if token_counts:\n",
    "            print(f\"  Avg tokens: {sum(token_counts)/len(token_counts):.1f}\")\n",
    "        \n",
    "        if costs:\n",
    "            print(f\"  Total cost: ${sum(costs):.4f}\")\n",
    "            print(f\"  Avg cost per request: ${sum(costs)/len(costs):.4f}\")\n",
    "\n",
    "# Test log analysis\n",
    "analyzer = LogAnalyzer()\n",
    "analyzer.start_capture()\n",
    "\n",
    "# Generate some test requests\n",
    "test_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain neural networks\",\n",
    "    \"How does AI work?\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    try:\n",
    "        llm_service.generate(\n",
    "            prompt=prompt,\n",
    "            tenant_id=f\"tenant_{i % 2 + 1}\",\n",
    "            max_tokens=15 + i * 5\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "analyzer.stop_capture()\n",
    "analyzer.analyze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Impact of Logging\n",
    "\n",
    "Measure the performance impact of structured logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_logging_impact(num_requests=10):\n",
    "    \"\"\"Benchmark the performance impact of logging.\"\"\"\n",
    "    \n",
    "    # Simple service without logging\n",
    "    class SimpleLLMService:\n",
    "        def generate(self, prompt, max_tokens=100):\n",
    "            time.sleep(max_tokens * 0.01)  # Simulate generation\n",
    "            return f\"Generated response with {max_tokens} tokens\"\n",
    "    \n",
    "    simple_service = SimpleLLMService()\n",
    "    \n",
    "    # Benchmark without logging\n",
    "    start_time = time.time()\n",
    "    for i in range(num_requests):\n",
    "        simple_service.generate(f\"Test prompt {i}\", max_tokens=20)\n",
    "    no_logging_time = time.time() - start_time\n",
    "    \n",
    "    # Benchmark with logging (capture output to avoid printing)\n",
    "    original_stdout = sys.stdout\n",
    "    sys.stdout = io.StringIO()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i in range(num_requests):\n",
    "        try:\n",
    "            llm_service.generate(\n",
    "                prompt=f\"Test prompt {i}\",\n",
    "                tenant_id=\"benchmark_tenant\",\n",
    "                max_tokens=20\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "    with_logging_time = time.time() - start_time\n",
    "    \n",
    "    sys.stdout = original_stdout\n",
    "    \n",
    "    # Calculate overhead\n",
    "    overhead = with_logging_time - no_logging_time\n",
    "    overhead_percent = (overhead / no_logging_time) * 100\n",
    "    \n",
    "    print(f\"=== Logging Performance Impact ===\")\n",
    "    print(f\"Requests: {num_requests}\")\n",
    "    print(f\"Without logging: {no_logging_time:.3f}s\")\n",
    "    print(f\"With logging: {with_logging_time:.3f}s\")\n",
    "    print(f\"Overhead: {overhead:.3f}s ({overhead_percent:.1f}%)\")\n",
    "    print(f\"Per-request overhead: {(overhead/num_requests)*1000:.2f}ms\")\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_logging_impact(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Production Logging Configuration\n",
    "\n",
    "Create a production-ready logging configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production logging configuration\n",
    "production_logging_config = \"\"\"\n",
    "# production_logging.py\n",
    "import os\n",
    "import logging\n",
    "import structlog\n",
    "from pythonjsonlogger import jsonlogger\n",
    "\n",
    "def setup_production_logging():\n",
    "    \"\"\"Setup production-ready structured logging.\"\"\"\n",
    "    \n",
    "    # Get log level from environment\n",
    "    log_level = os.getenv('LOG_LEVEL', 'INFO').upper()\n",
    "    \n",
    "    # Configure JSON formatter\n",
    "    json_formatter = jsonlogger.JsonFormatter(\n",
    "        fmt='%(asctime)s %(name)s %(levelname)s %(message)s',\n",
    "        datefmt='%Y-%m-%dT%H:%M:%S'\n",
    "    )\n",
    "    \n",
    "    # Configure handler\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(json_formatter)\n",
    "    \n",
    "    # Configure root logger\n",
    "    logging.basicConfig(\n",
    "        level=getattr(logging, log_level),\n",
    "        handlers=[handler],\n",
    "        format='%(message)s'\n",
    "    )\n",
    "    \n",
    "    # Configure structlog\n",
    "    structlog.configure(\n",
    "        processors=[\n",
    "            structlog.stdlib.filter_by_level,\n",
    "            structlog.stdlib.add_logger_name,\n",
    "            structlog.stdlib.add_log_level,\n",
    "            structlog.processors.TimeStamper(fmt=\"iso\"),\n",
    "            structlog.processors.add_log_level,\n",
    "            structlog.processors.JSONRenderer()\n",
    "        ],\n",
    "        context_class=dict,\n",
    "        logger_factory=structlog.stdlib.LoggerFactory(),\n",
    "        wrapper_class=structlog.stdlib.BoundLogger,\n",
    "        cache_logger_on_first_use=True,\n",
    "    )\n",
    "    \n",
    "    return structlog.get_logger()\n",
    "\n",
    "# Environment variables for configuration\n",
    "LOGGING_CONFIG = {\n",
    "    'LOG_LEVEL': 'INFO',\n",
    "    'LOG_FORMAT': 'json',\n",
    "    'LOG_OUTPUT': 'stdout',\n",
    "    'SERVICE_NAME': 'llm-service',\n",
    "    'SERVICE_VERSION': '1.0.0',\n",
    "    'ENVIRONMENT': 'production'\n",
    "}\n",
    "\n",
    "# Logging best practices:\n",
    "# 1. Use structured logging with consistent field names\n",
    "# 2. Include correlation IDs for request tracing\n",
    "# 3. Log at appropriate levels (DEBUG, INFO, WARN, ERROR)\n",
    "# 4. Include performance metrics in logs\n",
    "# 5. Sanitize sensitive information\n",
    "# 6. Use log sampling for high-volume operations\n",
    "# 7. Configure log rotation and retention\n",
    "\"\"\"\n",
    "\n",
    "# Write production config\n",
    "with open(\"production_logging.py\", \"w\") as f:\n",
    "    f.write(production_logging_config)\n",
    "\n",
    "print(\"Production logging configuration created: production_logging.py\")\n",
    "print(\"\\nKey Features:\")\n",
    "print(\"- JSON structured output\")\n",
    "print(\"- Environment-based configuration\")\n",
    "print(\"- Correlation ID support\")\n",
    "print(\"- Performance metrics logging\")\n",
    "print(\"- Error context preservation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Structured logging provides essential observability for LLM services:\n",
    "\n",
    "1. **Consistent Format**: JSON logs enable easy parsing and analysis\n",
    "2. **Rich Context**: Request IDs, tenant info, and performance metrics\n",
    "3. **Error Tracking**: Comprehensive error logging with context\n",
    "4. **Performance Monitoring**: Built-in timing and resource usage tracking\n",
    "\n",
    "**Best Practices**:\n",
    "- Use correlation IDs for request tracing\n",
    "- Include relevant business context\n",
    "- Monitor logging performance impact\n",
    "- Implement log sampling for high-volume operations\n",
    "\n",
    "Next, we'll explore metrics collection and monitoring for LLM services."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
