{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 35: Telemetry Integration for LLM Services - Part 5\n",
    "\n",
    "Integrating logging, metrics, and tracing into a cohesive telemetry system for comprehensive observability.\n",
    "\n",
    "## Overview\n",
    "1. Unified telemetry configuration\n",
    "2. Instrumented service with all three pillars\n",
    "3. Correlation between logs, metrics, and traces\n",
    "4. Production-ready telemetry setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q structlog prometheus-client opentelemetry-api opentelemetry-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import uuid\n",
    "import json\n",
    "import logging\n",
    "import structlog\n",
    "from typing import Dict, Any\n",
    "\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter\n",
    "from opentelemetry.trace import Status, StatusCode\n",
    "\n",
    "from prometheus_client import Counter, Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Unified Telemetry Setup\n",
    "\n",
    "Configure logging, metrics, and tracing in a single setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TelemetrySystem:\n",
    "    \"\"\"Unified telemetry system for LLM services.\"\"\"\n",
    "    \n",
    "    def __init__(self, service_name: str):\n",
    "        self.service_name = service_name\n",
    "        self.logger = self._setup_logging()\n",
    "        self.tracer = self._setup_tracing()\n",
    "        self.metrics = self._setup_metrics()\n",
    "        print(\"Telemetry system initialized\")\n",
    "    \n",
    "    def _setup_logging(self):\n",
    "        \"\"\"Configure structured logging.\"\"\"\n",
    "        structlog.configure(\n",
    "            processors=[\n",
    "                structlog.stdlib.filter_by_level,\n",
    "                structlog.processors.TimeStamper(fmt=\"iso\"),\n",
    "                self._add_trace_info,  # Add trace/span IDs\n",
    "                structlog.processors.JSONRenderer()\n",
    "            ],\n",
    "            logger_factory=structlog.stdlib.LoggerFactory(),\n",
    "            wrapper_class=structlog.stdlib.BoundLogger,\n",
    "        )\n",
    "        logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "        return structlog.get_logger(self.service_name)\n",
    "    \n",
    "    def _setup_tracing(self):\n",
    "        \"\"\"Configure OpenTelemetry tracing.\"\"\"\n",
    "        trace.set_tracer_provider(TracerProvider())\n",
    "        trace.get_tracer_provider().add_span_processor(\n",
    "            SimpleSpanProcessor(ConsoleSpanExporter())\n",
    "        )\n",
    "        return trace.get_tracer(self.service_name)\n",
    "    \n",
    "    def _setup_metrics(self):\n",
    "        \"\"\"Configure Prometheus metrics.\"\"\"\n",
    "        return {\n",
    "            'requests_total': Counter(\n",
    "                'llm_requests_total', 'Total requests', [\"model\", \"status\"]\n",
    "            ),\n",
    "            'request_duration': Histogram(\n",
    "                'llm_request_duration_seconds', 'Request duration', [\"model\"]\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    def _add_trace_info(self, logger, method_name, event_dict):\n",
    "        \"\"\"Add trace and span IDs to logs.\"\"\"\n",
    "        current_span = trace.get_current_span()\n",
    "        if current_span.is_recording():\n",
    "            span_context = current_span.get_span_context()\n",
    "            event_dict['trace_id'] = format(span_context.trace_id, '032x')\n",
    "            event_dict['span_id'] = format(span_context.span_id, '016x')\n",
    "        return event_dict\n",
    "\n",
    "# Initialize telemetry system\n",
    "telemetry = TelemetrySystem(\"llm-service\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fully Instrumented Service\n",
    "\n",
    "Create a service that uses all three observability pillars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyInstrumentedService:\n",
    "    \"\"\"LLM service with integrated telemetry.\"\"\"\n",
    "    \n",
    "    def __init__(self, telemetry_system: TelemetrySystem):\n",
    "        self.telemetry = telemetry_system\n",
    "        self.model_name = \"gpt-3.5-turbo\"\n",
    "    \n",
    "    def generate(self, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate text with integrated telemetry.\"\"\"\n",
    "        \n",
    "        # Start trace span\n",
    "        with self.telemetry.tracer.start_as_current_span(\n",
    "            \"generate_request\",\n",
    "            attributes={\"model\": self.model_name, \"prompt_length\": len(prompt)}\n",
    "        ) as span:\n",
    "            \n",
    "            start_time = time.time()\n",
    "            request_id = str(uuid.uuid4())\n",
    "            \n",
    "            # Bind context to logger\n",
    "            log = self.telemetry.logger.bind(request_id=request_id, model=self.model_name)\n",
    "            \n",
    "            try:\n",
    "                # Log request start\n",
    "                log.info(\"Request received\", prompt=prompt)\n",
    "                \n",
    "                # Simulate work\n",
    "                if not prompt.strip():\n",
    "                    raise ValueError(\"Prompt is empty\")\n",
    "                \n",
    "                time.sleep(0.2)  # Simulate processing\n",
    "                generated_text = f\"Response for: {prompt[:20]}...\"\n",
    "                \n",
    "                # Record success metrics\n",
    "                self.telemetry.metrics['requests_total'].labels(\n",
    "                    model=self.model_name, status='success'\n",
    "                ).inc()\n",
    "                \n",
    "                # Log success\n",
    "                log.info(\"Request successful\", generated_text=generated_text)\n",
    "                span.set_status(Status(StatusCode.OK))\n",
    "                \n",
    "                return {'text': generated_text}\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Record error metrics\n",
    "                self.telemetry.metrics['requests_total'].labels(\n",
    "                    model=self.model_name, status='error'\n",
    "                ).inc()\n",
    "                \n",
    "                # Log error\n",
    "                log.error(\"Request failed\", error=str(e))\n",
    "                span.record_exception(e)\n",
    "                span.set_status(Status(StatusCode.ERROR, str(e)))\n",
    "                \n",
    "                raise\n",
    "            \n",
    "            finally:\n",
    "                # Record request duration\n",
    "                duration = time.time() - start_time\n",
    "                self.telemetry.metrics['request_duration'].labels(\n",
    "                    model=self.model_name\n",
    "                ).observe(duration)\n",
    "\n",
    "# Initialize service\n",
    "service = FullyInstrumentedService(telemetry)\n",
    "print(\"Fully instrumented service initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing the Integrated System\n",
    "\n",
    "Generate requests and observe the correlated telemetry output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sys\n",
    "\n",
    "def test_telemetry_integration():\n",
    "    \"\"\"Test the integrated telemetry system.\"\"\"\n",
    "    \n",
    "    # Capture log output\n",
    "    log_capture = io.StringIO()\n",
    "    original_stdout = sys.stdout\n",
    "    sys.stdout = log_capture\n",
    "    \n",
    "    # Test successful request\n",
    "    print(\"--- Testing Successful Request ---\")\n",
    "    try:\n",
    "        result = service.generate(\"What is observability?\")\n",
    "        print(f\"Success: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    # Test error request\n",
    "    print(\"\\n--- Testing Error Request ---\")\n",
    "    try:\n",
    "        service.generate(\"\")  # Empty prompt\n",
    "    except Exception as e:\n",
    "        print(f\"Caught expected error: {e}\")\n",
    "    \n",
    "    # Restore stdout and get logs\n",
    "    sys.stdout = original_stdout\n",
    "    log_output = log_capture.getvalue()\n",
    "    \n",
    "    # Print captured logs\n",
    "    print(\"\\n--- Captured Logs ---\")\n",
    "    print(log_output)\n",
    "    \n",
    "    # Analyze logs for correlation\n",
    "    print(\"--- Log Correlation Analysis ---\")\n",
    "    trace_ids = set()\n",
    "    for line in log_output.strip().split('\\n'):\n",
    "        try:\n",
    "            log_entry = json.loads(line)\n",
    "            if 'trace_id' in log_entry:\n",
    "                trace_ids.add(log_entry['trace_id'])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Found {len(trace_ids)} unique trace IDs in logs\")\n",
    "    for trace_id in trace_ids:\n",
    "        print(f\"  Trace ID: {trace_id}\")\n",
    "        print(\"    Logs for this trace:\")\n",
    "        for line in log_output.strip().split('\\n'):\n",
    "            if trace_id in line:\n",
    "                print(f\"      {line}\")\n",
    "\n",
    "# Run the test\n",
    "test_telemetry_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Production Telemetry Architecture\n",
    "\n",
    "Here's a typical production telemetry architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production architecture diagram (Mermaid)\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "mermaid_diagram = \"\"\"\n",
    "graph TD\n",
    "    A[LLM Service] -->|Logs| B[Log Agent]\n",
    "    A -->|Metrics| C[Metrics Agent]\n",
    "    A -->|Traces| D[Trace Agent]\n",
    "    \n",
    "    B --> E[Log Aggregator e.g., Fluentd]\n",
    "    C --> F[Metrics Collector e.g., Prometheus]\n",
    "    D --> G[Trace Collector e.g., Jaeger/OTel Collector]\n",
    "    \n",
    "    E --> H[Log Storage e.g., Elasticsearch]\n",
    "    F --> I[Metrics Storage e.g., Prometheus TSDB]\n",
    "    G --> J[Trace Storage e.g., Jaeger Storage]\n",
    "    \n",
    "    H --> K[Observability Platform e.g., Grafana/Kibana]\n",
    "    I --> K\n",
    "    J --> K\n",
    "    \n",
    "    K --> L[Dashboards]\n",
    "    K --> M[Alerting]\n",
    "    K --> N[Analysis]\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(f\"```mermaid\\n{mermaid_diagram}```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Integrating logging, metrics, and tracing provides comprehensive observability:\n",
    "\n",
    "1. **Unified View**: Correlate logs, metrics, and traces for deep insights\n",
    "2. **Root Cause Analysis**: Quickly identify the cause of issues\n",
    "3. **Performance Optimization**: Find and fix performance bottlenecks\n",
    "4. **Proactive Monitoring**: Set up alerts based on correlated data\n",
    "\n",
    "**Best Practices**:\n",
    "- Use a unified telemetry system like OpenTelemetry\n",
    "- Include correlation IDs (trace/span IDs) in all telemetry data\n",
    "- Set up dashboards that combine logs, metrics, and traces\n",
    "- Automate instrumentation where possible\n",
    "\n",
    "This completes our exploration of observability for LLM services."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
