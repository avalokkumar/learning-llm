{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 35: Distributed Tracing for LLM Services - Part 3\n",
    "\n",
    "Implementing distributed tracing to understand request flows and performance bottlenecks in LLM services.\n",
    "\n",
    "## Overview\n",
    "1. Setting up OpenTelemetry tracing\n",
    "2. Instrumenting LLM operations\n",
    "3. Trace analysis and visualization\n",
    "4. Performance optimization insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q opentelemetry-api opentelemetry-sdk opentelemetry-instrumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import uuid\n",
    "import json\n",
    "from typing import Dict, Any, Optional\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter\n",
    "from opentelemetry.trace import Status, StatusCode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up OpenTelemetry Tracing\n",
    "\n",
    "Configure OpenTelemetry for distributed tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tracing():\n",
    "    \"\"\"Setup OpenTelemetry tracing.\"\"\"\n",
    "    \n",
    "    # Create tracer provider\n",
    "    trace.set_tracer_provider(TracerProvider())\n",
    "    \n",
    "    # Create console exporter for demo\n",
    "    console_exporter = ConsoleSpanExporter()\n",
    "    \n",
    "    # Create span processor\n",
    "    span_processor = BatchSpanProcessor(console_exporter)\n",
    "    \n",
    "    # Add processor to tracer provider\n",
    "    trace.get_tracer_provider().add_span_processor(span_processor)\n",
    "    \n",
    "    # Get tracer\n",
    "    tracer = trace.get_tracer(\"llm-service\", \"1.0.0\")\n",
    "    \n",
    "    return tracer\n",
    "\n",
    "# Initialize tracing\n",
    "tracer = setup_tracing()\n",
    "print(\"OpenTelemetry tracing configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Traced LLM Service\n",
    "\n",
    "Create an LLM service with comprehensive tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TracedLLMService:\n",
    "    \"\"\"LLM service with distributed tracing.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"gpt-3.5-turbo\"):\n",
    "        self.model_name = model_name\n",
    "        self.tracer = tracer\n",
    "    \n",
    "    def generate(self, prompt: str, tenant_id: str, max_tokens: int = 100) -> Dict[str, Any]:\n",
    "        \"\"\"Generate text with distributed tracing.\"\"\"\n",
    "        \n",
    "        # Start root span\n",
    "        with self.tracer.start_as_current_span(\n",
    "            \"llm_generate\",\n",
    "            attributes={\n",
    "                \"llm.model_name\": self.model_name,\n",
    "                \"llm.tenant_id\": tenant_id,\n",
    "                \"llm.max_tokens\": max_tokens,\n",
    "                \"llm.input_length\": len(prompt)\n",
    "            }\n",
    "        ) as span:\n",
    "            \n",
    "            try:\n",
    "                # Input validation span\n",
    "                result = self._validate_input(prompt, max_tokens)\n",
    "                if result:\n",
    "                    return result\n",
    "                \n",
    "                # Tokenization span\n",
    "                input_tokens = self._tokenize(prompt)\n",
    "                \n",
    "                # Model loading span\n",
    "                self._load_model()\n",
    "                \n",
    "                # Inference span\n",
    "                output_tokens, generated_text = self._run_inference(prompt, max_tokens)\n",
    "                \n",
    "                # Post-processing span\n",
    "                result = self._post_process(input_tokens, output_tokens, generated_text)\n",
    "                \n",
    "                # Add result attributes to span\n",
    "                span.set_attributes({\n",
    "                    \"llm.input_tokens\": input_tokens,\n",
    "                    \"llm.output_tokens\": output_tokens,\n",
    "                    \"llm.total_tokens\": input_tokens + output_tokens,\n",
    "                    \"llm.cost\": result['cost']\n",
    "                })\n",
    "                \n",
    "                span.set_status(Status(StatusCode.OK))\n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                span.record_exception(e)\n",
    "                span.set_status(Status(StatusCode.ERROR, str(e)))\n",
    "                raise\n",
    "    \n",
    "    def _validate_input(self, prompt: str, max_tokens: int) -> Optional[Dict]:\n",
    "        \"\"\"Validate input with tracing.\"\"\"\n",
    "        \n",
    "        with self.tracer.start_as_current_span(\n",
    "            \"validate_input\",\n",
    "            attributes={\n",
    "                \"validation.prompt_empty\": not prompt.strip(),\n",
    "                \"validation.max_tokens_valid\": max_tokens > 0\n",
    "            }\n",
    "        ) as span:\n",
    "            \n",
    "            if not prompt.strip():\n",
    "                span.add_event(\"Validation failed: empty prompt\")\n",
    "                span.set_status(Status(StatusCode.ERROR, \"Empty prompt\"))\n",
    "                raise ValueError(\"Prompt cannot be empty\")\n",
    "            \n",
    "            if max_tokens <= 0:\n",
    "                span.add_event(\"Validation failed: invalid max_tokens\")\n",
    "                span.set_status(Status(StatusCode.ERROR, \"Invalid max_tokens\"))\n",
    "                raise ValueError(\"max_tokens must be positive\")\n",
    "            \n",
    "            span.add_event(\"Input validation passed\")\n",
    "            span.set_status(Status(StatusCode.OK))\n",
    "            return None\n",
    "    \n",
    "    def _tokenize(self, prompt: str) -> int:\n",
    "        \"\"\"Tokenize input with tracing.\"\"\"\n",
    "        \n",
    "        with self.tracer.start_as_current_span(\n",
    "            \"tokenize\",\n",
    "            attributes={\"tokenizer.input_length\": len(prompt)}\n",
    "        ) as span:\n",
    "            \n",
    "            # Simulate tokenization\n",
    "            time.sleep(0.01)\n",
    "            tokens = len(prompt.split())\n",
    "            \n",
    "            span.set_attributes({\n",
    "                \"tokenizer.token_count\": tokens,\n",
    "                \"tokenizer.compression_ratio\": len(prompt) / tokens if tokens > 0 else 0\n",
    "            })\n",
    "            \n",
    "            span.add_event(f\"Tokenized {tokens} tokens\")\n",
    "            return tokens\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load model with tracing.\"\"\"\n",
    "        \n",
    "        with self.tracer.start_as_current_span(\n",
    "            \"load_model\",\n",
    "            attributes={\"model.name\": self.model_name}\n",
    "        ) as span:\n",
    "            \n",
    "            # Simulate model loading\n",
    "            time.sleep(0.05)\n",
    "            \n",
    "            span.add_event(\"Model loaded successfully\")\n",
    "            span.set_attributes({\n",
    "                \"model.loaded\": True,\n",
    "                \"model.memory_usage\": \"4.2GB\"  # Mock value\n",
    "            })\n",
    "    \n",
    "    def _run_inference(self, prompt: str, max_tokens: int) -> tuple:\n",
    "        \"\"\"Run inference with tracing.\"\"\"\n",
    "        \n",
    "        with self.tracer.start_as_current_span(\n",
    "            \"inference\",\n",
    "            attributes={\n",
    "                \"inference.max_tokens\": max_tokens,\n",
    "                \"inference.model\": self.model_name\n",
    "            }\n",
    "        ) as span:\n",
    "            \n",
    "            # Simulate inference phases\n",
    "            self._run_attention(prompt)\n",
    "            output_tokens, generated_text = self._generate_tokens(max_tokens)\n",
    "            \n",
    "            span.set_attributes({\n",
    "                \"inference.output_tokens\": output_tokens,\n",
    "                \"inference.tokens_per_second\": output_tokens / 0.5  # Mock calculation\n",
    "            })\n",
    "            \n",
    "            return output_tokens, generated_text\n",
    "    \n",
    "    def _run_attention(self, prompt: str):\n",
    "        \"\"\"Run attention mechanism with tracing.\"\"\"\n",
    "        \n",
    "        with self.tracer.start_as_current_span(\n",
    "            \"attention\",\n",
    "            attributes={\"attention.sequence_length\": len(prompt.split())}\n",
    "        ) as span:\n",
    "            \n",
    "            # Simulate attention computation\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "            span.add_event(\"Attention computation complete\")\n",
    "            span.set_attributes({\n",
    "                \"attention.heads\": 32,\n",
    "                \"attention.layers\": 24\n",
    "            })\n",
    "    \n",
    "    def _generate_tokens(self, max_tokens: int) -> tuple:\n",
    "        \"\"\"Generate tokens with tracing.\"\"\"\n",
    "        \n",
    "        with self.tracer.start_as_current_span(\n",
    "            \"token_generation\",\n",
    "            attributes={\"generation.max_tokens\": max_tokens}\n",
    "        ) as span:\n",
    "            \n",
    "            # Simulate token generation\n",
    "            time.sleep(max_tokens * 0.01)\n",
    "            \n",
    "            output_tokens = min(max_tokens, 20)  # Mock output\n",
    "            generated_text = f\" Generated response with {output_tokens} tokens.\"\n",
    "            \n",
    "            span.set_attributes({\n",
    "                \"generation.actual_tokens\": output_tokens,\n",
    "                \"generation.completion_reason\": \"length\"\n",
    "            })\n",
    "            \n",
    "            return output_tokens, generated_text\n",
    "    \n",
    "    def _post_process(self, input_tokens: int, output_tokens: int, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Post-process results with tracing.\"\"\"\n",
    "        \n",
    "        with self.tracer.start_as_current_span(\n",
    "            \"post_process\",\n",
    "            attributes={\n",
    "                \"post_process.input_tokens\": input_tokens,\n",
    "                \"post_process.output_tokens\": output_tokens\n",
    "            }\n",
    "        ) as span:\n",
    "            \n",
    "            # Calculate cost\n",
    "            total_tokens = input_tokens + output_tokens\n",
    "            cost = total_tokens * 0.001\n",
    "            \n",
    "            span.set_attributes({\n",
    "                \"post_process.total_tokens\": total_tokens,\n",
    "                \"post_process.cost\": cost\n",
    "            })\n",
    "            \n",
    "            return {\n",
    "                'text': text,\n",
    "                'usage': {\n",
    "                    'prompt_tokens': input_tokens,\n",
    "                    'completion_tokens': output_tokens,\n",
    "                    'total_tokens': total_tokens\n",
    "                },\n",
    "                'cost': cost\n",
    "            }\n",
    "\n",
    "# Initialize traced service\n",
    "traced_service = TracedLLMService()\n",
    "print(\"Traced LLM service initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing Distributed Tracing\n",
    "\n",
    "Generate requests to see traces in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tracing():\n",
    "    \"\"\"Test distributed tracing with various scenarios.\"\"\"\n",
    "    \n",
    "    print(\"=== Testing Successful Request ===\")\n",
    "    try:\n",
    "        result = traced_service.generate(\n",
    "            prompt=\"What is artificial intelligence?\",\n",
    "            tenant_id=\"tenant_123\",\n",
    "            max_tokens=30\n",
    "        )\n",
    "        print(f\"Success: {result['usage']['total_tokens']} tokens generated\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    print(\"=== Testing Error Case ===\")\n",
    "    try:\n",
    "        result = traced_service.generate(\n",
    "            prompt=\"\",  # Empty prompt to trigger error\n",
    "            tenant_id=\"tenant_456\",\n",
    "            max_tokens=20\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Expected error: {e}\")\n",
    "\n",
    "# Run tracing test\n",
    "test_tracing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trace Analysis\n",
    "\n",
    "Create a simple trace analyzer to extract insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraceAnalyzer:\n",
    "    \"\"\"Analyze traces for performance insights.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.traces = []\n",
    "    \n",
    "    def add_mock_trace(self, operation: str, duration_ms: float, attributes: dict):\n",
    "        \"\"\"Add a mock trace for analysis.\"\"\"\n",
    "        self.traces.append({\n",
    "            'operation': operation,\n",
    "            'duration_ms': duration_ms,\n",
    "            'attributes': attributes,\n",
    "            'timestamp': time.time()\n",
    "        })\n",
    "    \n",
    "    def analyze_performance(self):\n",
    "        \"\"\"Analyze trace performance.\"\"\"\n",
    "        if not self.traces:\n",
    "            print(\"No traces to analyze\")\n",
    "            return\n",
    "        \n",
    "        # Group by operation\n",
    "        operations = {}\n",
    "        for trace in self.traces:\n",
    "            op = trace['operation']\n",
    "            if op not in operations:\n",
    "                operations[op] = []\n",
    "            operations[op].append(trace['duration_ms'])\n",
    "        \n",
    "        print(\"=== Trace Performance Analysis ===\")\n",
    "        \n",
    "        total_time = 0\n",
    "        for op, durations in operations.items():\n",
    "            avg_duration = sum(durations) / len(durations)\n",
    "            max_duration = max(durations)\n",
    "            min_duration = min(durations)\n",
    "            \n",
    "            print(f\"\\n{op}:\")\n",
    "            print(f\"  Count: {len(durations)}\")\n",
    "            print(f\"  Avg: {avg_duration:.2f}ms\")\n",
    "            print(f\"  Max: {max_duration:.2f}ms\")\n",
    "            print(f\"  Min: {min_duration:.2f}ms\")\n",
    "            \n",
    "            total_time += sum(durations)\n",
    "        \n",
    "        print(f\"\\nTotal trace time: {total_time:.2f}ms\")\n",
    "        \n",
    "        # Find bottlenecks\n",
    "        bottlenecks = []\n",
    "        for op, durations in operations.items():\n",
    "            avg_duration = sum(durations) / len(durations)\n",
    "            if avg_duration > 100:  # Operations taking >100ms\n",
    "                bottlenecks.append((op, avg_duration))\n",
    "        \n",
    "        if bottlenecks:\n",
    "            print(\"\\nPerformance Bottlenecks:\")\n",
    "            for op, duration in sorted(bottlenecks, key=lambda x: x[1], reverse=True):\n",
    "                print(f\"  {op}: {duration:.2f}ms\")\n",
    "        else:\n",
    "            print(\"\\nNo significant bottlenecks detected\")\n",
    "\n",
    "# Create analyzer and add mock traces\n",
    "analyzer = TraceAnalyzer()\n",
    "\n",
    "# Add mock trace data (simulating real traces)\n",
    "mock_traces = [\n",
    "    ('llm_generate', 520.5, {'model': 'gpt-3.5-turbo', 'tokens': 45}),\n",
    "    ('validate_input', 2.1, {'valid': True}),\n",
    "    ('tokenize', 8.3, {'tokens': 15}),\n",
    "    ('load_model', 45.2, {'model': 'gpt-3.5-turbo'}),\n",
    "    ('inference', 380.8, {'tokens': 30}),\n",
    "    ('attention', 120.4, {'layers': 24}),\n",
    "    ('token_generation', 245.1, {'tokens': 30}),\n",
    "    ('post_process', 12.6, {'cost': 0.045})\n",
    "]\n",
    "\n",
    "for op, duration, attrs in mock_traces:\n",
    "    analyzer.add_mock_trace(op, duration, attrs)\n",
    "\n",
    "# Analyze performance\n",
    "analyzer.analyze_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Production Tracing Configuration\n",
    "\n",
    "Create production-ready tracing configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production tracing configuration\n",
    "production_tracing_config = \"\"\"\n",
    "# production_tracing.py\n",
    "import os\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "from opentelemetry.exporter.jaeger.thrift import JaegerExporter\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "\n",
    "def setup_production_tracing():\n",
    "    \"\"\"Setup production tracing with Jaeger.\"\"\"\n",
    "    \n",
    "    # Create resource with service information\n",
    "    resource = Resource.create({\n",
    "        \"service.name\": os.getenv(\"SERVICE_NAME\", \"llm-service\"),\n",
    "        \"service.version\": os.getenv(\"SERVICE_VERSION\", \"1.0.0\"),\n",
    "        \"deployment.environment\": os.getenv(\"ENVIRONMENT\", \"production\")\n",
    "    })\n",
    "    \n",
    "    # Create tracer provider\n",
    "    trace.set_tracer_provider(TracerProvider(resource=resource))\n",
    "    \n",
    "    # Configure Jaeger exporter\n",
    "    jaeger_exporter = JaegerExporter(\n",
    "        agent_host_name=os.getenv(\"JAEGER_AGENT_HOST\", \"localhost\"),\n",
    "        agent_port=int(os.getenv(\"JAEGER_AGENT_PORT\", \"6831\")),\n",
    "    )\n",
    "    \n",
    "    # Create span processor\n",
    "    span_processor = BatchSpanProcessor(\n",
    "        jaeger_exporter,\n",
    "        max_queue_size=2048,\n",
    "        max_export_batch_size=512,\n",
    "        export_timeout_millis=30000,\n",
    "    )\n",
    "    \n",
    "    # Add processor to tracer provider\n",
    "    trace.get_tracer_provider().add_span_processor(span_processor)\n",
    "    \n",
    "    return trace.get_tracer(__name__)\n",
    "\n",
    "# Sampling configuration\n",
    "TRACING_CONFIG = {\n",
    "    'SAMPLING_RATE': float(os.getenv('TRACING_SAMPLING_RATE', '0.1')),  # 10% sampling\n",
    "    'MAX_SPANS_PER_TRACE': int(os.getenv('MAX_SPANS_PER_TRACE', '100')),\n",
    "    'TRACE_TIMEOUT': int(os.getenv('TRACE_TIMEOUT', '30')),  # seconds\n",
    "}\n",
    "\n",
    "# Custom span decorator\n",
    "def trace_operation(operation_name: str, **attributes):\n",
    "    \"\"\"Decorator for tracing operations.\"\"\"\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            tracer = trace.get_tracer(__name__)\n",
    "            with tracer.start_as_current_span(operation_name) as span:\n",
    "                # Add custom attributes\n",
    "                for key, value in attributes.items():\n",
    "                    span.set_attribute(key, value)\n",
    "                \n",
    "                try:\n",
    "                    result = func(*args, **kwargs)\n",
    "                    span.set_status(trace.Status(trace.StatusCode.OK))\n",
    "                    return result\n",
    "                except Exception as e:\n",
    "                    span.record_exception(e)\n",
    "                    span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n",
    "                    raise\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Usage example:\n",
    "# @trace_operation(\"database_query\", db_type=\"postgresql\")\n",
    "# def query_database(query):\n",
    "#     # Database operation\n",
    "#     pass\n",
    "\"\"\"\n",
    "\n",
    "# Write production config\n",
    "with open(\"production_tracing.py\", \"w\") as f:\n",
    "    f.write(production_tracing_config)\n",
    "\n",
    "print(\"Production tracing configuration created: production_tracing.py\")\n",
    "print(\"\\nKey Features:\")\n",
    "print(\"- Jaeger integration for trace collection\")\n",
    "print(\"- Configurable sampling rates\")\n",
    "print(\"- Resource attribution (service name, version)\")\n",
    "print(\"- Custom span decorator for easy instrumentation\")\n",
    "print(\"- Batch processing for performance\")\n",
    "print(\"- Environment-based configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Distributed tracing provides deep insights into LLM service performance:\n",
    "\n",
    "1. **Request Flow Visibility**: See the complete journey of each request\n",
    "2. **Performance Bottlenecks**: Identify slow operations and optimize them\n",
    "3. **Error Context**: Understand where and why errors occur\n",
    "4. **Dependency Analysis**: Track interactions between services\n",
    "\n",
    "**Key Benefits for LLM Services**:\n",
    "- Understand token generation performance\n",
    "- Identify model loading bottlenecks\n",
    "- Track attention computation time\n",
    "- Analyze end-to-end request latency\n",
    "\n",
    "**Best Practices**:\n",
    "- Use appropriate sampling rates (1-10% in production)\n",
    "- Include relevant business context in spans\n",
    "- Set up proper span relationships (parent-child)\n",
    "- Monitor trace collection performance impact\n",
    "\n",
    "Next, we'll explore GPU monitoring for LLM services."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
