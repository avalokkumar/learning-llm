{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 33: Implementing Streaming API - Part 2\n",
    "\n",
    "In this notebook, we'll implement a streaming API for our LLM server. Streaming is essential for responsive user interfaces, allowing tokens to be displayed as they're generated rather than waiting for the entire response.\n",
    "\n",
    "## Overview\n",
    "1. Understanding streaming generation\n",
    "2. Implementing a streaming endpoint with FastAPI\n",
    "3. Testing the streaming API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Streaming Generation\n",
    "\n",
    "Streaming generation allows tokens to be sent to the client as they're produced, rather than waiting for the entire generation to complete. This provides a more responsive user experience, especially for longer responses.\n",
    "\n",
    "Key benefits of streaming:\n",
    "- Improved perceived latency (time to first token)\n",
    "- Better user experience for chat applications\n",
    "- Ability to cancel generation early if needed\n",
    "- More efficient use of client resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip3 install -q vllm fastapi uvicorn sse-starlette pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import asyncio\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Check if we're running in a notebook\n",
    "try:\n",
    "    get_ipython\n",
    "    is_notebook = True\n",
    "except NameError:\n",
    "    is_notebook = False\n",
    "\n",
    "print(f\"Running in {'notebook' if is_notebook else 'script'} mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing a Streaming Endpoint with FastAPI\n",
    "\n",
    "We'll create a FastAPI server with a streaming endpoint that uses Server-Sent Events (SSE) to stream tokens to the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file for our streaming API server\n",
    "streaming_server_code = \"\"\"\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import asyncio\n",
    "from typing import List, Dict, Any, Optional, AsyncGenerator\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import StreamingResponse, JSONResponse\n",
    "from sse_starlette.sse import EventSourceResponse\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Import vLLM for model serving\n",
    "try:\n",
    "    from vllm import LLM, SamplingParams\n",
    "    VLLM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"vLLM not available. Using mock implementation for demonstration.\")\n",
    "    VLLM_AVAILABLE = False\n",
    "\n",
    "# Define the model name\n",
    "MODEL_NAME = \"facebook/opt-350m\"  # Using a smaller model for demonstration\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(title=\"LLM Streaming API Server\")\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "if VLLM_AVAILABLE:\n",
    "    print(f\"Loading model: {MODEL_NAME}\")\n",
    "    try:\n",
    "        llm = LLM(model=MODEL_NAME)\n",
    "        print(\"Model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        VLLM_AVAILABLE = False\n",
    "else:\n",
    "    llm = None\n",
    "\n",
    "# Define request and response models\n",
    "class StreamingRequest(BaseModel):\n",
    "    prompt: str\n",
    "    max_tokens: int = Field(default=100, ge=1, le=1024)\n",
    "    temperature: float = Field(default=0.7, ge=0.0, le=2.0)\n",
    "    top_p: float = Field(default=0.95, ge=0.0, le=1.0)\n",
    "    stream: bool = Field(default=True)\n",
    "\n",
    "class TokenResponse(BaseModel):\n",
    "    token: str\n",
    "    index: int\n",
    "    finished: bool = False\n",
    "\n",
    "# Mock token generator for demonstration\n",
    "async def mock_token_generator(prompt: str, max_tokens: int) -> AsyncGenerator[str, None]:\n",
    "    \"\"\"Generate tokens one by one for demonstration purposes.\"\"\"\n",
    "    words = [\" AI\", \" is\", \" transforming\", \" the\", \" world\", \" through\", \" innovative\", \" solutions\", \n",
    "             \" that\", \" enhance\", \" productivity\", \" and\", \" creativity\", \".\"]\n",
    "    \n",
    "    for i in range(min(max_tokens, len(words))):\n",
    "        await asyncio.sleep(0.3)  # Simulate generation time\n",
    "        yield words[i]\n",
    "\n",
    "# vLLM streaming implementation\n",
    "async def vllm_stream_tokens(prompt: str, sampling_params: SamplingParams) -> AsyncGenerator[str, None]:\n",
    "    \"\"\"Stream tokens from vLLM.\"\"\"\n",
    "    # Start the generation\n",
    "    outputs = llm.generate([prompt], sampling_params, use_tqdm=False)\n",
    "    generated_text = outputs[0].outputs[0].text\n",
    "    \n",
    "    # Simulate streaming by yielding one token at a time\n",
    "    # Note: vLLM doesn't natively support token-by-token streaming,\n",
    "    # so we're simulating it here for demonstration purposes\n",
    "    tokens = generated_text.split()\n",
    "    for token in tokens:\n",
    "        await asyncio.sleep(0.1)  # Simulate generation time\n",
    "        yield f\" {token}\"\n",
    "\n",
    "# Define API endpoints\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"LLM Streaming API Server is running\", \"model\": MODEL_NAME}\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def generate(request: StreamingRequest):\n",
    "    \"\"\"Generate text with or without streaming.\"\"\"\n",
    "    try:\n",
    "        if request.stream:\n",
    "            return await stream_generate(request)\n",
    "        else:\n",
    "            return await complete_generate(request)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "async def complete_generate(request: StreamingRequest):\n",
    "    \"\"\"Generate complete text without streaming.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if VLLM_AVAILABLE:\n",
    "        # Set up sampling parameters\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=request.temperature,\n",
    "            top_p=request.top_p,\n",
    "            max_tokens=request.max_tokens\n",
    "        )\n",
    "        \n",
    "        # Generate text\n",
    "        outputs = llm.generate([request.prompt], sampling_params)\n",
    "        generated_text = outputs[0].outputs[0].text\n",
    "    else:\n",
    "        # Mock generation\n",
    "        await asyncio.sleep(1)  # Simulate processing time\n",
    "        generated_text = \" AI is transforming the world through innovative solutions that enhance productivity and creativity.\"\n",
    "    \n",
    "    # Calculate token counts (approximate)\n",
    "    prompt_tokens = len(request.prompt.split())\n",
    "    completion_tokens = len(generated_text.split())\n",
    "    total_tokens = prompt_tokens + completion_tokens\n",
    "    \n",
    "    # Calculate request time\n",
    "    request_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        \"text\": generated_text,\n",
    "        \"usage\": {\n",
    "            \"prompt_tokens\": prompt_tokens,\n",
    "            \"completion_tokens\": completion_tokens,\n",
    "            \"total_tokens\": total_tokens\n",
    "        },\n",
    "        \"request_time\": request_time\n",
    "    }\n",
    "\n",
    "async def stream_generate(request: StreamingRequest):\n",
    "    \"\"\"Stream generated tokens as Server-Sent Events.\"\"\"\n",
    "    async def event_generator():\n",
    "        token_index = 0\n",
    "        \n",
    "        # Choose the appropriate token generator\n",
    "        if VLLM_AVAILABLE:\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=request.temperature,\n",
    "                top_p=request.top_p,\n",
    "                max_tokens=request.max_tokens\n",
    "            )\n",
    "            token_stream = vllm_stream_tokens(request.prompt, sampling_params)\n",
    "        else:\n",
    "            token_stream = mock_token_generator(request.prompt, request.max_tokens)\n",
    "        \n",
    "        # Stream each token\n",
    "        async for token in token_stream:\n",
    "            response = TokenResponse(\n",
    "                token=token,\n",
    "                index=token_index,\n",
    "                finished=False\n",
    "            )\n",
    "            token_index += 1\n",
    "            yield json.dumps(response.dict())\n",
    "        \n",
    "        # Send final message\n",
    "        final_response = TokenResponse(\n",
    "            token=\"\",\n",
    "            index=token_index,\n",
    "            finished=True\n",
    "        )\n",
    "        yield json.dumps(final_response.dict())\n",
    "    \n",
    "    return EventSourceResponse(event_generator())\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "# Run the server\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\"\"\"\n",
    "\n",
    "# Write the server code to a file\n",
    "with open(\"streaming_server.py\", \"w\") as f:\n",
    "    f.write(streaming_server_code)\n",
    "\n",
    "print(\"Streaming server code written to streaming_server.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing the Streaming API\n",
    "\n",
    "Now let's create a simple client to test our streaming API. We'll use the `requests` library for non-streaming requests and a custom function for streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple HTML client for testing the streaming API\n",
    "html_client = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>LLM Streaming Client</title>\n",
    "    <style>\n",
    "        body { font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }\n",
    "        textarea { width: 100%; height: 100px; margin-bottom: 10px; }\n",
    "        #output { border: 1px solid #ccc; padding: 10px; min-height: 200px; white-space: pre-wrap; }\n",
    "        button { padding: 10px; margin-right: 10px; }\n",
    "        .controls { margin: 20px 0; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>LLM Streaming Client</h1>\n",
    "    \n",
    "    <div>\n",
    "        <textarea id=\"prompt\" placeholder=\"Enter your prompt here...\">Artificial intelligence will transform the future by</textarea>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"controls\">\n",
    "        <button id=\"streamBtn\">Stream Generation</button>\n",
    "        <button id=\"completeBtn\">Complete Generation</button>\n",
    "        <button id=\"clearBtn\">Clear Output</button>\n",
    "    </div>\n",
    "    \n",
    "    <div>\n",
    "        <h3>Output:</h3>\n",
    "        <div id=\"output\"></div>\n",
    "    </div>\n",
    "    \n",
    "    <script>\n",
    "        const API_URL = 'http://localhost:8000/generate';\n",
    "        const promptInput = document.getElementById('prompt');\n",
    "        const outputDiv = document.getElementById('output');\n",
    "        const streamBtn = document.getElementById('streamBtn');\n",
    "        const completeBtn = document.getElementById('completeBtn');\n",
    "        const clearBtn = document.getElementById('clearBtn');\n",
    "        \n",
    "        // Stream generation\n",
    "        streamBtn.addEventListener('click', async () => {\n",
    "            const prompt = promptInput.value.trim();\n",
    "            if (!prompt) return;\n",
    "            \n",
    "            outputDiv.textContent = prompt;\n",
    "            streamBtn.disabled = true;\n",
    "            completeBtn.disabled = true;\n",
    "            \n",
    "            try {\n",
    "                const response = await fetch(API_URL, {\n",
    "                    method: 'POST',\n",
    "                    headers: {\n",
    "                        'Content-Type': 'application/json',\n",
    "                    },\n",
    "                    body: JSON.stringify({\n",
    "                        prompt,\n",
    "                        max_tokens: 100,\n",
    "                        temperature: 0.7,\n",
    "                        top_p: 0.95,\n",
    "                        stream: true\n",
    "                    })\n",
    "                });\n",
    "                \n",
    "                const reader = response.body.getReader();\n",
    "                const decoder = new TextDecoder();\n",
    "                \n",
    "                while (true) {\n",
    "                    const { value, done } = await reader.read();\n",
    "                    if (done) break;\n",
    "                    \n",
    "                    const chunk = decoder.decode(value);\n",
    "                    const lines = chunk.split('\\n');\n",
    "                    \n",
    "                    for (const line of lines) {\n",
    "                        if (line.startsWith('data:')) {\n",
    "                            const data = line.slice(5).trim();\n",
    "                            if (data) {\n",
    "                                try {\n",
    "                                    const tokenData = JSON.parse(data);\n",
    "                                    if (!tokenData.finished) {\n",
    "                                        outputDiv.textContent += tokenData.token;\n",
    "                                    }\n",
    "                                } catch (e) {\n",
    "                                    console.error('Error parsing JSON:', e);\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            } catch (error) {\n",
    "                console.error('Error:', error);\n",
    "                outputDiv.textContent += '\\n\\nError: ' + error.message;\n",
    "            } finally {\n",
    "                streamBtn.disabled = false;\n",
    "                completeBtn.disabled = false;\n",
    "            }\n",
    "        });\n",
    "        \n",
    "        // Complete generation\n",
    "        completeBtn.addEventListener('click', async () => {\n",
    "            const prompt = promptInput.value.trim();\n",
    "            if (!prompt) return;\n",
    "            \n",
    "            outputDiv.textContent = 'Generating...';\n",
    "            streamBtn.disabled = true;\n",
    "            completeBtn.disabled = true;\n",
    "            \n",
    "            try {\n",
    "                const response = await fetch(API_URL, {\n",
    "                    method: 'POST',\n",
    "                    headers: {\n",
    "                        'Content-Type': 'application/json',\n",
    "                    },\n",
    "                    body: JSON.stringify({\n",
    "                        prompt,\n",
    "                        max_tokens: 100,\n",
    "                        temperature: 0.7,\n",
    "                        top_p: 0.95,\n",
    "                        stream: false\n",
    "                    })\n",
    "                });\n",
    "                \n",
    "                const data = await response.json();\n",
    "                outputDiv.textContent = prompt + data.text;\n",
    "            } catch (error) {\n",
    "                console.error('Error:', error);\n",
    "                outputDiv.textContent = 'Error: ' + error.message;\n",
    "            } finally {\n",
    "                streamBtn.disabled = false;\n",
    "                completeBtn.disabled = false;\n",
    "            }\n",
    "        });\n",
    "        \n",
    "        // Clear output\n",
    "        clearBtn.addEventListener('click', () => {\n",
    "            outputDiv.textContent = '';\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Write the HTML client to a file\n",
    "with open(\"streaming_client.html\", \"w\") as f:\n",
    "    f.write(html_client)\n",
    "\n",
    "print(\"HTML client written to streaming_client.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running the Server and Testing\n",
    "\n",
    "To run the server and test the streaming API, follow these steps:\n",
    "\n",
    "1. Run the server in a terminal:\n",
    "```bash\n",
    "python streaming_server.py\n",
    "```\n",
    "\n",
    "2. Open the HTML client in a web browser:\n",
    "```bash\n",
    "# On macOS\n",
    "open streaming_client.html\n",
    "# On Linux\n",
    "xdg-open streaming_client.html\n",
    "```\n",
    "\n",
    "3. Enter a prompt and click \"Stream Generation\" to see tokens appear one by one, or \"Complete Generation\" to get the full response at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python client for testing the streaming API\n",
    "import requests\n",
    "import json\n",
    "import sseclient\n",
    "\n",
    "def test_complete_generation(prompt=\"Artificial intelligence will transform the future by\"):\n",
    "    \"\"\"Test the non-streaming API endpoint.\"\"\"\n",
    "    try:\n",
    "        # Define the API endpoint\n",
    "        url = \"http://localhost:8000/generate\"\n",
    "        \n",
    "        # Define the request payload\n",
    "        payload = {\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 100,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.95,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        # Send the request\n",
    "        response = requests.post(url, json=payload)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(f\"Generated text: {prompt}{result['text']}\")\n",
    "            print(f\"\\nUsage: {result['usage']}\")\n",
    "            print(f\"Request time: {result['request_time']:.2f} seconds\")\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error testing API: {e}\")\n",
    "        print(\"Make sure the server is running.\")\n",
    "\n",
    "# Note: This function requires the sseclient package\n",
    "# !pip install sseclient-py\n",
    "def test_streaming_generation(prompt=\"Artificial intelligence will transform the future by\"):\n",
    "    \"\"\"Test the streaming API endpoint.\"\"\"\n",
    "    try:\n",
    "        # Define the API endpoint\n",
    "        url = \"http://localhost:8000/generate\"\n",
    "        \n",
    "        # Define the request payload\n",
    "        payload = {\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 100,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.95,\n",
    "            \"stream\": True\n",
    "        }\n",
    "        \n",
    "        # Send the request\n",
    "        response = requests.post(url, json=payload, stream=True)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            client = sseclient.SSEClient(response)\n",
    "            \n",
    "            # Print the prompt first\n",
    "            print(prompt, end=\"\", flush=True)\n",
    "            \n",
    "            # Process each event\n",
    "            for event in client.events():\n",
    "                try:\n",
    "                    data = json.loads(event.data)\n",
    "                    if not data[\"finished\"]:\n",
    "                        print(data[\"token\"], end=\"\", flush=True)\n",
    "                    else:\n",
    "                        print(\"\\n\\nGeneration complete.\")\n",
    "                        break\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"\\nError parsing event data: {event.data}\")\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error testing streaming API: {e}\")\n",
    "        print(\"Make sure the server is running.\")\n",
    "\n",
    "# Uncomment to test the API\n",
    "# Note: This requires the server to be running\n",
    "# test_complete_generation()\n",
    "# test_streaming_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Streaming Performance Considerations\n",
    "\n",
    "When implementing streaming in production, consider these factors:\n",
    "\n",
    "1. **Connection Management**: Ensure your server can handle many concurrent SSE connections\n",
    "2. **Backpressure Handling**: Implement mechanisms to handle slow clients\n",
    "3. **Error Handling**: Gracefully handle disconnections and errors\n",
    "4. **Monitoring**: Track streaming-specific metrics like time-to-first-token\n",
    "5. **Load Balancing**: Configure load balancers to support long-lived connections\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we've implemented a streaming API for our LLM server using FastAPI and Server-Sent Events. Streaming provides a more responsive user experience by sending tokens to the client as they're generated.\n",
    "\n",
    "Key takeaways:\n",
    "- Server-Sent Events (SSE) provide a simple way to implement streaming\n",
    "- Streaming significantly improves perceived latency\n",
    "- The implementation can be adapted for different LLM frameworks\n",
    "\n",
    "In the next notebook, we'll explore Text Generation Inference (TGI), another popular framework for LLM serving."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
