{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 33: Text Generation Inference (TGI) - Part 3\n",
    "\n",
    "Text Generation Inference (TGI) is Hugging Face's solution for deploying and serving LLMs in production. It's optimized for performance with Rust backend and supports advanced features like quantization.\n",
    "\n",
    "## Overview\n",
    "1. TGI setup and installation\n",
    "2. Basic model serving with TGI\n",
    "3. Performance comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TGI Setup and Installation\n",
    "\n",
    "TGI is typically deployed using Docker for easy setup and dependency management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Docker is available\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def check_docker():\n",
    "    try:\n",
    "        result = subprocess.run(['docker', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"Docker available: {result.stdout.strip()}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Docker not available\")\n",
    "            return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"Docker not installed\")\n",
    "        return False\n",
    "\n",
    "docker_available = check_docker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TGI Docker Setup\n",
    "\n",
    "We'll create a Docker command to run TGI with a small model for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TGI Docker run script\n",
    "tgi_script = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "# TGI Docker run script\n",
    "MODEL_NAME=\"microsoft/DialoGPT-small\"\n",
    "PORT=8080\n",
    "\n",
    "echo \"Starting TGI server with model: $MODEL_NAME\"\n",
    "echo \"Server will be available at: http://localhost:$PORT\"\n",
    "\n",
    "docker run --gpus all \\\n",
    "    --shm-size 1g \\\n",
    "    -p $PORT:80 \\\n",
    "    -v $HOME/.cache/huggingface:/data \\\n",
    "    ghcr.io/huggingface/text-generation-inference:1.4 \\\n",
    "    --model-id $MODEL_NAME \\\n",
    "    --num-shard 1 \\\n",
    "    --port 80 \\\n",
    "    --quantize bitsandbytes-nf4\n",
    "\"\"\"\n",
    "\n",
    "# Write the script to a file\n",
    "with open(\"run_tgi.sh\", \"w\") as f:\n",
    "    f.write(tgi_script)\n",
    "\n",
    "# Make it executable\n",
    "os.chmod(\"run_tgi.sh\", 0o755)\n",
    "\n",
    "print(\"TGI run script created: run_tgi.sh\")\n",
    "print(\"To run TGI server, execute: ./run_tgi.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TGI Client Implementation\n",
    "\n",
    "Let's create a simple client to interact with the TGI server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "class TGIClient:\n",
    "    def __init__(self, base_url=\"http://localhost:8080\"):\n",
    "        self.base_url = base_url\n",
    "    \n",
    "    def generate(self, prompt, max_new_tokens=50, temperature=0.7, do_sample=True):\n",
    "        \"\"\"Generate text using TGI server.\"\"\"\n",
    "        url = f\"{self.base_url}/generate\"\n",
    "        \n",
    "        payload = {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"do_sample\": do_sample,\n",
    "                \"return_full_text\": False\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, json=payload)\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                return result[\"generated_text\"]\n",
    "            else:\n",
    "                return f\"Error: {response.status_code} - {response.text}\"\n",
    "        except Exception as e:\n",
    "            return f\"Connection error: {e}\"\n",
    "    \n",
    "    def generate_stream(self, prompt, max_new_tokens=50, temperature=0.7):\n",
    "        \"\"\"Generate text with streaming using TGI server.\"\"\"\n",
    "        url = f\"{self.base_url}/generate_stream\"\n",
    "        \n",
    "        payload = {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"do_sample\": True\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, json=payload, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                for line in response.iter_lines():\n",
    "                    if line:\n",
    "                        line = line.decode('utf-8')\n",
    "                        if line.startswith('data:'):\n",
    "                            data = line[5:].strip()\n",
    "                            if data and data != '[DONE]':\n",
    "                                try:\n",
    "                                    token_data = json.loads(data)\n",
    "                                    yield token_data.get('token', {}).get('text', '')\n",
    "                                except json.JSONDecodeError:\n",
    "                                    continue\n",
    "            else:\n",
    "                yield f\"Error: {response.status_code} - {response.text}\"\n",
    "        except Exception as e:\n",
    "            yield f\"Connection error: {e}\"\n",
    "    \n",
    "    def health_check(self):\n",
    "        \"\"\"Check if TGI server is healthy.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.base_url}/health\")\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "# Initialize client\n",
    "tgi_client = TGIClient()\n",
    "print(\"TGI client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing TGI Server\n",
    "\n",
    "Let's test our TGI client (requires TGI server to be running)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tgi_server():\n",
    "    \"\"\"Test TGI server functionality.\"\"\"\n",
    "    \n",
    "    # Check server health\n",
    "    print(\"Checking TGI server health...\")\n",
    "    if not tgi_client.health_check():\n",
    "        print(\"❌ TGI server is not running or not healthy\")\n",
    "        print(\"To start TGI server, run: ./run_tgi.sh\")\n",
    "        return\n",
    "    \n",
    "    print(\"✅ TGI server is healthy\")\n",
    "    \n",
    "    # Test basic generation\n",
    "    print(\"\\n=== Testing Basic Generation ===\")\n",
    "    prompt = \"The future of artificial intelligence is\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = tgi_client.generate(prompt, max_new_tokens=30)\n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {result}\")\n",
    "    print(f\"Generation time: {generation_time:.2f} seconds\")\n",
    "    \n",
    "    # Test streaming generation\n",
    "    print(\"\\n=== Testing Streaming Generation ===\")\n",
    "    prompt = \"Machine learning will transform\"\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"Streaming response: \", end=\"\", flush=True)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for token in tgi_client.generate_stream(prompt, max_new_tokens=20):\n",
    "        print(token, end=\"\", flush=True)\n",
    "        time.sleep(0.05)  # Small delay to show streaming effect\n",
    "    \n",
    "    streaming_time = time.time() - start_time\n",
    "    print(f\"\\nStreaming time: {streaming_time:.2f} seconds\")\n",
    "\n",
    "# Run the test (only if server is available)\n",
    "test_tgi_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TGI Performance Comparison\n",
    "\n",
    "Let's create a simple benchmark to compare TGI performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_tgi(num_requests=5):\n",
    "    \"\"\"Benchmark TGI server performance.\"\"\"\n",
    "    \n",
    "    if not tgi_client.health_check():\n",
    "        print(\"TGI server not available for benchmarking\")\n",
    "        return\n",
    "    \n",
    "    prompts = [\n",
    "        \"Artificial intelligence will\",\n",
    "        \"The benefits of renewable energy include\",\n",
    "        \"Space exploration helps us\",\n",
    "        \"Quantum computing enables\",\n",
    "        \"The future of healthcare involves\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"Benchmarking TGI with {num_requests} requests...\")\n",
    "    \n",
    "    total_time = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for i in range(num_requests):\n",
    "        prompt = prompts[i % len(prompts)]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = tgi_client.generate(prompt, max_new_tokens=25)\n",
    "        request_time = time.time() - start_time\n",
    "        \n",
    "        if not result.startswith(\"Error\"):\n",
    "            tokens = len(result.split())\n",
    "            total_tokens += tokens\n",
    "            total_time += request_time\n",
    "            \n",
    "            print(f\"Request {i+1}: {request_time:.2f}s, {tokens} tokens\")\n",
    "        else:\n",
    "            print(f\"Request {i+1}: {result}\")\n",
    "    \n",
    "    if total_time > 0:\n",
    "        avg_time = total_time / num_requests\n",
    "        throughput = total_tokens / total_time\n",
    "        \n",
    "        print(f\"\\nBenchmark Results:\")\n",
    "        print(f\"Average time per request: {avg_time:.2f}s\")\n",
    "        print(f\"Total tokens generated: {total_tokens}\")\n",
    "        print(f\"Throughput: {throughput:.2f} tokens/second\")\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_tgi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TGI Configuration Options\n",
    "\n",
    "TGI supports various configuration options for optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced TGI configuration script\n",
    "advanced_tgi_script = \"\"\"\n",
    "#!/bin/bash\n",
    "\n",
    "# Advanced TGI configuration\n",
    "MODEL_NAME=\"microsoft/DialoGPT-medium\"\n",
    "PORT=8080\n",
    "MAX_CONCURRENT_REQUESTS=128\n",
    "MAX_BEST_OF=4\n",
    "MAX_STOP_SEQUENCES=4\n",
    "MAX_INPUT_LENGTH=1024\n",
    "MAX_TOTAL_TOKENS=2048\n",
    "\n",
    "echo \"Starting TGI server with advanced configuration...\"\n",
    "\n",
    "docker run --gpus all \\\n",
    "    --shm-size 1g \\\n",
    "    -p $PORT:80 \\\n",
    "    -v $HOME/.cache/huggingface:/data \\\n",
    "    ghcr.io/huggingface/text-generation-inference:1.4 \\\n",
    "    --model-id $MODEL_NAME \\\n",
    "    --num-shard 1 \\\n",
    "    --port 80 \\\n",
    "    --max-concurrent-requests $MAX_CONCURRENT_REQUESTS \\\n",
    "    --max-best-of $MAX_BEST_OF \\\n",
    "    --max-stop-sequences $MAX_STOP_SEQUENCES \\\n",
    "    --max-input-length $MAX_INPUT_LENGTH \\\n",
    "    --max-total-tokens $MAX_TOTAL_TOKENS \\\n",
    "    --quantize bitsandbytes-nf4 \\\n",
    "    --trust-remote-code\n",
    "\"\"\"\n",
    "\n",
    "# Write advanced script\n",
    "with open(\"run_tgi_advanced.sh\", \"w\") as f:\n",
    "    f.write(advanced_tgi_script)\n",
    "\n",
    "os.chmod(\"run_tgi_advanced.sh\", 0o755)\n",
    "\n",
    "print(\"Advanced TGI configuration created: run_tgi_advanced.sh\")\n",
    "print(\"\\nKey TGI Configuration Options:\")\n",
    "print(\"- --quantize: Enable quantization (bitsandbytes, gptq)\")\n",
    "print(\"- --num-shard: Number of GPU shards for tensor parallelism\")\n",
    "print(\"- --max-concurrent-requests: Maximum concurrent requests\")\n",
    "print(\"- --max-input-length: Maximum input sequence length\")\n",
    "print(\"- --trust-remote-code: Allow custom model code execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Text Generation Inference (TGI) provides:\n",
    "\n",
    "1. **High Performance**: Rust backend with optimized kernels\n",
    "2. **Easy Deployment**: Docker-based deployment\n",
    "3. **Advanced Features**: Quantization, streaming, tensor parallelism\n",
    "4. **HuggingFace Integration**: Seamless model loading from HF Hub\n",
    "\n",
    "TGI is ideal for production deployments requiring high throughput and easy scaling.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To run TGI:\n",
    "1. Execute `./run_tgi.sh` to start the server\n",
    "2. Wait for model loading to complete\n",
    "3. Run the test functions above to verify functionality\n",
    "\n",
    "In the next notebook, we'll explore TensorRT-LLM for NVIDIA GPU optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
