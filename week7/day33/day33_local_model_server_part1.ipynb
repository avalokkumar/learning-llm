{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 33: Deploying a Local Model Server - Part 1\n",
    "\n",
    "In this notebook, we'll set up a local model server using vLLM, one of the most popular frameworks for efficient LLM serving. We'll deploy a model and create a simple API endpoint for text generation.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Setup and dependencies\n",
    "2. Understanding vLLM architecture\n",
    "3. Setting up a local model server\n",
    "4. Creating a basic inference endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "Let's start by installing the necessary packages. vLLM requires PyTorch and several other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q vllm transformers fastapi uvicorn pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check vLLM installation\n",
    "try:\n",
    "    import vllm\n",
    "    print(f\"vLLM version: {vllm.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"vLLM not installed properly. Please check installation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding vLLM Architecture\n",
    "\n",
    "vLLM is a high-performance library for LLM inference and serving. It introduces several key optimizations:\n",
    "\n",
    "1. **PagedAttention**: Memory-efficient KV cache management using a paging mechanism\n",
    "2. **Continuous Batching**: Dynamic handling of requests for optimal throughput\n",
    "3. **Tensor Parallelism**: Distributing model weights across multiple GPUs\n",
    "4. **Optimized CUDA Kernels**: Efficient implementation of key operations\n",
    "\n",
    "These optimizations allow vLLM to achieve significantly higher throughput compared to traditional implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 PagedAttention\n",
    "\n",
    "PagedAttention is the core innovation in vLLM. It applies virtual memory concepts to KV caching:\n",
    "\n",
    "1. Divides the KV cache into fixed-size blocks or \"pages\"\n",
    "2. Allocates pages non-contiguously in physical memory\n",
    "3. Uses a block table to map logical positions to physical memory locations\n",
    "\n",
    "This approach significantly reduces memory fragmentation and enables more efficient memory utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Continuous Batching\n",
    "\n",
    "Continuous batching allows vLLM to dynamically handle requests:\n",
    "\n",
    "1. New requests can join existing batches at any time\n",
    "2. Completed requests can leave batches without disrupting others\n",
    "3. Batches are processed efficiently regardless of varying sequence lengths\n",
    "\n",
    "This approach maximizes GPU utilization and throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting Up a Local Model Server\n",
    "\n",
    "Let's set up a local model server using vLLM. We'll use a smaller model for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model to use\n",
    "model_name = \"facebook/opt-350m\"  # Using a smaller model for demonstration\n",
    "\n",
    "# Check if we're running in a notebook or script\n",
    "try:\n",
    "    get_ipython\n",
    "    is_notebook = True\n",
    "except NameError:\n",
    "    is_notebook = False\n",
    "\n",
    "print(f\"Running in {'notebook' if is_notebook else 'script'} mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Loading a Model with vLLM\n",
    "\n",
    "First, let's load a model using vLLM's `LLM` class to understand the basic usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Initialize the model\n",
    "try:\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    llm = LLM(model=model_name)\n",
    "    print(\"Model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Falling back to CPU mode for demonstration\")\n",
    "    # If GPU loading fails, we'll simulate the API for demonstration\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Basic Inference with vLLM\n",
    "\n",
    "Let's try a simple inference with the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# Test prompt\n",
    "prompt = \"Artificial intelligence will transform the future by\"\n",
    "\n",
    "# Generate text\n",
    "try:\n",
    "    outputs = llm.generate([prompt], sampling_params)\n",
    "    \n",
    "    # Print the generated text\n",
    "    for output in outputs:\n",
    "        generated_text = output.outputs[0].text\n",
    "        print(f\"Generated text: {prompt}{generated_text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during generation: {e}\")\n",
    "    print(\"Falling back to standard Hugging Face generation for demonstration\")\n",
    "    \n",
    "    # Fallback to standard generation if vLLM fails\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95\n",
    "        )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Generated text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating a FastAPI Server for Model Serving\n",
    "\n",
    "Now, let's create a FastAPI server to serve our model. We'll define the API endpoints and request/response models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file for our FastAPI server\n",
    "server_code = \"\"\"\n",
    "import os\n",
    "import time\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Define the model name\n",
    "MODEL_NAME = \"facebook/opt-350m\"  # Using a smaller model for demonstration\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(title=\"vLLM API Server\")\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "llm = LLM(model=MODEL_NAME)\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# Define request and response models\n",
    "class GenerationRequest(BaseModel):\n",
    "    prompt: str\n",
    "    max_tokens: int = Field(default=100, ge=1, le=1024)\n",
    "    temperature: float = Field(default=0.7, ge=0.0, le=2.0)\n",
    "    top_p: float = Field(default=0.95, ge=0.0, le=1.0)\n",
    "    n: int = Field(default=1, ge=1, le=5)\n",
    "\n",
    "class GenerationResponse(BaseModel):\n",
    "    text: str\n",
    "    usage: Dict[str, int]\n",
    "    request_time: float\n",
    "\n",
    "# Define API endpoints\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"vLLM API Server is running\", \"model\": MODEL_NAME}\n",
    "\n",
    "@app.post(\"/generate\", response_model=GenerationResponse)\n",
    "async def generate(request: GenerationRequest):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Set up sampling parameters\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=request.temperature,\n",
    "            top_p=request.top_p,\n",
    "            max_tokens=request.max_tokens,\n",
    "            n=request.n\n",
    "        )\n",
    "        \n",
    "        # Generate text\n",
    "        outputs = llm.generate([request.prompt], sampling_params)\n",
    "        generated_text = outputs[0].outputs[0].text\n",
    "        \n",
    "        # Calculate token counts (approximate)\n",
    "        prompt_tokens = len(request.prompt.split())\n",
    "        completion_tokens = len(generated_text.split())\n",
    "        total_tokens = prompt_tokens + completion_tokens\n",
    "        \n",
    "        # Calculate request time\n",
    "        request_time = time.time() - start_time\n",
    "        \n",
    "        return GenerationResponse(\n",
    "            text=generated_text,\n",
    "            usage={\n",
    "                \"prompt_tokens\": prompt_tokens,\n",
    "                \"completion_tokens\": completion_tokens,\n",
    "                \"total_tokens\": total_tokens\n",
    "            },\n",
    "            request_time=request_time\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "# Run the server\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\"\"\"\n",
    "\n",
    "# Write the server code to a file\n",
    "with open(\"vllm_server.py\", \"w\") as f:\n",
    "    f.write(server_code)\n",
    "\n",
    "print(\"Server code written to vllm_server.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Running the FastAPI Server\n",
    "\n",
    "Now, let's run the FastAPI server. In a notebook, we'll use a background process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the server in the background\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_server():\n",
    "    print(\"Starting vLLM server...\")\n",
    "    try:\n",
    "        # Run the server in a separate process\n",
    "        server_process = subprocess.Popen([\"python\", \"vllm_server.py\"])\n",
    "        print(\"Server started. Waiting for it to initialize...\")\n",
    "        time.sleep(5)  # Give the server some time to start\n",
    "        return server_process\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting server: {e}\")\n",
    "        return None\n",
    "\n",
    "# Only run the server if we're in a script, not in a notebook\n",
    "if not is_notebook:\n",
    "    server_process = run_server()\n",
    "else:\n",
    "    print(\"In notebook mode. To run the server, execute the following in a terminal:\")\n",
    "    print(\"python vllm_server.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Testing the API\n",
    "\n",
    "Let's test our API by sending requests to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def test_api(prompt=\"Artificial intelligence will transform the future by\"):\n",
    "    try:\n",
    "        # Define the API endpoint\n",
    "        url = \"http://localhost:8000/generate\"\n",
    "        \n",
    "        # Define the request payload\n",
    "        payload = {\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 100,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.95,\n",
    "            \"n\": 1\n",
    "        }\n",
    "        \n",
    "        # Send the request\n",
    "        response = requests.post(url, json=payload)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(f\"Generated text: {prompt}{result['text']}\")\n",
    "            print(f\"\\nUsage: {result['usage']}\")\n",
    "            print(f\"Request time: {result['request_time']:.2f} seconds\")\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error testing API: {e}\")\n",
    "        print(\"Make sure the server is running. If you're in a notebook, run the server in a separate terminal.\")\n",
    "\n",
    "# Test the API\n",
    "# Note: This will only work if the server is running\n",
    "test_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Measuring Server Performance\n",
    "\n",
    "Let's measure the performance of our server by sending multiple requests and calculating throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_api(num_requests=5):\n",
    "    try:\n",
    "        # Define the API endpoint\n",
    "        url = \"http://localhost:8000/generate\"\n",
    "        \n",
    "        # Define test prompts\n",
    "        prompts = [\n",
    "            \"Artificial intelligence will transform healthcare by\",\n",
    "            \"The future of renewable energy depends on\",\n",
    "            \"Space exploration in the next decade will focus on\",\n",
    "            \"Quantum computing offers advantages such as\",\n",
    "            \"The most significant ethical concerns in technology are\"\n",
    "        ]\n",
    "        \n",
    "        # Send requests and measure time\n",
    "        start_time = time.time()\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for i in range(num_requests):\n",
    "            prompt = prompts[i % len(prompts)]\n",
    "            \n",
    "            # Define the request payload\n",
    "            payload = {\n",
    "                \"prompt\": prompt,\n",
    "                \"max_tokens\": 50,\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 0.95,\n",
    "                \"n\": 1\n",
    "            }\n",
    "            \n",
    "            # Send the request\n",
    "            response = requests.post(url, json=payload)\n",
    "            \n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                total_tokens += result[\"usage\"][\"total_tokens\"]\n",
    "                print(f\"Request {i+1}/{num_requests} completed in {result['request_time']:.2f} seconds\")\n",
    "            else:\n",
    "                print(f\"Error in request {i+1}: {response.status_code} - {response.text}\")\n",
    "        \n",
    "        # Calculate total time and throughput\n",
    "        total_time = time.time() - start_time\n",
    "        throughput = total_tokens / total_time\n",
    "        \n",
    "        print(f\"\\nBenchmark Results:\")\n",
    "        print(f\"Total requests: {num_requests}\")\n",
    "        print(f\"Total tokens: {total_tokens}\")\n",
    "        print(f\"Total time: {total_time:.2f} seconds\")\n",
    "        print(f\"Throughput: {throughput:.2f} tokens per second\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error benchmarking API: {e}\")\n",
    "        print(\"Make sure the server is running. If you're in a notebook, run the server in a separate terminal.\")\n",
    "\n",
    "# Benchmark the API\n",
    "# Note: This will only work if the server is running\n",
    "benchmark_api(num_requests=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stopping the Server\n",
    "\n",
    "If you started the server from this notebook, make sure to stop it when you're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_server(server_process):\n",
    "    if server_process:\n",
    "        print(\"Stopping server...\")\n",
    "        server_process.terminate()\n",
    "        server_process.wait()\n",
    "        print(\"Server stopped.\")\n",
    "\n",
    "# Only stop the server if we started it\n",
    "if not is_notebook and 'server_process' in locals():\n",
    "    stop_server(server_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored how to set up a local model server using vLLM. We've created a FastAPI server that exposes an API endpoint for text generation and tested its performance.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. vLLM provides significant performance improvements for LLM serving through PagedAttention and continuous batching\n",
    "2. Setting up a local model server is straightforward with FastAPI and vLLM\n",
    "3. The API can be easily tested and benchmarked to measure performance\n",
    "\n",
    "In the next part, we'll extend our server to support streaming generation and explore more advanced features of vLLM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
