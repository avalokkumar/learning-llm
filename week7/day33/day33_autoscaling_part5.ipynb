{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 33: Autoscaling for LLM Services - Part 5\n",
    "\n",
    "Autoscaling is crucial for cost-effective LLM deployment. This notebook covers strategies for automatically scaling LLM services based on demand.\n",
    "\n",
    "## Overview\n",
    "1. Autoscaling metrics and triggers\n",
    "2. Simple autoscaling implementation\n",
    "3. Production considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Autoscaling Metrics\n",
    "\n",
    "Key metrics for LLM autoscaling decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScalingMetrics:\n",
    "    queue_length: int\n",
    "    avg_response_time: float\n",
    "    requests_per_second: float\n",
    "    gpu_utilization: float\n",
    "    memory_usage: float\n",
    "    active_instances: int\n",
    "\n",
    "class MetricsCollector:\n",
    "    def __init__(self, window_size=60):\n",
    "        self.window_size = window_size\n",
    "        self.metrics_history = deque(maxlen=window_size)\n",
    "        self.request_times = deque(maxlen=100)\n",
    "        self.current_queue_length = 0\n",
    "        self.active_instances = 1\n",
    "    \n",
    "    def record_request(self, response_time: float):\n",
    "        \"\"\"Record a completed request.\"\"\"\n",
    "        self.request_times.append(response_time)\n",
    "    \n",
    "    def update_queue_length(self, length: int):\n",
    "        \"\"\"Update current queue length.\"\"\"\n",
    "        self.current_queue_length = length\n",
    "    \n",
    "    def get_current_metrics(self) -> ScalingMetrics:\n",
    "        \"\"\"Get current scaling metrics.\"\"\"\n",
    "        # Calculate average response time\n",
    "        avg_response_time = np.mean(self.request_times) if self.request_times else 0\n",
    "        \n",
    "        # Calculate requests per second (last 10 seconds)\n",
    "        recent_requests = len([t for t in self.request_times if time.time() - t < 10])\n",
    "        requests_per_second = recent_requests / 10.0\n",
    "        \n",
    "        # Mock GPU metrics (in production, use nvidia-ml-py)\n",
    "        gpu_utilization = np.random.uniform(40, 95)  # Mock GPU usage\n",
    "        memory_usage = np.random.uniform(60, 90)     # Mock memory usage\n",
    "        \n",
    "        metrics = ScalingMetrics(\n",
    "            queue_length=self.current_queue_length,\n",
    "            avg_response_time=avg_response_time,\n",
    "            requests_per_second=requests_per_second,\n",
    "            gpu_utilization=gpu_utilization,\n",
    "            memory_usage=memory_usage,\n",
    "            active_instances=self.active_instances\n",
    "        )\n",
    "        \n",
    "        self.metrics_history.append(metrics)\n",
    "        return metrics\n",
    "\n",
    "# Initialize metrics collector\n",
    "metrics_collector = MetricsCollector()\n",
    "print(\"Metrics collector initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Autoscaler Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAutoscaler:\n",
    "    def __init__(self, min_instances=1, max_instances=10):\n",
    "        self.min_instances = min_instances\n",
    "        self.max_instances = max_instances\n",
    "        self.current_instances = min_instances\n",
    "        self.last_scale_time = time.time()\n",
    "        self.cooldown_period = 300  # 5 minutes\n",
    "        \n",
    "        # Scaling thresholds\n",
    "        self.scale_up_thresholds = {\n",
    "            'queue_length': 10,\n",
    "            'avg_response_time': 5.0,\n",
    "            'gpu_utilization': 85.0\n",
    "        }\n",
    "        \n",
    "        self.scale_down_thresholds = {\n",
    "            'queue_length': 2,\n",
    "            'avg_response_time': 1.0,\n",
    "            'gpu_utilization': 30.0\n",
    "        }\n",
    "    \n",
    "    def should_scale_up(self, metrics: ScalingMetrics) -> bool:\n",
    "        \"\"\"Determine if we should scale up.\"\"\"\n",
    "        conditions = [\n",
    "            metrics.queue_length > self.scale_up_thresholds['queue_length'],\n",
    "            metrics.avg_response_time > self.scale_up_thresholds['avg_response_time'],\n",
    "            metrics.gpu_utilization > self.scale_up_thresholds['gpu_utilization']\n",
    "        ]\n",
    "        \n",
    "        # Scale up if any condition is met\n",
    "        return any(conditions) and self.current_instances < self.max_instances\n",
    "    \n",
    "    def should_scale_down(self, metrics: ScalingMetrics) -> bool:\n",
    "        \"\"\"Determine if we should scale down.\"\"\"\n",
    "        conditions = [\n",
    "            metrics.queue_length < self.scale_down_thresholds['queue_length'],\n",
    "            metrics.avg_response_time < self.scale_down_thresholds['avg_response_time'],\n",
    "            metrics.gpu_utilization < self.scale_down_thresholds['gpu_utilization']\n",
    "        ]\n",
    "        \n",
    "        # Scale down only if all conditions are met\n",
    "        return all(conditions) and self.current_instances > self.min_instances\n",
    "    \n",
    "    def can_scale(self) -> bool:\n",
    "        \"\"\"Check if we're outside the cooldown period.\"\"\"\n",
    "        return time.time() - self.last_scale_time > self.cooldown_period\n",
    "    \n",
    "    def scale_up(self) -> bool:\n",
    "        \"\"\"Scale up by one instance.\"\"\"\n",
    "        if self.current_instances < self.max_instances:\n",
    "            self.current_instances += 1\n",
    "            self.last_scale_time = time.time()\n",
    "            print(f\"ðŸ”¼ Scaled UP to {self.current_instances} instances\")\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def scale_down(self) -> bool:\n",
    "        \"\"\"Scale down by one instance.\"\"\"\n",
    "        if self.current_instances > self.min_instances:\n",
    "            self.current_instances -= 1\n",
    "            self.last_scale_time = time.time()\n",
    "            print(f\"ðŸ”½ Scaled DOWN to {self.current_instances} instances\")\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def evaluate(self, metrics: ScalingMetrics) -> Optional[str]:\n",
    "        \"\"\"Evaluate metrics and make scaling decision.\"\"\"\n",
    "        if not self.can_scale():\n",
    "            return \"cooldown\"\n",
    "        \n",
    "        if self.should_scale_up(metrics):\n",
    "            if self.scale_up():\n",
    "                return \"scaled_up\"\n",
    "        elif self.should_scale_down(metrics):\n",
    "            if self.scale_down():\n",
    "                return \"scaled_down\"\n",
    "        \n",
    "        return \"no_action\"\n",
    "\n",
    "# Initialize autoscaler\n",
    "autoscaler = SimpleAutoscaler(min_instances=1, max_instances=5)\n",
    "print(f\"Autoscaler initialized: {autoscaler.min_instances}-{autoscaler.max_instances} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Autoscaling Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_workload_pattern(duration_minutes=30):\n",
    "    \"\"\"Simulate a realistic workload pattern.\"\"\"\n",
    "    \n",
    "    print(f\"Simulating {duration_minutes} minutes of workload...\")\n",
    "    \n",
    "    # Simulation data storage\n",
    "    timeline = []\n",
    "    metrics_history = []\n",
    "    scaling_events = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for minute in range(duration_minutes):\n",
    "        current_time = start_time + minute * 60\n",
    "        \n",
    "        # Simulate different load patterns\n",
    "        if minute < 5:  # Low load\n",
    "            base_load = 2\n",
    "        elif minute < 15:  # Increasing load\n",
    "            base_load = 2 + (minute - 5) * 2\n",
    "        elif minute < 20:  # High load\n",
    "            base_load = 20\n",
    "        else:  # Decreasing load\n",
    "            base_load = max(2, 20 - (minute - 20) * 3)\n",
    "        \n",
    "        # Add some randomness\n",
    "        queue_length = max(0, int(base_load + np.random.normal(0, 2)))\n",
    "        \n",
    "        # Update metrics\n",
    "        metrics_collector.update_queue_length(queue_length)\n",
    "        metrics_collector.active_instances = autoscaler.current_instances\n",
    "        \n",
    "        # Simulate some completed requests\n",
    "        for _ in range(np.random.randint(1, 5)):\n",
    "            response_time = np.random.uniform(0.5, 3.0)\n",
    "            metrics_collector.record_request(response_time)\n",
    "        \n",
    "        # Get current metrics\n",
    "        current_metrics = metrics_collector.get_current_metrics()\n",
    "        \n",
    "        # Make scaling decision\n",
    "        scaling_action = autoscaler.evaluate(current_metrics)\n",
    "        \n",
    "        # Record data\n",
    "        timeline.append(minute)\n",
    "        metrics_history.append(current_metrics)\n",
    "        \n",
    "        if scaling_action in ['scaled_up', 'scaled_down']:\n",
    "            scaling_events.append((minute, scaling_action, autoscaler.current_instances))\n",
    "        \n",
    "        # Print status every 5 minutes\n",
    "        if minute % 5 == 0:\n",
    "            print(f\"Minute {minute}: Queue={queue_length}, Instances={autoscaler.current_instances}, Action={scaling_action}\")\n",
    "        \n",
    "        # Small delay for simulation\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    return timeline, metrics_history, scaling_events\n",
    "\n",
    "# Run simulation\n",
    "timeline, metrics_history, scaling_events = simulate_workload_pattern(20)\n",
    "\n",
    "print(f\"\\nSimulation completed. Scaling events: {len(scaling_events)}\")\n",
    "for event in scaling_events:\n",
    "    minute, action, instances = event\n",
    "    print(f\"  Minute {minute}: {action} -> {instances} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing Autoscaling Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for plotting\n",
    "queue_lengths = [m.queue_length for m in metrics_history]\n",
    "instance_counts = [m.active_instances for m in metrics_history]\n",
    "gpu_utilizations = [m.gpu_utilization for m in metrics_history]\n",
    "response_times = [m.avg_response_time for m in metrics_history]\n",
    "\n",
    "# Create visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Queue length and instances\n",
    "ax1.plot(timeline, queue_lengths, 'b-', label='Queue Length', linewidth=2)\n",
    "ax1_twin = ax1.twinx()\n",
    "ax1_twin.plot(timeline, instance_counts, 'r-', label='Instances', linewidth=2, marker='o')\n",
    "ax1.set_xlabel('Time (minutes)')\n",
    "ax1.set_ylabel('Queue Length', color='b')\n",
    "ax1_twin.set_ylabel('Active Instances', color='r')\n",
    "ax1.set_title('Queue Length vs Active Instances')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# GPU utilization\n",
    "ax2.plot(timeline, gpu_utilizations, 'g-', linewidth=2)\n",
    "ax2.axhline(y=85, color='r', linestyle='--', alpha=0.7, label='Scale Up Threshold')\n",
    "ax2.axhline(y=30, color='b', linestyle='--', alpha=0.7, label='Scale Down Threshold')\n",
    "ax2.set_xlabel('Time (minutes)')\n",
    "ax2.set_ylabel('GPU Utilization (%)')\n",
    "ax2.set_title('GPU Utilization Over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Response times\n",
    "ax3.plot(timeline, response_times, 'm-', linewidth=2)\n",
    "ax3.axhline(y=5.0, color='r', linestyle='--', alpha=0.7, label='Scale Up Threshold')\n",
    "ax3.axhline(y=1.0, color='b', linestyle='--', alpha=0.7, label='Scale Down Threshold')\n",
    "ax3.set_xlabel('Time (minutes)')\n",
    "ax3.set_ylabel('Avg Response Time (s)')\n",
    "ax3.set_title('Average Response Time')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Scaling events\n",
    "ax4.plot(timeline, instance_counts, 'r-', linewidth=2, marker='o')\n",
    "for event in scaling_events:\n",
    "    minute, action, instances = event\n",
    "    color = 'green' if action == 'scaled_up' else 'orange'\n",
    "    ax4.axvline(x=minute, color=color, linestyle=':', alpha=0.7)\n",
    "    ax4.annotate(f'{action}\\n({instances})', xy=(minute, instances), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "ax4.set_xlabel('Time (minutes)')\n",
    "ax4.set_ylabel('Active Instances')\n",
    "ax4.set_title('Scaling Events Timeline')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n=== Autoscaling Summary ===\")\n",
    "print(f\"Total scaling events: {len(scaling_events)}\")\n",
    "print(f\"Max instances used: {max(instance_counts)}\")\n",
    "print(f\"Min instances used: {min(instance_counts)}\")\n",
    "print(f\"Average queue length: {np.mean(queue_lengths):.1f}\")\n",
    "print(f\"Average response time: {np.mean(response_times):.2f}s\")\n",
    "print(f\"Average GPU utilization: {np.mean(gpu_utilizations):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Production Autoscaling Script\n",
    "\n",
    "Here's a production-ready autoscaling script template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production autoscaling script\n",
    "production_autoscaler = \"\"\"\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "# Production LLM Autoscaler\n",
    "# This script monitors LLM service metrics and scales instances accordingly\n",
    "\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class AutoscalerConfig:\n",
    "    min_instances: int = 1\n",
    "    max_instances: int = 10\n",
    "    scale_up_threshold_queue: int = 10\n",
    "    scale_down_threshold_queue: int = 2\n",
    "    scale_up_threshold_latency: float = 5.0\n",
    "    scale_down_threshold_latency: float = 1.0\n",
    "    cooldown_period: int = 300  # 5 minutes\n",
    "    check_interval: int = 60    # 1 minute\n",
    "    metrics_endpoint: str = \"http://localhost:8000/metrics\"\n",
    "\n",
    "class ProductionAutoscaler:\n",
    "    def __init__(self, config: AutoscalerConfig):\n",
    "        self.config = config\n",
    "        self.current_instances = config.min_instances\n",
    "        self.last_scale_time = 0\n",
    "        \n",
    "    def get_metrics(self) -> Dict:\n",
    "        \"\"\"Fetch metrics from the LLM service.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(self.config.metrics_endpoint, timeout=10)\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to fetch metrics: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def scale_up(self) -> bool:\n",
    "        \"\"\"Scale up the service.\"\"\"\n",
    "        if self.current_instances >= self.config.max_instances:\n",
    "            logger.warning(\"Already at maximum instances\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # Example: Kubernetes scaling command\n",
    "            cmd = f\"kubectl scale deployment llm-service --replicas={self.current_instances + 1}\"\n",
    "            subprocess.run(cmd.split(), check=True)\n",
    "            \n",
    "            self.current_instances += 1\n",
    "            self.last_scale_time = time.time()\n",
    "            logger.info(f\"Scaled UP to {self.current_instances} instances\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to scale up: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def scale_down(self) -> bool:\n",
    "        \"\"\"Scale down the service.\"\"\"\n",
    "        if self.current_instances <= self.config.min_instances:\n",
    "            logger.warning(\"Already at minimum instances\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # Example: Kubernetes scaling command\n",
    "            cmd = f\"kubectl scale deployment llm-service --replicas={self.current_instances - 1}\"\n",
    "            subprocess.run(cmd.split(), check=True)\n",
    "            \n",
    "            self.current_instances -= 1\n",
    "            self.last_scale_time = time.time()\n",
    "            logger.info(f\"Scaled DOWN to {self.current_instances} instances\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to scale down: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def should_scale(self, metrics: Dict) -> str:\n",
    "        \"\"\"Determine scaling action based on metrics.\"\"\"\n",
    "        if time.time() - self.last_scale_time < self.config.cooldown_period:\n",
    "            return \"cooldown\"\n",
    "        \n",
    "        queue_length = metrics.get(\"queue_length\", 0)\n",
    "        avg_latency = metrics.get(\"avg_response_time\", 0)\n",
    "        \n",
    "        # Scale up conditions\n",
    "        if (queue_length > self.config.scale_up_threshold_queue or \n",
    "            avg_latency > self.config.scale_up_threshold_latency):\n",
    "            return \"scale_up\"\n",
    "        \n",
    "        # Scale down conditions\n",
    "        if (queue_length < self.config.scale_down_threshold_queue and \n",
    "            avg_latency < self.config.scale_down_threshold_latency):\n",
    "            return \"scale_down\"\n",
    "        \n",
    "        return \"no_action\"\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Main autoscaler loop.\"\"\"\n",
    "        logger.info(\"Starting LLM autoscaler...\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                # Get current metrics\n",
    "                metrics = self.get_metrics()\n",
    "                \n",
    "                if not metrics:\n",
    "                    logger.warning(\"No metrics available, skipping scaling decision\")\n",
    "                    time.sleep(self.config.check_interval)\n",
    "                    continue\n",
    "                \n",
    "                # Make scaling decision\n",
    "                action = self.should_scale(metrics)\n",
    "                \n",
    "                if action == \"scale_up\":\n",
    "                    self.scale_up()\n",
    "                elif action == \"scale_down\":\n",
    "                    self.scale_down()\n",
    "                else:\n",
    "                    logger.debug(f\"No scaling action needed: {action}\")\n",
    "                \n",
    "                # Log current status\n",
    "                logger.info(f\"Status: {self.current_instances} instances, \"\n",
    "                          f\"Queue: {metrics.get('queue_length', 0)}, \"\n",
    "                          f\"Latency: {metrics.get('avg_response_time', 0):.2f}s\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                logger.info(\"Autoscaler stopped by user\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in autoscaler loop: {e}\")\n",
    "            \n",
    "            time.sleep(self.config.check_interval)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = AutoscalerConfig()\n",
    "    autoscaler = ProductionAutoscaler(config)\n",
    "    autoscaler.run()\n",
    "\"\"\"\n",
    "\n",
    "# Write production autoscaler\n",
    "with open(\"production_autoscaler.py\", \"w\") as f:\n",
    "    f.write(production_autoscaler)\n",
    "\n",
    "print(\"Production autoscaler created: production_autoscaler.py\")\n",
    "print(\"\\nTo run: python production_autoscaler.py\")\n",
    "print(\"\\nProduction Features:\")\n",
    "print(\"- Configurable thresholds and parameters\")\n",
    "print(\"- Kubernetes integration for scaling\")\n",
    "print(\"- Comprehensive logging\")\n",
    "print(\"- Error handling and recovery\")\n",
    "print(\"- Cooldown periods to prevent thrashing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Effective autoscaling for LLM services requires:\n",
    "\n",
    "1. **Appropriate Metrics**: Queue length, response time, GPU utilization\n",
    "2. **Smart Thresholds**: Balance responsiveness with stability\n",
    "3. **Cooldown Periods**: Prevent scaling thrashing\n",
    "4. **Production Integration**: Kubernetes, monitoring, logging\n",
    "\n",
    "Key considerations:\n",
    "- LLM cold start times are significant\n",
    "- GPU resources are expensive and granular\n",
    "- Request processing times vary widely\n",
    "- Cost optimization is crucial\n",
    "\n",
    "This completes our exploration of LLM serving stacks and deployment strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
