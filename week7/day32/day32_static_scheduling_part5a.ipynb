{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 32: Static Scheduling Strategies - Part 5a\n",
    "\n",
    "In this notebook, we'll explore static scheduling strategies for LLM inference. Static scheduling involves processing requests in fixed-size batches, which is simpler but less efficient than dynamic scheduling.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Understanding static scheduling\n",
    "2. Implementing a basic static scheduler\n",
    "3. Measuring throughput and latency\n",
    "4. Analyzing the impact of batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Static Scheduling\n",
    "\n",
    "Static scheduling processes requests in fixed-size batches:\n",
    "\n",
    "1. Requests are collected until a batch is filled\n",
    "2. The entire batch is processed together\n",
    "3. All sequences in the batch are processed until completion\n",
    "4. A new batch is formed and processed\n",
    "\n",
    "This approach is simple to implement but has several limitations:\n",
    "- Waiting for a batch to fill introduces latency\n",
    "- Processing all sequences to completion is inefficient\n",
    "- Early-finishing sequences waste resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import queue\n",
    "import threading\n",
    "from collections import deque\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing a Basic Static Scheduler\n",
    "\n",
    "Let's implement a basic static scheduler that processes requests in fixed-size batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Request:\n",
    "    \"\"\"Class representing an inference request.\"\"\"\n",
    "    def __init__(self, id, prompt, max_tokens=20):\n",
    "        self.id = id\n",
    "        self.prompt = prompt\n",
    "        self.max_tokens = max_tokens\n",
    "        self.result = None\n",
    "        self.arrival_time = time.time()\n",
    "        self.start_time = None\n",
    "        self.completion_time = None\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Mark the request as started.\"\"\"\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def complete(self, result):\n",
    "        \"\"\"Mark the request as completed.\"\"\"\n",
    "        self.result = result\n",
    "        self.completion_time = time.time()\n",
    "    \n",
    "    @property\n",
    "    def waiting_time(self):\n",
    "        \"\"\"Time spent waiting in the queue.\"\"\"\n",
    "        if self.start_time is None:\n",
    "            return None\n",
    "        return self.start_time - self.arrival_time\n",
    "    \n",
    "    @property\n",
    "    def processing_time(self):\n",
    "        \"\"\"Time spent processing the request.\"\"\"\n",
    "        if self.start_time is None or self.completion_time is None:\n",
    "            return None\n",
    "        return self.completion_time - self.start_time\n",
    "    \n",
    "    @property\n",
    "    def total_time(self):\n",
    "        \"\"\"Total time from arrival to completion.\"\"\"\n",
    "        if self.completion_time is None:\n",
    "            return None\n",
    "        return self.completion_time - self.arrival_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockModel:\n",
    "    \"\"\"Mock model for simulation purposes.\"\"\"\n",
    "    def __init__(self, token_generation_time=0.05):\n",
    "        self.token_generation_time = token_generation_time\n",
    "    \n",
    "    def generate(self, prompts, max_tokens=20):\n",
    "        \"\"\"Simulate token generation.\"\"\"\n",
    "        # Simulate processing time based on batch size and sequence length\n",
    "        batch_size = len(prompts)\n",
    "        \n",
    "        # Simulate some parallelism benefit for larger batches\n",
    "        if batch_size > 1:\n",
    "            # Assume some efficiency gain with larger batches\n",
    "            efficiency_factor = 1.0 - 0.1 * np.log(batch_size)\n",
    "            efficiency_factor = max(0.5, efficiency_factor)  # Cap at 50% efficiency\n",
    "        else:\n",
    "            efficiency_factor = 1.0\n",
    "        \n",
    "        # Simulate generation time\n",
    "        time.sleep(max_tokens * self.token_generation_time * efficiency_factor)\n",
    "        \n",
    "        # Generate mock results\n",
    "        results = []\n",
    "        for prompt in prompts:\n",
    "            # Create a simple result by appending tokens to the prompt\n",
    "            result = prompt + \" \" + \"generated_text\" * (max_tokens // 2)\n",
    "            results.append(result)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticScheduler:\n",
    "    \"\"\"Static scheduler for batch processing.\"\"\"\n",
    "    def __init__(self, model, batch_size=4):\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.request_queue = queue.Queue()\n",
    "        self.completed_requests = []\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "    \n",
    "    def submit_request(self, request):\n",
    "        \"\"\"Submit a request to the scheduler.\"\"\"\n",
    "        self.request_queue.put(request)\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Start the scheduler.\"\"\"\n",
    "        if self.running:\n",
    "            return\n",
    "        \n",
    "        self.running = True\n",
    "        self.thread = threading.Thread(target=self._process_batches)\n",
    "        self.thread.daemon = True\n",
    "        self.thread.start()\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop the scheduler.\"\"\"\n",
    "        self.running = False\n",
    "        if self.thread:\n",
    "            self.thread.join()\n",
    "    \n",
    "    def _process_batches(self):\n",
    "        \"\"\"Process requests in batches.\"\"\"\n",
    "        while self.running:\n",
    "            # Collect a batch of requests\n",
    "            batch = []\n",
    "            max_tokens = 0\n",
    "            \n",
    "            # Try to fill the batch\n",
    "            try:\n",
    "                while len(batch) < self.batch_size:\n",
    "                    # Wait for a request with a timeout\n",
    "                    request = self.request_queue.get(timeout=0.1)\n",
    "                    batch.append(request)\n",
    "                    max_tokens = max(max_tokens, request.max_tokens)\n",
    "                    request.start()\n",
    "            except queue.Empty:\n",
    "                # If the queue is empty and we have at least one request, process it\n",
    "                if not batch:\n",
    "                    continue\n",
    "            \n",
    "            # If we have requests to process\n",
    "            if batch:\n",
    "                # Process the batch\n",
    "                prompts = [request.prompt for request in batch]\n",
    "                \n",
    "                # Generate responses\n",
    "                results = self.model.generate(prompts, max_tokens=max_tokens)\n",
    "                \n",
    "                # Update requests with results\n",
    "                for request, result in zip(batch, results):\n",
    "                    request.complete(result)\n",
    "                    self.completed_requests.append(request)\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        \"\"\"Get performance metrics.\"\"\"\n",
    "        if not self.completed_requests:\n",
    "            return {}\n",
    "        \n",
    "        waiting_times = [r.waiting_time for r in self.completed_requests if r.waiting_time is not None]\n",
    "        processing_times = [r.processing_time for r in self.completed_requests if r.processing_time is not None]\n",
    "        total_times = [r.total_time for r in self.completed_requests if r.total_time is not None]\n",
    "        \n",
    "        return {\n",
    "            \"num_completed\": len(self.completed_requests),\n",
    "            \"avg_waiting_time\": np.mean(waiting_times) if waiting_times else 0,\n",
    "            \"avg_processing_time\": np.mean(processing_times) if processing_times else 0,\n",
    "            \"avg_total_time\": np.mean(total_times) if total_times else 0,\n",
    "            \"throughput\": len(self.completed_requests) / (max(total_times) - min([r.arrival_time for r in self.completed_requests])) if total_times else 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing the Static Scheduler\n",
    "\n",
    "Let's test our static scheduler with a simple workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_static_scheduler_test(batch_size=4, num_requests=20, arrival_rate=2):\n",
    "    \"\"\"Run a test of the static scheduler.\"\"\"\n",
    "    # Create a mock model\n",
    "    model = MockModel(token_generation_time=0.05)\n",
    "    \n",
    "    # Create a scheduler\n",
    "    scheduler = StaticScheduler(model, batch_size=batch_size)\n",
    "    scheduler.start()\n",
    "    \n",
    "    # Generate test prompts\n",
    "    test_prompts = [\n",
    "        \"The future of artificial intelligence is\",\n",
    "        \"Climate change will impact our planet by\",\n",
    "        \"Space exploration in the next decade will focus on\",\n",
    "        \"Quantum computing offers advantages such as\",\n",
    "        \"The most significant ethical concerns in technology are\"\n",
    "    ]\n",
    "    \n",
    "    # Submit requests at the specified arrival rate\n",
    "    print(f\"Submitting {num_requests} requests at rate of {arrival_rate} per second...\")\n",
    "    for i in range(num_requests):\n",
    "        # Create a request with a random prompt and token length\n",
    "        prompt = test_prompts[i % len(test_prompts)]\n",
    "        max_tokens = np.random.randint(10, 30)\n",
    "        request = Request(id=i, prompt=prompt, max_tokens=max_tokens)\n",
    "        \n",
    "        # Submit the request\n",
    "        scheduler.submit_request(request)\n",
    "        \n",
    "        # Wait according to arrival rate\n",
    "        time.sleep(1.0 / arrival_rate)\n",
    "    \n",
    "    # Wait for all requests to complete\n",
    "    while scheduler.request_queue.qsize() > 0 or len(scheduler.completed_requests) < num_requests:\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    # Stop the scheduler\n",
    "    scheduler.stop()\n",
    "    \n",
    "    # Get metrics\n",
    "    metrics = scheduler.get_metrics()\n",
    "    \n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"Completed requests: {metrics['num_completed']}\")\n",
    "    print(f\"Average waiting time: {metrics['avg_waiting_time']:.2f} seconds\")\n",
    "    print(f\"Average processing time: {metrics['avg_processing_time']:.2f} seconds\")\n",
    "    print(f\"Average total time: {metrics['avg_total_time']:.2f} seconds\")\n",
    "    print(f\"Throughput: {metrics['throughput']:.2f} requests per second\")\n",
    "    \n",
    "    return metrics, scheduler.completed_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a test with batch size 4\n",
    "metrics_batch4, completed_batch4 = run_static_scheduler_test(batch_size=4, num_requests=20, arrival_rate=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyzing the Impact of Batch Size\n",
    "\n",
    "Let's analyze how batch size affects throughput and latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_batch_size_impact(batch_sizes, num_requests=20, arrival_rate=2):\n",
    "    \"\"\"Analyze the impact of batch size on performance.\"\"\"\n",
    "    throughputs = []\n",
    "    avg_latencies = []\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"\\nTesting batch size: {batch_size}\")\n",
    "        metrics, _ = run_static_scheduler_test(batch_size=batch_size, num_requests=num_requests, arrival_rate=arrival_rate)\n",
    "        throughputs.append(metrics[\"throughput\"])\n",
    "        avg_latencies.append(metrics[\"avg_total_time\"])\n",
    "    \n",
    "    return throughputs, avg_latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different batch sizes\n",
    "batch_sizes = [1, 2, 4, 8, 16]\n",
    "throughputs, avg_latencies = analyze_batch_size_impact(batch_sizes, num_requests=30, arrival_rate=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot throughput\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(batch_sizes, throughputs, marker='o')\n",
    "plt.xlabel(\"Batch Size\")\n",
    "plt.ylabel(\"Throughput (requests/second)\")\n",
    "plt.title(\"Throughput vs. Batch Size\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot latency\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(batch_sizes, avg_latencies, marker='o')\n",
    "plt.xlabel(\"Batch Size\")\n",
    "plt.ylabel(\"Average Latency (seconds)\")\n",
    "plt.title(\"Latency vs. Batch Size\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing the Throughput-Latency Trade-off\n",
    "\n",
    "There's a fundamental trade-off between throughput and latency in static scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the throughput-latency trade-off\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(avg_latencies, throughputs, s=100)\n",
    "\n",
    "# Add labels for each point\n",
    "for i, batch_size in enumerate(batch_sizes):\n",
    "    plt.annotate(\n",
    "        f\"Batch={batch_size}\",\n",
    "        (avg_latencies[i], throughputs[i]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0, 10),\n",
    "        ha='center'\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Average Latency (seconds)\")\n",
    "plt.ylabel(\"Throughput (requests/second)\")\n",
    "plt.title(\"Throughput-Latency Trade-off\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Limitations of Static Scheduling\n",
    "\n",
    "Static scheduling has several limitations that make it suboptimal for LLM inference:\n",
    "\n",
    "1. **Head-of-Line Blocking**: Fast requests can be blocked behind slow ones\n",
    "2. **Batch Formation Delay**: Waiting for a batch to fill introduces latency\n",
    "3. **Resource Underutilization**: Early-finishing sequences waste resources\n",
    "4. **Fixed Batch Size**: Doesn't adapt to varying workloads\n",
    "5. **No Prioritization**: All requests are treated equally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Request Processing\n",
    "\n",
    "Let's visualize how requests are processed in a static scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_request_processing(completed_requests):\n",
    "    \"\"\"Visualize how requests are processed over time.\"\"\"\n",
    "    # Sort requests by arrival time\n",
    "    sorted_requests = sorted(completed_requests, key=lambda r: r.arrival_time)\n",
    "    \n",
    "    # Get the earliest arrival time as reference\n",
    "    t0 = sorted_requests[0].arrival_time\n",
    "    \n",
    "    # Prepare data for visualization\n",
    "    request_ids = []\n",
    "    arrival_times = []\n",
    "    start_times = []\n",
    "    completion_times = []\n",
    "    waiting_times = []\n",
    "    processing_times = []\n",
    "    \n",
    "    for request in sorted_requests:\n",
    "        request_ids.append(request.id)\n",
    "        arrival_times.append(request.arrival_time - t0)\n",
    "        start_times.append(request.start_time - t0)\n",
    "        completion_times.append(request.completion_time - t0)\n",
    "        waiting_times.append(request.waiting_time)\n",
    "        processing_times.append(request.processing_time)\n",
    "    \n",
    "    # Create a Gantt chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot waiting time\n",
    "    for i, request_id in enumerate(request_ids):\n",
    "        plt.barh(\n",
    "            request_id,\n",
    "            waiting_times[i],\n",
    "            left=arrival_times[i],\n",
    "            color='lightgray',\n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "    # Plot processing time\n",
    "    for i, request_id in enumerate(request_ids):\n",
    "        plt.barh(\n",
    "            request_id,\n",
    "            processing_times[i],\n",
    "            left=start_times[i],\n",
    "            color='blue',\n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "    # Add legend\n",
    "    plt.barh([], [], color='lightgray', label='Waiting Time')\n",
    "    plt.barh([], [], color='blue', label='Processing Time')\n",
    "    \n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Request ID\")\n",
    "    plt.title(\"Request Processing Timeline\")\n",
    "    plt.legend()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize request processing\n",
    "visualize_request_processing(completed_batch4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored static scheduling strategies for LLM inference. We've implemented a basic static scheduler and analyzed its performance characteristics.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. Static scheduling processes requests in fixed-size batches\n",
    "2. There's a trade-off between throughput and latency based on batch size\n",
    "3. Larger batch sizes generally improve throughput but increase latency\n",
    "4. Static scheduling has several limitations that make it suboptimal for LLM inference\n",
    "\n",
    "In the next part, we'll explore dynamic scheduling strategies that address these limitations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
