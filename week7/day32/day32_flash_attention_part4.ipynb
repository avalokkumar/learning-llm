{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 32: FlashAttention - Part 4\n",
    "\n",
    "In this notebook, we'll explore FlashAttention, an algorithm that optimizes the attention computation in transformer models, reducing both memory usage and computation time.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Understanding the attention bottleneck\n",
    "2. How FlashAttention works\n",
    "3. Implementing a simplified version of FlashAttention\n",
    "4. Measuring performance improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the Attention Bottleneck\n",
    "\n",
    "The standard attention computation in transformer models faces two main challenges:\n",
    "\n",
    "1. **Memory Bottleneck**: Storing the full attention matrix (NÃ—N) in high-precision\n",
    "2. **I/O Bound**: Multiple reads/writes to high-bandwidth memory (HBM)\n",
    "\n",
    "These challenges limit the maximum sequence length and batch size that can be processed efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Standard Attention Implementation\n",
    "\n",
    "Let's first implement the standard attention computation to understand the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_attention(q, k, v, scale=None, mask=None):\n",
    "    \"\"\"Standard attention computation.\n",
    "    \n",
    "    Args:\n",
    "        q: Query tensor of shape (batch_size, seq_len, head_dim)\n",
    "        k: Key tensor of shape (batch_size, seq_len, head_dim)\n",
    "        v: Value tensor of shape (batch_size, seq_len, head_dim)\n",
    "        scale: Scaling factor for attention scores\n",
    "        mask: Optional attention mask\n",
    "        \n",
    "    Returns:\n",
    "        Output tensor of shape (batch_size, seq_len, head_dim)\n",
    "    \"\"\"\n",
    "    # Get dimensions\n",
    "    batch_size, seq_len, head_dim = q.shape\n",
    "    \n",
    "    # Set scale if not provided\n",
    "    if scale is None:\n",
    "        scale = 1.0 / np.sqrt(head_dim)\n",
    "    \n",
    "    # Compute attention scores: (batch_size, seq_len, seq_len)\n",
    "    scores = torch.bmm(q, k.transpose(1, 2)) * scale\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    attn_weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Compute output: (batch_size, seq_len, head_dim)\n",
    "    output = torch.bmm(attn_weights, v)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How FlashAttention Works\n",
    "\n",
    "FlashAttention optimizes attention computation through three key techniques:\n",
    "\n",
    "1. **Block-wise Processing**: Divides matrices into blocks that fit in fast SRAM\n",
    "2. **Operation Fusion**: Combines multiple operations to reduce memory access\n",
    "3. **Recomputation**: Trades additional computation for reduced memory usage\n",
    "\n",
    "This approach significantly reduces the memory I/O cost, which is often the bottleneck in attention computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing a Simplified FlashAttention\n",
    "\n",
    "Let's implement a simplified version of FlashAttention to demonstrate its core concepts. Note that this is a pedagogical implementation and not optimized for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplified_flash_attention(q, k, v, block_size=64, scale=None, mask=None):\n",
    "    \"\"\"Simplified implementation of FlashAttention.\n",
    "    \n",
    "    Args:\n",
    "        q: Query tensor of shape (batch_size, seq_len, head_dim)\n",
    "        k: Key tensor of shape (batch_size, seq_len, head_dim)\n",
    "        v: Value tensor of shape (batch_size, seq_len, head_dim)\n",
    "        block_size: Size of blocks for tiled computation\n",
    "        scale: Scaling factor for attention scores\n",
    "        mask: Optional attention mask\n",
    "        \n",
    "    Returns:\n",
    "        Output tensor of shape (batch_size, seq_len, head_dim)\n",
    "    \"\"\"\n",
    "    # Get dimensions\n",
    "    batch_size, seq_len, head_dim = q.shape\n",
    "    \n",
    "    # Set scale if not provided\n",
    "    if scale is None:\n",
    "        scale = 1.0 / np.sqrt(head_dim)\n",
    "    \n",
    "    # Initialize output and softmax normalization terms\n",
    "    output = torch.zeros_like(q)\n",
    "    normalizer = torch.zeros((batch_size, seq_len, 1), device=q.device)\n",
    "    \n",
    "    # Process in blocks\n",
    "    for i in range(0, seq_len, block_size):\n",
    "        # Current block size (might be smaller at the end)\n",
    "        current_block_size = min(block_size, seq_len - i)\n",
    "        \n",
    "        # Extract query block: (batch_size, block_size, head_dim)\n",
    "        q_block = q[:, i:i+current_block_size, :]\n",
    "        \n",
    "        # Initialize block output and normalization\n",
    "        block_output = torch.zeros_like(q_block)\n",
    "        block_normalizer = torch.zeros((batch_size, current_block_size, 1), device=q.device)\n",
    "        \n",
    "        # Process key-value blocks\n",
    "        for j in range(0, seq_len, block_size):\n",
    "            # Current key-value block size\n",
    "            current_kv_block_size = min(block_size, seq_len - j)\n",
    "            \n",
    "            # Extract key and value blocks\n",
    "            k_block = k[:, j:j+current_kv_block_size, :]\n",
    "            v_block = v[:, j:j+current_kv_block_size, :]\n",
    "            \n",
    "            # Compute attention scores for this block: (batch_size, block_size, kv_block_size)\n",
    "            scores = torch.bmm(q_block, k_block.transpose(1, 2)) * scale\n",
    "            \n",
    "            # Apply mask if provided\n",
    "            if mask is not None:\n",
    "                block_mask = mask[:, i:i+current_block_size, j:j+current_kv_block_size]\n",
    "                scores = scores.masked_fill(block_mask == 0, -1e9)\n",
    "            \n",
    "            # Apply softmax approximation (exp only, normalize later)\n",
    "            attn_weights = torch.exp(scores)\n",
    "            \n",
    "            # Update block output and normalization term\n",
    "            block_output += torch.bmm(attn_weights, v_block)\n",
    "            block_normalizer += attn_weights.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Normalize block output\n",
    "        block_output = block_output / (block_normalizer + 1e-6)\n",
    "        \n",
    "        # Update output\n",
    "        output[:, i:i+current_block_size, :] = block_output\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Measuring Performance\n",
    "\n",
    "Let's compare the performance of standard attention and our simplified FlashAttention implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_attention_performance(batch_size, seq_len, head_dim, num_runs=5):\n",
    "    \"\"\"Measure performance of standard and FlashAttention.\"\"\"\n",
    "    # Create random query, key, value tensors\n",
    "    q = torch.randn(batch_size, seq_len, head_dim, device=device)\n",
    "    k = torch.randn(batch_size, seq_len, head_dim, device=device)\n",
    "    v = torch.randn(batch_size, seq_len, head_dim, device=device)\n",
    "    \n",
    "    # Measure standard attention time\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    standard_times = []\n",
    "    standard_memory = []\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        # Clear cache\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "        # Record memory before\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            mem_before = torch.cuda.memory_allocated()\n",
    "        \n",
    "        # Time standard attention\n",
    "        start_time = time.time()\n",
    "        output_standard = standard_attention(q, k, v)\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        standard_times.append(time.time() - start_time)\n",
    "        \n",
    "        # Record memory usage\n",
    "        if torch.cuda.is_available():\n",
    "            mem_after = torch.cuda.max_memory_allocated()\n",
    "            standard_memory.append(mem_after - mem_before)\n",
    "    \n",
    "    # Measure FlashAttention time\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    flash_times = []\n",
    "    flash_memory = []\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        # Clear cache\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "        # Record memory before\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            mem_before = torch.cuda.memory_allocated()\n",
    "        \n",
    "        # Time FlashAttention\n",
    "        start_time = time.time()\n",
    "        output_flash = simplified_flash_attention(q, k, v)\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        flash_times.append(time.time() - start_time)\n",
    "        \n",
    "        # Record memory usage\n",
    "        if torch.cuda.is_available():\n",
    "            mem_after = torch.cuda.max_memory_allocated()\n",
    "            flash_memory.append(mem_after - mem_before)\n",
    "    \n",
    "    # Calculate average times and memory usage\n",
    "    avg_standard_time = sum(standard_times) / len(standard_times)\n",
    "    avg_flash_time = sum(flash_times) / len(flash_times)\n",
    "    \n",
    "    avg_standard_memory = sum(standard_memory) / len(standard_memory) if standard_memory else 0\n",
    "    avg_flash_memory = sum(flash_memory) / len(flash_memory) if flash_memory else 0\n",
    "    \n",
    "    # Check correctness (outputs should be similar)\n",
    "    if torch.cuda.is_available():\n",
    "        error = torch.abs(output_standard - output_flash).mean().item()\n",
    "    else:\n",
    "        error = 0  # Skip error check if not on CUDA\n",
    "    \n",
    "    return {\n",
    "        \"standard_time\": avg_standard_time,\n",
    "        \"flash_time\": avg_flash_time,\n",
    "        \"speedup\": avg_standard_time / avg_flash_time,\n",
    "        \"standard_memory\": avg_standard_memory / (1024 * 1024),  # Convert to MB\n",
    "        \"flash_memory\": avg_flash_memory / (1024 * 1024),  # Convert to MB\n",
    "        \"memory_reduction\": avg_standard_memory / avg_flash_memory if avg_flash_memory > 0 else 0,\n",
    "        \"error\": error\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure performance for a specific configuration\n",
    "results = measure_attention_performance(\n",
    "    batch_size=4,\n",
    "    seq_len=1024,\n",
    "    head_dim=64\n",
    ")\n",
    "\n",
    "print(\"Performance Comparison:\")\n",
    "print(f\"Standard Attention Time: {results['standard_time']:.4f} seconds\")\n",
    "print(f\"FlashAttention Time: {results['flash_time']:.4f} seconds\")\n",
    "print(f\"Speedup: {results['speedup']:.2f}x\")\n",
    "print(f\"\\nStandard Attention Memory: {results['standard_memory']:.2f} MB\")\n",
    "print(f\"FlashAttention Memory: {results['flash_memory']:.2f} MB\")\n",
    "print(f\"Memory Reduction: {results['memory_reduction']:.2f}x\")\n",
    "print(f\"\\nOutput Error: {results['error']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Scaling with Sequence Length\n",
    "\n",
    "One of the key benefits of FlashAttention is its improved scaling with sequence length. Let's measure how performance changes as sequence length increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_scaling_with_seq_len(batch_size, head_dim, seq_lengths):\n",
    "    \"\"\"Measure how performance scales with sequence length.\"\"\"\n",
    "    standard_times = []\n",
    "    flash_times = []\n",
    "    standard_memory = []\n",
    "    flash_memory = []\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        print(f\"Testing sequence length: {seq_len}\")\n",
    "        try:\n",
    "            result = measure_attention_performance(batch_size, seq_len, head_dim)\n",
    "            standard_times.append(result[\"standard_time\"])\n",
    "            flash_times.append(result[\"flash_time\"])\n",
    "            standard_memory.append(result[\"standard_memory\"])\n",
    "            flash_memory.append(result[\"flash_memory\"])\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error at sequence length {seq_len}: {e}\")\n",
    "            # If we run out of memory, stop the experiment\n",
    "            break\n",
    "    \n",
    "    return standard_times, flash_times, standard_memory, flash_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different sequence lengths\n",
    "seq_lengths = [128, 256, 512, 1024, 2048]\n",
    "standard_times, flash_times, standard_memory, flash_memory = measure_scaling_with_seq_len(\n",
    "    batch_size=2,\n",
    "    head_dim=64,\n",
    "    seq_lengths=seq_lengths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot execution time\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(seq_lengths[:len(standard_times)], standard_times, marker='o', label=\"Standard Attention\")\n",
    "plt.plot(seq_lengths[:len(flash_times)], flash_times, marker='s', label=\"FlashAttention\")\n",
    "plt.xlabel(\"Sequence Length\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.title(\"Execution Time vs. Sequence Length\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot memory usage\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(seq_lengths[:len(standard_memory)], standard_memory, marker='o', label=\"Standard Attention\")\n",
    "plt.plot(seq_lengths[:len(flash_memory)], flash_memory, marker='s', label=\"FlashAttention\")\n",
    "plt.xlabel(\"Sequence Length\")\n",
    "plt.ylabel(\"Memory Usage (MB)\")\n",
    "plt.title(\"Memory Usage vs. Sequence Length\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. FlashAttention-2 Improvements\n",
    "\n",
    "FlashAttention-2 further optimizes the algorithm with:\n",
    "- Improved tiling strategies for different GPU architectures\n",
    "- Online softmax algorithm for better numerical stability\n",
    "- Optimized memory access patterns\n",
    "- Support for different attention mask patterns\n",
    "\n",
    "These improvements lead to even better performance, especially for longer sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Using FlashAttention in PyTorch\n",
    "\n",
    "FlashAttention is available in PyTorch through the `flash-attn` package. Let's see how to use it if it's installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import flash_attn\n",
    "try:\n",
    "    from flash_attn import flash_attn_func\n",
    "    flash_attn_available = True\n",
    "    print(\"FlashAttention is available!\")\n",
    "except ImportError:\n",
    "    flash_attn_available = False\n",
    "    print(\"FlashAttention is not installed. You can install it with:\")\n",
    "    print(\"pip install flash-attn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of FlashAttention if available\n",
    "if flash_attn_available:\n",
    "    # Create random query, key, value tensors\n",
    "    batch_size = 2\n",
    "    seq_len = 1024\n",
    "    num_heads = 8\n",
    "    head_dim = 64\n",
    "    \n",
    "    # FlashAttention expects shape (batch_size, seq_len, num_heads, head_dim)\n",
    "    q = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)\n",
    "    k = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)\n",
    "    v = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)\n",
    "    \n",
    "    # Use FlashAttention\n",
    "    output = flash_attn_func(q, k, v, causal=True)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "else:\n",
    "    print(\"Skipping FlashAttention example as it's not installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. FlashAttention in Hugging Face Transformers\n",
    "\n",
    "Hugging Face Transformers also supports FlashAttention in some models. Here's how you can enable it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using FlashAttention in Hugging Face Transformers\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "# This is just an example, it won't run without the flash_attn package\n",
    "def load_model_with_flash_attn():\n",
    "    config = AutoConfig.from_pretrained(\"gpt2\")\n",
    "    config.use_flash_attention_2 = True  # Enable FlashAttention-2\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"gpt2\",\n",
    "        config=config,\n",
    "        torch_dtype=torch.float16  # FlashAttention works best with float16\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Note: This is just example code and won't run without the flash_attn package.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored FlashAttention, an algorithm that optimizes attention computation in transformer models. We've implemented a simplified version to demonstrate its core concepts and measured its performance benefits.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. FlashAttention reduces memory usage and computation time by using block-wise processing and operation fusion\n",
    "2. The algorithm scales better with sequence length compared to standard attention\n",
    "3. FlashAttention-2 further improves performance with optimized tiling and memory access patterns\n",
    "4. These optimizations enable processing longer sequences and larger batch sizes, which is critical for efficient LLM inference\n",
    "\n",
    "For production use, it's recommended to use the optimized implementations available in libraries like `flash-attn` or through Hugging Face Transformers' built-in support."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
