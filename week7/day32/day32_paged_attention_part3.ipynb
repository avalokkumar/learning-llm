{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 32: Paged Attention - Part 3\n",
    "\n",
    "In this notebook, we'll explore paged attention, a memory management technique that significantly improves the efficiency of KV caching for large language models.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Understanding paged attention\n",
    "2. The problem with traditional KV caching\n",
    "3. Implementing a simplified paged attention mechanism\n",
    "4. Comparing memory efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Paged Attention\n",
    "\n",
    "Paged attention is a technique introduced by vLLM that applies virtual memory concepts to KV caching in transformer models. It addresses the memory fragmentation and inefficient memory utilization issues of traditional KV caching.\n",
    "\n",
    "Key concepts:\n",
    "1. **Pages**: Fixed-size blocks of memory for storing KV cache\n",
    "2. **Block Table**: Maps logical positions to physical memory locations\n",
    "3. **Non-contiguous Allocation**: Allows flexible memory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Problem with Traditional KV Caching\n",
    "\n",
    "Traditional KV caching allocates contiguous memory blocks for each sequence, which leads to several issues:\n",
    "\n",
    "1. **Memory Fragmentation**: When sequences complete at different times, they leave gaps in memory\n",
    "2. **Inefficient Memory Utilization**: These gaps cannot be easily reused\n",
    "3. **Limited Concurrent Requests**: The number of sequences that can be processed simultaneously is limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate traditional KV caching memory allocation\n",
    "def simulate_traditional_kv_cache(num_sequences, max_seq_length, hidden_dim):\n",
    "    \"\"\"Simulate traditional KV cache memory allocation.\"\"\"\n",
    "    # Initialize memory usage tracking\n",
    "    total_allocated = 0\n",
    "    wasted_memory = 0\n",
    "    active_sequences = []\n",
    "    \n",
    "    # Simulate sequence processing with random lengths\n",
    "    for i in range(num_sequences):\n",
    "        # Random sequence length between 10 and max_seq_length\n",
    "        seq_length = np.random.randint(10, max_seq_length + 1)\n",
    "        \n",
    "        # Memory required for this sequence\n",
    "        memory_required = seq_length * hidden_dim\n",
    "        \n",
    "        # Allocate memory (always allocate max_seq_length for traditional approach)\n",
    "        allocated_memory = max_seq_length * hidden_dim\n",
    "        total_allocated += allocated_memory\n",
    "        \n",
    "        # Calculate wasted memory\n",
    "        wasted = allocated_memory - memory_required\n",
    "        wasted_memory += wasted\n",
    "        \n",
    "        # Add to active sequences\n",
    "        active_sequences.append({\n",
    "            \"id\": i,\n",
    "            \"length\": seq_length,\n",
    "            \"allocated\": allocated_memory,\n",
    "            \"wasted\": wasted\n",
    "        })\n",
    "        \n",
    "        # Randomly complete some sequences\n",
    "        if i > 0 and np.random.random() < 0.3:\n",
    "            # Remove a random active sequence\n",
    "            idx = np.random.randint(0, len(active_sequences))\n",
    "            active_sequences.pop(idx)\n",
    "    \n",
    "    # Calculate efficiency\n",
    "    efficiency = 1 - (wasted_memory / total_allocated) if total_allocated > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"total_allocated\": total_allocated,\n",
    "        \"wasted_memory\": wasted_memory,\n",
    "        \"efficiency\": efficiency,\n",
    "        \"active_sequences\": len(active_sequences)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the simulation\n",
    "traditional_results = simulate_traditional_kv_cache(\n",
    "    num_sequences=100,\n",
    "    max_seq_length=1024,\n",
    "    hidden_dim=64\n",
    ")\n",
    "\n",
    "print(\"Traditional KV Cache Simulation Results:\")\n",
    "print(f\"Total allocated memory: {traditional_results['total_allocated']:,}\")\n",
    "print(f\"Wasted memory: {traditional_results['wasted_memory']:,}\")\n",
    "print(f\"Memory efficiency: {traditional_results['efficiency']:.2%}\")\n",
    "print(f\"Active sequences at end: {traditional_results['active_sequences']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing a Simplified Paged Attention Mechanism\n",
    "\n",
    "Now, let's implement a simplified version of paged attention to demonstrate its memory efficiency benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PagedAttentionSimulator:\n",
    "    def __init__(self, page_size, num_pages, hidden_dim):\n",
    "        \"\"\"Initialize a paged attention simulator.\n",
    "        \n",
    "        Args:\n",
    "            page_size: Number of tokens per page\n",
    "            num_pages: Total number of pages in memory\n",
    "            hidden_dim: Hidden dimension size\n",
    "        \"\"\"\n",
    "        self.page_size = page_size\n",
    "        self.num_pages = num_pages\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Initialize memory pages\n",
    "        self.pages = [None] * num_pages  # None means the page is free\n",
    "        \n",
    "        # Block tables for each sequence\n",
    "        self.block_tables = {}\n",
    "        \n",
    "        # Statistics\n",
    "        self.total_tokens_stored = 0\n",
    "        self.total_pages_allocated = 0\n",
    "    \n",
    "    def allocate_sequence(self, seq_id, seq_length):\n",
    "        \"\"\"Allocate memory for a new sequence.\"\"\"\n",
    "        # Calculate number of pages needed\n",
    "        num_pages_needed = (seq_length + self.page_size - 1) // self.page_size\n",
    "        \n",
    "        # Find free pages\n",
    "        free_pages = [i for i, page in enumerate(self.pages) if page is None]\n",
    "        \n",
    "        if len(free_pages) < num_pages_needed:\n",
    "            return False  # Not enough free pages\n",
    "        \n",
    "        # Allocate pages\n",
    "        allocated_pages = free_pages[:num_pages_needed]\n",
    "        for page_idx in allocated_pages:\n",
    "            self.pages[page_idx] = seq_id\n",
    "        \n",
    "        # Create block table\n",
    "        self.block_tables[seq_id] = allocated_pages\n",
    "        \n",
    "        # Update statistics\n",
    "        self.total_tokens_stored += seq_length\n",
    "        self.total_pages_allocated += num_pages_needed\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def free_sequence(self, seq_id):\n",
    "        \"\"\"Free memory for a completed sequence.\"\"\"\n",
    "        if seq_id not in self.block_tables:\n",
    "            return False\n",
    "        \n",
    "        # Free pages\n",
    "        for page_idx in self.block_tables[seq_id]:\n",
    "            self.pages[page_idx] = None\n",
    "        \n",
    "        # Remove block table\n",
    "        del self.block_tables[seq_id]\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_memory_stats(self):\n",
    "        \"\"\"Get memory usage statistics.\"\"\"\n",
    "        total_capacity = self.num_pages * self.page_size * self.hidden_dim\n",
    "        used_capacity = self.total_pages_allocated * self.page_size * self.hidden_dim\n",
    "        wasted_capacity = used_capacity - self.total_tokens_stored * self.hidden_dim\n",
    "        efficiency = 1 - (wasted_capacity / used_capacity) if used_capacity > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"total_capacity\": total_capacity,\n",
    "            \"used_capacity\": used_capacity,\n",
    "            \"wasted_capacity\": wasted_capacity,\n",
    "            \"efficiency\": efficiency,\n",
    "            \"active_sequences\": len(self.block_tables),\n",
    "            \"free_pages\": self.pages.count(None),\n",
    "            \"total_pages\": self.num_pages\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate paged attention memory allocation\n",
    "def simulate_paged_attention(num_sequences, max_seq_length, hidden_dim, page_size=16):\n",
    "    \"\"\"Simulate paged attention memory allocation.\"\"\"\n",
    "    # Calculate total pages needed (with some extra capacity)\n",
    "    total_tokens = num_sequences * max_seq_length * 0.6  # Assuming 60% average utilization\n",
    "    num_pages = int((total_tokens + page_size - 1) // page_size * 1.2)  # 20% extra capacity\n",
    "    \n",
    "    # Initialize paged attention simulator\n",
    "    simulator = PagedAttentionSimulator(page_size, num_pages, hidden_dim)\n",
    "    \n",
    "    # Simulate sequence processing\n",
    "    active_seqs = []\n",
    "    for i in range(num_sequences):\n",
    "        # Random sequence length between 10 and max_seq_length\n",
    "        seq_length = np.random.randint(10, max_seq_length + 1)\n",
    "        \n",
    "        # Try to allocate memory for this sequence\n",
    "        success = simulator.allocate_sequence(i, seq_length)\n",
    "        \n",
    "        if success:\n",
    "            active_seqs.append(i)\n",
    "        \n",
    "        # Randomly complete some sequences\n",
    "        if len(active_seqs) > 0 and np.random.random() < 0.3:\n",
    "            # Remove a random active sequence\n",
    "            idx = np.random.randint(0, len(active_seqs))\n",
    "            seq_id = active_seqs.pop(idx)\n",
    "            simulator.free_sequence(seq_id)\n",
    "    \n",
    "    # Get memory statistics\n",
    "    return simulator.get_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the paged attention simulation\n",
    "paged_results = simulate_paged_attention(\n",
    "    num_sequences=100,\n",
    "    max_seq_length=1024,\n",
    "    hidden_dim=64,\n",
    "    page_size=16\n",
    ")\n",
    "\n",
    "print(\"Paged Attention Simulation Results:\")\n",
    "print(f\"Total capacity: {paged_results['total_capacity']:,}\")\n",
    "print(f\"Used capacity: {paged_results['used_capacity']:,}\")\n",
    "print(f\"Wasted capacity: {paged_results['wasted_capacity']:,}\")\n",
    "print(f\"Memory efficiency: {paged_results['efficiency']:.2%}\")\n",
    "print(f\"Active sequences at end: {paged_results['active_sequences']}\")\n",
    "print(f\"Free pages: {paged_results['free_pages']} / {paged_results['total_pages']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing Memory Efficiency\n",
    "\n",
    "Now, let's compare the memory efficiency of traditional KV caching and paged attention across different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_memory_efficiency(num_sequences_list, max_seq_length, hidden_dim, page_size=16):\n",
    "    \"\"\"Compare memory efficiency of traditional KV caching and paged attention.\"\"\"\n",
    "    traditional_efficiency = []\n",
    "    paged_efficiency = []\n",
    "    \n",
    "    for num_sequences in num_sequences_list:\n",
    "        # Run traditional simulation\n",
    "        trad_result = simulate_traditional_kv_cache(num_sequences, max_seq_length, hidden_dim)\n",
    "        traditional_efficiency.append(trad_result[\"efficiency\"])\n",
    "        \n",
    "        # Run paged attention simulation\n",
    "        paged_result = simulate_paged_attention(num_sequences, max_seq_length, hidden_dim, page_size)\n",
    "        paged_efficiency.append(paged_result[\"efficiency\"])\n",
    "    \n",
    "    return traditional_efficiency, paged_efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare memory efficiency for different numbers of sequences\n",
    "num_sequences_list = [10, 20, 50, 100, 200]\n",
    "traditional_efficiency, paged_efficiency = compare_memory_efficiency(\n",
    "    num_sequences_list,\n",
    "    max_seq_length=1024,\n",
    "    hidden_dim=64\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(num_sequences_list, [e * 100 for e in traditional_efficiency], marker='o', label=\"Traditional KV Cache\")\n",
    "plt.plot(num_sequences_list, [e * 100 for e in paged_efficiency], marker='s', label=\"Paged Attention\")\n",
    "plt.xlabel(\"Number of Sequences\")\n",
    "plt.ylabel(\"Memory Efficiency (%)\")\n",
    "plt.title(\"Memory Efficiency Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Impact of Page Size\n",
    "\n",
    "The page size is an important parameter in paged attention. Let's examine how it affects memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_page_size_impact(page_sizes, num_sequences, max_seq_length, hidden_dim):\n",
    "    \"\"\"Analyze the impact of page size on memory efficiency.\"\"\"\n",
    "    efficiency_results = []\n",
    "    \n",
    "    for page_size in page_sizes:\n",
    "        result = simulate_paged_attention(num_sequences, max_seq_length, hidden_dim, page_size)\n",
    "        efficiency_results.append(result[\"efficiency\"])\n",
    "    \n",
    "    return efficiency_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the impact of page size\n",
    "page_sizes = [4, 8, 16, 32, 64, 128]\n",
    "efficiency_by_page_size = analyze_page_size_impact(\n",
    "    page_sizes,\n",
    "    num_sequences=100,\n",
    "    max_seq_length=1024,\n",
    "    hidden_dim=64\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(page_sizes, [e * 100 for e in efficiency_by_page_size], marker='o')\n",
    "plt.xlabel(\"Page Size (tokens)\")\n",
    "plt.ylabel(\"Memory Efficiency (%)\")\n",
    "plt.title(\"Impact of Page Size on Memory Efficiency\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Simulating Concurrent Requests\n",
    "\n",
    "One of the key benefits of paged attention is supporting more concurrent requests. Let's simulate this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_concurrent_requests(max_memory, max_seq_length, hidden_dim, page_size=16):\n",
    "    \"\"\"Simulate how many concurrent requests can be handled with limited memory.\"\"\"\n",
    "    # Traditional approach\n",
    "    trad_max_sequences = max_memory // (max_seq_length * hidden_dim)\n",
    "    \n",
    "    # Paged attention approach\n",
    "    # Assuming average sequence length is 60% of max_seq_length\n",
    "    avg_seq_length = max_seq_length * 0.6\n",
    "    pages_per_seq = (avg_seq_length + page_size - 1) // page_size\n",
    "    total_pages = max_memory // (page_size * hidden_dim)\n",
    "    paged_max_sequences = total_pages // pages_per_seq\n",
    "    \n",
    "    return trad_max_sequences, paged_max_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate concurrent requests with different memory sizes\n",
    "memory_sizes = [1e6, 2e6, 5e6, 1e7, 2e7, 5e7]  # Different memory sizes\n",
    "trad_concurrent = []\n",
    "paged_concurrent = []\n",
    "\n",
    "for memory_size in memory_sizes:\n",
    "    trad, paged = simulate_concurrent_requests(\n",
    "        max_memory=int(memory_size),\n",
    "        max_seq_length=1024,\n",
    "        hidden_dim=64\n",
    "    )\n",
    "    trad_concurrent.append(trad)\n",
    "    paged_concurrent.append(paged)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(memory_sizes)), trad_concurrent, width=0.4, label=\"Traditional KV Cache\", align=\"edge\")\n",
    "plt.bar([x + 0.4 for x in range(len(memory_sizes))], paged_concurrent, width=0.4, label=\"Paged Attention\", align=\"edge\")\n",
    "plt.xlabel(\"Memory Size\")\n",
    "plt.ylabel(\"Max Concurrent Sequences\")\n",
    "plt.title(\"Maximum Concurrent Sequences by Memory Size\")\n",
    "plt.xticks([x + 0.2 for x in range(len(memory_sizes))], [f\"{int(m/1e6)}M\" for m in memory_sizes])\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored paged attention, a memory management technique that significantly improves the efficiency of KV caching for large language models.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. Paged attention divides the KV cache into fixed-size pages, enabling non-contiguous memory allocation\n",
    "2. This approach significantly reduces memory fragmentation and improves memory utilization\n",
    "3. The choice of page size affects memory efficiency - smaller pages reduce internal fragmentation but increase overhead\n",
    "4. Paged attention enables supporting more concurrent requests with the same amount of memory\n",
    "\n",
    "These benefits make paged attention a critical optimization for deploying large language models in production, especially for serving multiple users simultaneously."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
