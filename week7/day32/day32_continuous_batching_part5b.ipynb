{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 32: Continuous Batching - Part 5b\n",
    "\n",
    "Continuous batching allows requests to join and leave batches dynamically, improving efficiency over static batching.\n",
    "\n",
    "## Overview\n",
    "1. Understanding continuous batching\n",
    "2. Simple implementation\n",
    "3. Performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import queue\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Continuous Batching\n",
    "\n",
    "Unlike static batching, continuous batching:\n",
    "- Processes requests as they arrive\n",
    "- Removes completed sequences immediately\n",
    "- Adds new requests to existing batches\n",
    "- Maximizes GPU utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousBatchScheduler:\n",
    "    def __init__(self, max_batch_size=8, token_generation_time=0.05):\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.token_generation_time = token_generation_time\n",
    "        self.active_requests = {}\n",
    "        self.request_queue = queue.Queue()\n",
    "        self.completed_requests = []\n",
    "        self.running = False\n",
    "        \n",
    "    def submit_request(self, request_id, prompt, max_tokens=20):\n",
    "        request = {\n",
    "            'id': request_id,\n",
    "            'prompt': prompt,\n",
    "            'max_tokens': max_tokens,\n",
    "            'generated_tokens': 0,\n",
    "            'arrival_time': time.time(),\n",
    "            'start_time': None,\n",
    "            'completion_time': None,\n",
    "            'result': prompt\n",
    "        }\n",
    "        self.request_queue.put(request)\n",
    "    \n",
    "    def start(self):\n",
    "        self.running = True\n",
    "        self.thread = threading.Thread(target=self._process_continuously)\n",
    "        self.thread.daemon = True\n",
    "        self.thread.start()\n",
    "    \n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        if hasattr(self, 'thread'):\n",
    "            self.thread.join()\n",
    "    \n",
    "    def _process_continuously(self):\n",
    "        while self.running:\n",
    "            # Add new requests to active batch\n",
    "            while (not self.request_queue.empty() and \n",
    "                   len(self.active_requests) < self.max_batch_size):\n",
    "                request = self.request_queue.get()\n",
    "                request['start_time'] = time.time()\n",
    "                self.active_requests[request['id']] = request\n",
    "            \n",
    "            # Process active requests if any\n",
    "            if self.active_requests:\n",
    "                self._generate_tokens()\n",
    "                self._remove_completed()\n",
    "            else:\n",
    "                time.sleep(0.01)  # Small sleep if no active requests\n",
    "    \n",
    "    def _generate_tokens(self):\n",
    "        # Simulate token generation for all active requests\n",
    "        batch_size = len(self.active_requests)\n",
    "        \n",
    "        # Efficiency factor for batching\n",
    "        efficiency = max(0.5, 1.0 - 0.1 * np.log(batch_size))\n",
    "        \n",
    "        # Simulate generation time\n",
    "        time.sleep(self.token_generation_time * efficiency)\n",
    "        \n",
    "        # Update all active requests\n",
    "        for request in self.active_requests.values():\n",
    "            request['generated_tokens'] += 1\n",
    "            request['result'] += \" token\"\n",
    "    \n",
    "    def _remove_completed(self):\n",
    "        completed_ids = []\n",
    "        for req_id, request in self.active_requests.items():\n",
    "            if request['generated_tokens'] >= request['max_tokens']:\n",
    "                request['completion_time'] = time.time()\n",
    "                self.completed_requests.append(request)\n",
    "                completed_ids.append(req_id)\n",
    "        \n",
    "        # Remove completed requests\n",
    "        for req_id in completed_ids:\n",
    "            del self.active_requests[req_id]\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        if not self.completed_requests:\n",
    "            return {}\n",
    "        \n",
    "        latencies = []\n",
    "        for req in self.completed_requests:\n",
    "            latency = req['completion_time'] - req['arrival_time']\n",
    "            latencies.append(latency)\n",
    "        \n",
    "        total_time = max([r['completion_time'] for r in self.completed_requests]) - \\\n",
    "                    min([r['arrival_time'] for r in self.completed_requests])\n",
    "        \n",
    "        return {\n",
    "            'num_completed': len(self.completed_requests),\n",
    "            'avg_latency': np.mean(latencies),\n",
    "            'throughput': len(self.completed_requests) / total_time if total_time > 0 else 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing Continuous Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_continuous_batching(max_batch_size=8, num_requests=20, arrival_rate=3):\n",
    "    scheduler = ContinuousBatchScheduler(max_batch_size=max_batch_size)\n",
    "    scheduler.start()\n",
    "    \n",
    "    # Submit requests\n",
    "    for i in range(num_requests):\n",
    "        prompt = f\"Request {i}: AI will\"\n",
    "        max_tokens = np.random.randint(5, 15)\n",
    "        scheduler.submit_request(i, prompt, max_tokens)\n",
    "        time.sleep(1.0 / arrival_rate)\n",
    "    \n",
    "    # Wait for completion\n",
    "    while len(scheduler.completed_requests) < num_requests:\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    scheduler.stop()\n",
    "    return scheduler.get_metrics()\n",
    "\n",
    "# Test continuous batching\n",
    "metrics = test_continuous_batching(max_batch_size=8, num_requests=20, arrival_rate=3)\n",
    "print(f\"Completed: {metrics['num_completed']}\")\n",
    "print(f\"Avg Latency: {metrics['avg_latency']:.2f}s\")\n",
    "print(f\"Throughput: {metrics['throughput']:.2f} req/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparing Static vs Continuous Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple static batching for comparison\n",
    "class StaticBatchScheduler:\n",
    "    def __init__(self, batch_size=8, token_generation_time=0.05):\n",
    "        self.batch_size = batch_size\n",
    "        self.token_generation_time = token_generation_time\n",
    "        self.completed_requests = []\n",
    "    \n",
    "    def process_requests(self, requests):\n",
    "        # Process in fixed batches\n",
    "        for i in range(0, len(requests), self.batch_size):\n",
    "            batch = requests[i:i+self.batch_size]\n",
    "            max_tokens = max(req['max_tokens'] for req in batch)\n",
    "            \n",
    "            # Mark start time\n",
    "            start_time = time.time()\n",
    "            for req in batch:\n",
    "                req['start_time'] = start_time\n",
    "            \n",
    "            # Simulate processing entire batch\n",
    "            efficiency = max(0.5, 1.0 - 0.1 * np.log(len(batch)))\n",
    "            processing_time = max_tokens * self.token_generation_time * efficiency\n",
    "            time.sleep(processing_time)\n",
    "            \n",
    "            # Mark completion\n",
    "            completion_time = time.time()\n",
    "            for req in batch:\n",
    "                req['completion_time'] = completion_time\n",
    "                req['result'] = req['prompt'] + \" \" + \"token\" * req['max_tokens']\n",
    "                self.completed_requests.append(req)\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        if not self.completed_requests:\n",
    "            return {}\n",
    "        \n",
    "        latencies = []\n",
    "        for req in self.completed_requests:\n",
    "            latency = req['completion_time'] - req['arrival_time']\n",
    "            latencies.append(latency)\n",
    "        \n",
    "        total_time = max([r['completion_time'] for r in self.completed_requests]) - \\\n",
    "                    min([r['arrival_time'] for r in self.completed_requests])\n",
    "        \n",
    "        return {\n",
    "            'num_completed': len(self.completed_requests),\n",
    "            'avg_latency': np.mean(latencies),\n",
    "            'throughput': len(self.completed_requests) / total_time if total_time > 0 else 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_scheduling_methods(num_requests=20, arrival_rate=3):\n",
    "    # Create requests\n",
    "    requests = []\n",
    "    arrival_time = time.time()\n",
    "    \n",
    "    for i in range(num_requests):\n",
    "        req = {\n",
    "            'id': i,\n",
    "            'prompt': f\"Request {i}: AI will\",\n",
    "            'max_tokens': np.random.randint(5, 15),\n",
    "            'arrival_time': arrival_time + i / arrival_rate\n",
    "        }\n",
    "        requests.append(req)\n",
    "    \n",
    "    # Test static batching\n",
    "    static_scheduler = StaticBatchScheduler(batch_size=8)\n",
    "    static_requests = [req.copy() for req in requests]\n",
    "    static_scheduler.process_requests(static_requests)\n",
    "    static_metrics = static_scheduler.get_metrics()\n",
    "    \n",
    "    # Test continuous batching\n",
    "    continuous_metrics = test_continuous_batching(max_batch_size=8, num_requests=num_requests, arrival_rate=arrival_rate)\n",
    "    \n",
    "    return static_metrics, continuous_metrics\n",
    "\n",
    "# Compare methods\n",
    "static_metrics, continuous_metrics = compare_scheduling_methods()\n",
    "\n",
    "print(\"Static Batching:\")\n",
    "print(f\"  Avg Latency: {static_metrics['avg_latency']:.2f}s\")\n",
    "print(f\"  Throughput: {static_metrics['throughput']:.2f} req/s\")\n",
    "\n",
    "print(\"\\nContinuous Batching:\")\n",
    "print(f\"  Avg Latency: {continuous_metrics['avg_latency']:.2f}s\")\n",
    "print(f\"  Throughput: {continuous_metrics['throughput']:.2f} req/s\")\n",
    "\n",
    "print(f\"\\nImprovement:\")\n",
    "print(f\"  Latency: {static_metrics['avg_latency']/continuous_metrics['avg_latency']:.2f}x better\")\n",
    "print(f\"  Throughput: {continuous_metrics['throughput']/static_metrics['throughput']:.2f}x better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing the Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different batch sizes\n",
    "batch_sizes = [2, 4, 8, 16]\n",
    "static_latencies = []\n",
    "continuous_latencies = []\n",
    "static_throughputs = []\n",
    "continuous_throughputs = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"Testing batch size: {batch_size}\")\n",
    "    \n",
    "    # Static batching\n",
    "    static_scheduler = StaticBatchScheduler(batch_size=batch_size)\n",
    "    requests = [{\n",
    "        'id': i,\n",
    "        'prompt': f\"Request {i}\",\n",
    "        'max_tokens': 10,\n",
    "        'arrival_time': time.time() + i * 0.1\n",
    "    } for i in range(20)]\n",
    "    \n",
    "    static_scheduler.process_requests([req.copy() for req in requests])\n",
    "    static_metrics = static_scheduler.get_metrics()\n",
    "    \n",
    "    # Continuous batching\n",
    "    continuous_metrics = test_continuous_batching(max_batch_size=batch_size, num_requests=20, arrival_rate=10)\n",
    "    \n",
    "    static_latencies.append(static_metrics['avg_latency'])\n",
    "    continuous_latencies.append(continuous_metrics['avg_latency'])\n",
    "    static_throughputs.append(static_metrics['throughput'])\n",
    "    continuous_throughputs.append(continuous_metrics['throughput'])\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(batch_sizes, static_latencies, 'o-', label='Static Batching')\n",
    "plt.plot(batch_sizes, continuous_latencies, 's-', label='Continuous Batching')\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Average Latency (s)')\n",
    "plt.title('Latency Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(batch_sizes, static_throughputs, 'o-', label='Static Batching')\n",
    "plt.plot(batch_sizes, continuous_throughputs, 's-', label='Continuous Batching')\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Throughput (req/s)')\n",
    "plt.title('Throughput Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Continuous batching provides significant advantages over static batching:\n",
    "\n",
    "1. **Lower Latency**: Requests start processing immediately\n",
    "2. **Higher Throughput**: Better resource utilization\n",
    "3. **Flexibility**: Adapts to varying request patterns\n",
    "4. **Efficiency**: No waiting for batch formation\n",
    "\n",
    "This makes continuous batching essential for production LLM serving systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
