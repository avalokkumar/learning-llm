{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 32: Batching and Throughput Optimization - Part 2\n",
    "\n",
    "In this notebook, we'll explore batching strategies for optimizing throughput when serving large language models. We'll implement basic batching and measure its impact on overall throughput.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Setup and dependencies\n",
    "2. Understanding batching for LLMs\n",
    "3. Implementing static batching\n",
    "4. Measuring throughput improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers datasets evaluate accelerate matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Batching for LLMs\n",
    "\n",
    "Batching is a technique where multiple requests are processed together to maximize hardware utilization. For language models, this means generating tokens for multiple prompts simultaneously.\n",
    "\n",
    "Benefits of batching include:\n",
    "1. Better utilization of parallel processing capabilities (especially on GPUs)\n",
    "2. Amortized overhead of model execution\n",
    "3. Higher overall throughput (tokens per second)\n",
    "\n",
    "However, batching can increase latency for individual requests, so there's a trade-off between throughput and latency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading a Pre-trained Model\n",
    "\n",
    "Let's load a small pre-trained model to demonstrate batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model name\n",
    "model_name = \"gpt2\"  # Using a smaller model for demonstration\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Print model information\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing Batched Generation\n",
    "\n",
    "Let's implement a function to generate text for multiple prompts in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(model, tokenizer, prompts, max_length=50, temperature=1.0):\n",
    "    \"\"\"Generate text for multiple prompts in a batch.\"\"\"\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Tokenize all prompts\n",
    "    batch_inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=batch_inputs[\"input_ids\"],\n",
    "            attention_mask=batch_inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    \n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    generation_time = end_time - start_time\n",
    "    \n",
    "    # Calculate tokens generated\n",
    "    total_tokens = sum(len(output) for output in outputs)\n",
    "    tokens_per_second = total_tokens / generation_time\n",
    "    \n",
    "    return generated_texts, generation_time, tokens_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequentially(model, tokenizer, prompts, max_length=50, temperature=1.0):\n",
    "    \"\"\"Generate text for multiple prompts sequentially (one by one).\"\"\"\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    generated_texts = []\n",
    "    total_tokens = 0\n",
    "    \n",
    "    # Process each prompt sequentially\n",
    "    for prompt in prompts:\n",
    "        # Tokenize prompt\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Generate text\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                max_length=max_length,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                use_cache=True\n",
    "            )\n",
    "        \n",
    "        # Decode the generated text\n",
    "        text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_texts.append(text)\n",
    "        total_tokens += len(outputs[0])\n",
    "    \n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    generation_time = end_time - start_time\n",
    "    tokens_per_second = total_tokens / generation_time\n",
    "    \n",
    "    return generated_texts, generation_time, tokens_per_second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Measuring Throughput Improvements\n",
    "\n",
    "Let's compare the throughput of batched vs. sequential generation for different batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test prompts\n",
    "prompts = [\n",
    "    \"Artificial intelligence will transform the future by\",\n",
    "    \"The key challenges in climate change are\",\n",
    "    \"Space exploration in the next decade will focus on\",\n",
    "    \"Quantum computing offers advantages such as\",\n",
    "    \"The future of renewable energy depends on\",\n",
    "    \"Biotechnology innovations are changing medicine through\",\n",
    "    \"Smart cities of tomorrow will incorporate\",\n",
    "    \"The most significant ethical concerns in technology are\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sequential generation\n",
    "print(\"Testing sequential generation...\")\n",
    "sequential_texts, sequential_time, sequential_tps = generate_sequentially(\n",
    "    model, tokenizer, prompts, max_length=50, temperature=0.7\n",
    ")\n",
    "\n",
    "print(f\"Sequential generation time: {sequential_time:.4f} seconds\")\n",
    "print(f\"Sequential throughput: {sequential_tps:.2f} tokens per second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test batched generation\n",
    "print(\"Testing batched generation...\")\n",
    "batched_texts, batched_time, batched_tps = generate_batch(\n",
    "    model, tokenizer, prompts, max_length=50, temperature=0.7\n",
    ")\n",
    "\n",
    "print(f\"Batched generation time: {batched_time:.4f} seconds\")\n",
    "print(f\"Batched throughput: {batched_tps:.2f} tokens per second\")\n",
    "print(f\"Speedup: {batched_tps / sequential_tps:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyzing the Impact of Batch Size\n",
    "\n",
    "Let's analyze how throughput changes with different batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_batch_performance(model, tokenizer, prompts, batch_sizes):\n",
    "    \"\"\"Measure performance for different batch sizes.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # First, measure sequential performance (batch size 1)\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"Testing batch size: {batch_size}\")\n",
    "        \n",
    "        # Create batches\n",
    "        num_batches = len(prompts) // batch_size\n",
    "        if len(prompts) % batch_size != 0:\n",
    "            num_batches += 1\n",
    "        \n",
    "        total_time = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            batch_prompts = prompts[i * batch_size : min((i + 1) * batch_size, len(prompts))]\n",
    "            \n",
    "            if len(batch_prompts) == 1:\n",
    "                # Sequential generation for batch size 1\n",
    "                _, time_taken, tokens_per_second = generate_sequentially(\n",
    "                    model, tokenizer, batch_prompts, max_length=50, temperature=0.7\n",
    "                )\n",
    "            else:\n",
    "                # Batched generation\n",
    "                _, time_taken, tokens_per_second = generate_batch(\n",
    "                    model, tokenizer, batch_prompts, max_length=50, temperature=0.7\n",
    "                )\n",
    "            \n",
    "            # Estimate tokens generated (approximate)\n",
    "            tokens_generated = tokens_per_second * time_taken\n",
    "            \n",
    "            total_time += time_taken\n",
    "            total_tokens += tokens_generated\n",
    "        \n",
    "        avg_throughput = total_tokens / total_time\n",
    "        \n",
    "        results.append({\n",
    "            \"batch_size\": batch_size,\n",
    "            \"total_time\": total_time,\n",
    "            \"throughput\": avg_throughput\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different batch sizes\n",
    "batch_sizes = [1, 2, 4, 8]\n",
    "batch_results = measure_batch_performance(model, tokenizer, prompts, batch_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"Batch Size Performance Results:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Batch Size':<15} {'Total Time (s)':<20} {'Throughput (tokens/s)':<25}\")\n",
    "print(\"-\" * 60)\n",
    "for result in batch_results:\n",
    "    print(f\"{result['batch_size']:<15} {result['total_time']:<20.4f} {result['throughput']:<25.2f}\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot throughput vs batch size\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(\n",
    "    [r[\"batch_size\"] for r in batch_results],\n",
    "    [r[\"throughput\"] for r in batch_results],\n",
    "    marker='o',\n",
    "    linewidth=2\n",
    ")\n",
    "plt.title(\"Throughput vs Batch Size\")\n",
    "plt.xlabel(\"Batch Size\")\n",
    "plt.ylabel(\"Throughput (tokens/s)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot total time vs batch size\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(\n",
    "    [r[\"batch_size\"] for r in batch_results],\n",
    "    [r[\"total_time\"] for r in batch_results],\n",
    "    marker='o',\n",
    "    linewidth=2,\n",
    "    color='green'\n",
    ")\n",
    "plt.title(\"Total Time vs Batch Size\")\n",
    "plt.xlabel(\"Batch Size\")\n",
    "plt.ylabel(\"Total Time (seconds)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyzing Latency vs. Throughput Trade-off\n",
    "\n",
    "Let's examine the trade-off between latency (time per request) and throughput (total tokens per second)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate latency per request\n",
    "for result in batch_results:\n",
    "    # Average time per request\n",
    "    result[\"latency_per_request\"] = result[\"total_time\"] / len(prompts)\n",
    "\n",
    "# Display results with latency\n",
    "print(\"Latency vs. Throughput Trade-off:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Batch Size':<15} {'Throughput (tokens/s)':<25} {'Avg Latency per Request (s)':<30}\")\n",
    "print(\"-\" * 80)\n",
    "for result in batch_results:\n",
    "    print(f\"{result['batch_size']:<15} {result['throughput']:<25.2f} {result['latency_per_request']:<30.4f}\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trade-off\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(\n",
    "    [r[\"latency_per_request\"] for r in batch_results],\n",
    "    [r[\"throughput\"] for r in batch_results],\n",
    "    s=100,  # marker size\n",
    "    c=range(len(batch_results)),  # color by batch size\n",
    "    cmap='viridis',\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "# Add labels for each point\n",
    "for i, result in enumerate(batch_results):\n",
    "    plt.annotate(\n",
    "        f\"Batch={result['batch_size']}\",\n",
    "        (result[\"latency_per_request\"], result[\"throughput\"]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0, 10),\n",
    "        ha='center'\n",
    "    )\n",
    "\n",
    "plt.title(\"Throughput vs. Latency Trade-off\", fontsize=14)\n",
    "plt.xlabel(\"Latency per Request (seconds)\", fontsize=12)\n",
    "plt.ylabel(\"Throughput (tokens/second)\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practical Considerations for Batching\n",
    "\n",
    "When implementing batching in production, consider the following factors:\n",
    "\n",
    "1. **Optimal Batch Size**: The ideal batch size depends on your hardware, model size, and request patterns\n",
    "2. **Variable Sequence Lengths**: Handling sequences of different lengths efficiently\n",
    "3. **Latency Requirements**: Some applications prioritize low latency over throughput\n",
    "4. **Memory Constraints**: Larger batch sizes require more memory\n",
    "5. **Dynamic Batching**: Adapting batch size based on current load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored batching strategies for optimizing throughput when serving large language models. We've implemented basic batching and measured its impact on overall throughput.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. Batching significantly improves throughput by better utilizing GPU parallelism\n",
    "2. There's a trade-off between throughput and latency\n",
    "3. The optimal batch size depends on hardware, model size, and application requirements\n",
    "4. For latency-sensitive applications, smaller batch sizes may be preferred\n",
    "5. For throughput-oriented applications, larger batch sizes are generally better\n",
    "\n",
    "In the next part, we'll explore more advanced techniques like continuous batching and dynamic scheduling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
