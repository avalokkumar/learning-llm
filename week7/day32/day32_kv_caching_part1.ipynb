{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 32: KV Caching Implementation - Part 1\n",
    "\n",
    "In this notebook, we'll explore Key-Value (KV) caching, a fundamental optimization technique for efficient inference in large language models. We'll implement a basic version of KV caching and measure its impact on inference speed.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Setup and dependencies\n",
    "2. Understanding KV caching\n",
    "3. Implementing KV caching from scratch\n",
    "4. Measuring performance improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers datasets evaluate accelerate matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding KV Caching\n",
    "\n",
    "KV caching is an optimization technique that stores the key (K) and value (V) tensors computed during the forward pass of a transformer model. When generating text token by token, instead of recomputing these tensors for all tokens in each step, we reuse the cached values from previous steps and only compute K and V for the new token.\n",
    "\n",
    "This significantly reduces the computational cost of autoregressive generation, especially for long sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The Transformer Attention Mechanism\n",
    "\n",
    "Let's first review how attention works in transformer models:\n",
    "\n",
    "1. Input tokens are embedded and passed through the model\n",
    "2. For each layer, we compute query (Q), key (K), and value (V) matrices\n",
    "3. Attention scores are computed as `softmax(Q * K^T / sqrt(d_k))`\n",
    "4. The output is `attention_scores * V`\n",
    "\n",
    "During autoregressive generation, we generate one token at a time. Without KV caching, we would recompute Q, K, and V for all tokens in each step, which is inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 KV Caching Approach\n",
    "\n",
    "With KV caching:\n",
    "\n",
    "1. We compute and store K and V for all tokens in the initial input\n",
    "2. For each new token, we only compute K and V for that token and append to the cache\n",
    "3. We compute Q only for the new token\n",
    "4. We use the cached K and V along with the new Q to compute attention\n",
    "\n",
    "This reduces the computational complexity from O(nÂ²) to O(n) for sequence generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading a Pre-trained Model\n",
    "\n",
    "Let's load a small pre-trained model to demonstrate KV caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model name\n",
    "model_name = \"gpt2\"  # Using a smaller model for demonstration\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Print model information\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "print(f\"Number of layers: {len(model.transformer.h)}\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"Number of attention heads: {model.config.num_attention_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing KV Caching from Scratch\n",
    "\n",
    "Now, let's implement a basic version of KV caching for autoregressive generation. We'll create two functions:\n",
    "\n",
    "1. `generate_without_kv_cache`: Standard generation without KV caching\n",
    "2. `generate_with_kv_cache`: Optimized generation with KV caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_without_kv_cache(model, input_ids, max_length=50, temperature=1.0):\n",
    "    \"\"\"Generate text without using KV caching.\"\"\"\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Move input to device\n",
    "    input_ids = input_ids.to(device)\n",
    "    current_length = input_ids.shape[1]\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    for _ in range(max_length - current_length):\n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            # Apply temperature\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append the new token to the sequence\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "    \n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    generation_time = end_time - start_time\n",
    "    \n",
    "    return input_ids, generation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_kv_cache(model, input_ids, max_length=50, temperature=1.0):\n",
    "    \"\"\"Generate text using KV caching.\"\"\"\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Move input to device\n",
    "    input_ids = input_ids.to(device)\n",
    "    current_length = input_ids.shape[1]\n",
    "    \n",
    "    # Initialize the KV cache\n",
    "    past_key_values = None\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    for _ in range(max_length - current_length):\n",
    "        # Forward pass through the model with past_key_values\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids if past_key_values is None else input_ids[:, -1:],\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True\n",
    "            )\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            # Update the KV cache\n",
    "            past_key_values = outputs.past_key_values\n",
    "            \n",
    "            # Apply temperature\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append the new token to the sequence\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "    \n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    generation_time = end_time - start_time\n",
    "    \n",
    "    return input_ids, generation_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Measuring Performance Improvements\n",
    "\n",
    "Now, let's compare the performance of generation with and without KV caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test prompt\n",
    "prompt = \"Artificial intelligence will transform the future by\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text without KV caching\n",
    "print(\"Generating without KV caching...\")\n",
    "output_without_cache, time_without_cache = generate_without_kv_cache(\n",
    "    model, input_ids.clone(), max_length=100, temperature=0.7\n",
    ")\n",
    "\n",
    "# Generate text with KV caching\n",
    "print(\"Generating with KV caching...\")\n",
    "output_with_cache, time_with_cache = generate_with_kv_cache(\n",
    "    model, input_ids.clone(), max_length=100, temperature=0.7\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(f\"\\nGeneration time without KV caching: {time_without_cache:.4f} seconds\")\n",
    "print(f\"Generation time with KV caching: {time_with_cache:.4f} seconds\")\n",
    "print(f\"Speedup: {time_without_cache / time_with_cache:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the generated text\n",
    "text_without_cache = tokenizer.decode(output_without_cache[0], skip_special_tokens=True)\n",
    "text_with_cache = tokenizer.decode(output_with_cache[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated text without KV caching:\")\n",
    "print(text_without_cache)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "print(\"Generated text with KV caching:\")\n",
    "print(text_with_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyzing KV Cache Memory Usage\n",
    "\n",
    "Let's analyze the memory usage of the KV cache for different sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kv_cache_size(model, seq_length):\n",
    "    \"\"\"Calculate the size of the KV cache for a given sequence length.\"\"\"\n",
    "    # Get model configuration\n",
    "    num_layers = len(model.transformer.h)\n",
    "    hidden_size = model.config.hidden_size\n",
    "    num_heads = model.config.num_attention_heads\n",
    "    head_dim = hidden_size // num_heads\n",
    "    \n",
    "    # Calculate size in bytes (assuming FP16 - 2 bytes per element)\n",
    "    bytes_per_element = 2  # FP16\n",
    "    \n",
    "    # Each layer has both K and V caches\n",
    "    # Each cache has shape [batch_size, num_heads, seq_length, head_dim]\n",
    "    batch_size = 1\n",
    "    kv_cache_size = 2 * num_layers * batch_size * num_heads * seq_length * head_dim * bytes_per_element\n",
    "    \n",
    "    # Convert to MB\n",
    "    kv_cache_size_mb = kv_cache_size / (1024 * 1024)\n",
    "    \n",
    "    return kv_cache_size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate KV cache size for different sequence lengths\n",
    "seq_lengths = [128, 256, 512, 1024, 2048, 4096, 8192]\n",
    "cache_sizes = [calculate_kv_cache_size(model, length) for length in seq_lengths]\n",
    "\n",
    "# Print the results\n",
    "print(\"KV Cache Size Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Sequence Length':<20} {'Cache Size (MB)':<15}\")\n",
    "print(\"-\" * 40)\n",
    "for length, size in zip(seq_lengths, cache_sizes):\n",
    "    print(f\"{length:<20} {size:<15.2f}\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the KV cache size growth\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(seq_lengths, cache_sizes, marker='o', linewidth=2)\n",
    "plt.title('KV Cache Size vs. Sequence Length', fontsize=14)\n",
    "plt.xlabel('Sequence Length', fontsize=12)\n",
    "plt.ylabel('Cache Size (MB)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log', base=2)\n",
    "plt.xticks(seq_lengths, [str(x) for x in seq_lengths])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Measuring Speedup for Different Sequence Lengths\n",
    "\n",
    "Let's measure the speedup provided by KV caching for different sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_speedup(model, tokenizer, prompt, gen_length, num_runs=3):\n",
    "    \"\"\"Measure speedup of KV caching for a given prompt and generation length.\"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Measure time without KV caching\n",
    "    times_without_cache = []\n",
    "    for _ in range(num_runs):\n",
    "        _, time_without_cache = generate_without_kv_cache(\n",
    "            model, input_ids.clone(), max_length=len(input_ids[0])+gen_length, temperature=0.7\n",
    "        )\n",
    "        times_without_cache.append(time_without_cache)\n",
    "    avg_time_without_cache = sum(times_without_cache) / len(times_without_cache)\n",
    "    \n",
    "    # Measure time with KV caching\n",
    "    times_with_cache = []\n",
    "    for _ in range(num_runs):\n",
    "        _, time_with_cache = generate_with_kv_cache(\n",
    "            model, input_ids.clone(), max_length=len(input_ids[0])+gen_length, temperature=0.7\n",
    "        )\n",
    "        times_with_cache.append(time_with_cache)\n",
    "    avg_time_with_cache = sum(times_with_cache) / len(times_with_cache)\n",
    "    \n",
    "    # Calculate speedup\n",
    "    speedup = avg_time_without_cache / avg_time_with_cache\n",
    "    \n",
    "    return avg_time_without_cache, avg_time_with_cache, speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test cases with different generation lengths\n",
    "gen_lengths = [10, 20, 50, 100]\n",
    "results = []\n",
    "\n",
    "# Measure speedup for each generation length\n",
    "for gen_length in gen_lengths:\n",
    "    print(f\"Measuring speedup for generation length: {gen_length}\")\n",
    "    time_without_cache, time_with_cache, speedup = measure_speedup(\n",
    "        model, tokenizer, prompt, gen_length\n",
    "    )\n",
    "    results.append({\n",
    "        \"gen_length\": gen_length,\n",
    "        \"time_without_cache\": time_without_cache,\n",
    "        \"time_with_cache\": time_with_cache,\n",
    "        \"speedup\": speedup\n",
    "    })\n",
    "    print(f\"  Time without cache: {time_without_cache:.4f}s\")\n",
    "    print(f\"  Time with cache: {time_with_cache:.4f}s\")\n",
    "    print(f\"  Speedup: {speedup:.2f}x\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the speedup results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot generation times\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(\n",
    "    [str(r[\"gen_length\"]) for r in results], \n",
    "    [r[\"time_without_cache\"] for r in results],\n",
    "    label=\"Without KV Cache\",\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.bar(\n",
    "    [str(r[\"gen_length\"]) for r in results], \n",
    "    [r[\"time_with_cache\"] for r in results],\n",
    "    label=\"With KV Cache\",\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title(\"Generation Time Comparison\")\n",
    "plt.xlabel(\"Generation Length\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.legend()\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Plot speedup\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(\n",
    "    [str(r[\"gen_length\"]) for r in results], \n",
    "    [r[\"speedup\"] for r in results],\n",
    "    color=\"green\"\n",
    ")\n",
    "plt.title(\"Speedup Factor\")\n",
    "plt.xlabel(\"Generation Length\")\n",
    "plt.ylabel(\"Speedup (x)\")\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Understanding the Trade-offs\n",
    "\n",
    "KV caching provides significant speedup for autoregressive generation, but it comes with trade-offs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Memory Usage\n",
    "\n",
    "The KV cache consumes memory proportional to the sequence length. For large models and long sequences, this can be substantial.\n",
    "\n",
    "For example, for a model like GPT-3 (175B parameters) with 2048 tokens, the KV cache can be several gigabytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate KV cache size for a large model (hypothetical)\n",
    "def calculate_large_model_kv_cache(seq_length, num_layers=96, hidden_size=12288, num_heads=96):\n",
    "    \"\"\"Calculate KV cache size for a large model like GPT-3.\"\"\"\n",
    "    head_dim = hidden_size // num_heads\n",
    "    bytes_per_element = 2  # FP16\n",
    "    batch_size = 1\n",
    "    \n",
    "    kv_cache_size = 2 * num_layers * batch_size * num_heads * seq_length * head_dim * bytes_per_element\n",
    "    kv_cache_size_gb = kv_cache_size / (1024 * 1024 * 1024)\n",
    "    \n",
    "    return kv_cache_size_gb\n",
    "\n",
    "# Calculate for different sequence lengths\n",
    "large_seq_lengths = [1024, 2048, 4096, 8192, 16384, 32768]\n",
    "large_cache_sizes = [calculate_large_model_kv_cache(length) for length in large_seq_lengths]\n",
    "\n",
    "# Print the results\n",
    "print(\"KV Cache Size for Large Model (GPT-3 scale):\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Sequence Length':<20} {'Cache Size (GB)':<15}\")\n",
    "print(\"-\" * 40)\n",
    "for length, size in zip(large_seq_lengths, large_cache_sizes):\n",
    "    print(f\"{length:<20} {size:<15.2f}\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Batch Processing\n",
    "\n",
    "KV caching is most beneficial for autoregressive generation. For batch processing of fixed-length sequences, the benefits may be less significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored KV caching, a fundamental optimization technique for efficient inference in large language models. We've implemented a basic version of KV caching and measured its impact on inference speed.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. KV caching significantly speeds up autoregressive generation by reusing previously computed key and value tensors\n",
    "2. The speedup increases with the length of the generated sequence\n",
    "3. KV caching trades memory for computation, with memory usage growing linearly with sequence length\n",
    "4. For large models and long sequences, memory management becomes critical\n",
    "\n",
    "In the next part, we'll explore more advanced techniques like paged attention, which addresses the memory management challenges of KV caching."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
