{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 32: Priority-Based Scheduling - Part 5c\n",
    "\n",
    "Priority-based scheduling allows different requests to be processed with different priorities, enabling SLA guarantees and fair resource allocation.\n",
    "\n",
    "## Overview\n",
    "1. Understanding priority scheduling\n",
    "2. Implementation with priority queues\n",
    "3. Measuring fairness and SLA compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import heapq\n",
    "import threading\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Priority Levels and Request Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Priority(Enum):\n",
    "    HIGH = 1\n",
    "    MEDIUM = 2\n",
    "    LOW = 3\n",
    "\n",
    "class PriorityRequest:\n",
    "    def __init__(self, id, prompt, priority, max_tokens=20, sla_target=None):\n",
    "        self.id = id\n",
    "        self.prompt = prompt\n",
    "        self.priority = priority\n",
    "        self.max_tokens = max_tokens\n",
    "        self.sla_target = sla_target  # Target completion time in seconds\n",
    "        self.arrival_time = time.time()\n",
    "        self.start_time = None\n",
    "        self.completion_time = None\n",
    "        self.generated_tokens = 0\n",
    "        self.result = prompt\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        # For heapq - lower priority value = higher priority\n",
    "        if self.priority.value != other.priority.value:\n",
    "            return self.priority.value < other.priority.value\n",
    "        # If same priority, use arrival time (FIFO)\n",
    "        return self.arrival_time < other.arrival_time\n",
    "    \n",
    "    @property\n",
    "    def latency(self):\n",
    "        if self.completion_time is None:\n",
    "            return None\n",
    "        return self.completion_time - self.arrival_time\n",
    "    \n",
    "    @property\n",
    "    def sla_met(self):\n",
    "        if self.sla_target is None or self.latency is None:\n",
    "            return None\n",
    "        return self.latency <= self.sla_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Priority-Based Scheduler Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriorityScheduler:\n",
    "    def __init__(self, max_batch_size=8, token_generation_time=0.05):\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.token_generation_time = token_generation_time\n",
    "        self.request_heap = []  # Priority queue\n",
    "        self.active_requests = {}\n",
    "        self.completed_requests = []\n",
    "        self.running = False\n",
    "        self.lock = threading.Lock()\n",
    "    \n",
    "    def submit_request(self, request):\n",
    "        with self.lock:\n",
    "            heapq.heappush(self.request_heap, request)\n",
    "    \n",
    "    def start(self):\n",
    "        self.running = True\n",
    "        self.thread = threading.Thread(target=self._process_with_priority)\n",
    "        self.thread.daemon = True\n",
    "        self.thread.start()\n",
    "    \n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        if hasattr(self, 'thread'):\n",
    "            self.thread.join()\n",
    "    \n",
    "    def _process_with_priority(self):\n",
    "        while self.running:\n",
    "            # Add high-priority requests first\n",
    "            self._add_requests_by_priority()\n",
    "            \n",
    "            # Process active requests\n",
    "            if self.active_requests:\n",
    "                self._generate_tokens()\n",
    "                self._remove_completed()\n",
    "            else:\n",
    "                time.sleep(0.01)\n",
    "    \n",
    "    def _add_requests_by_priority(self):\n",
    "        with self.lock:\n",
    "            # Add requests up to batch size, prioritizing high-priority ones\n",
    "            while (len(self.active_requests) < self.max_batch_size and \n",
    "                   self.request_heap):\n",
    "                request = heapq.heappop(self.request_heap)\n",
    "                request.start_time = time.time()\n",
    "                self.active_requests[request.id] = request\n",
    "    \n",
    "    def _generate_tokens(self):\n",
    "        batch_size = len(self.active_requests)\n",
    "        efficiency = max(0.5, 1.0 - 0.1 * np.log(batch_size))\n",
    "        time.sleep(self.token_generation_time * efficiency)\n",
    "        \n",
    "        # Update all active requests\n",
    "        for request in self.active_requests.values():\n",
    "            request.generated_tokens += 1\n",
    "            request.result += \" token\"\n",
    "    \n",
    "    def _remove_completed(self):\n",
    "        completed_ids = []\n",
    "        for req_id, request in self.active_requests.items():\n",
    "            if request.generated_tokens >= request.max_tokens:\n",
    "                request.completion_time = time.time()\n",
    "                self.completed_requests.append(request)\n",
    "                completed_ids.append(req_id)\n",
    "        \n",
    "        for req_id in completed_ids:\n",
    "            del self.active_requests[req_id]\n",
    "    \n",
    "    def get_metrics_by_priority(self):\n",
    "        metrics = {}\n",
    "        \n",
    "        for priority in Priority:\n",
    "            priority_requests = [r for r in self.completed_requests \n",
    "                               if r.priority == priority]\n",
    "            \n",
    "            if priority_requests:\n",
    "                latencies = [r.latency for r in priority_requests]\n",
    "                sla_compliance = [r.sla_met for r in priority_requests \n",
    "                                if r.sla_met is not None]\n",
    "                \n",
    "                metrics[priority.name] = {\n",
    "                    'count': len(priority_requests),\n",
    "                    'avg_latency': np.mean(latencies),\n",
    "                    'max_latency': np.max(latencies),\n",
    "                    'sla_compliance': np.mean(sla_compliance) if sla_compliance else None\n",
    "                }\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing Priority Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_priority_scheduling():\n",
    "    scheduler = PriorityScheduler(max_batch_size=4)\n",
    "    scheduler.start()\n",
    "    \n",
    "    # Submit mixed priority requests\n",
    "    requests = [\n",
    "        # High priority with tight SLA\n",
    "        PriorityRequest(1, \"Urgent: Stock price\", Priority.HIGH, 10, sla_target=2.0),\n",
    "        PriorityRequest(2, \"Critical: System alert\", Priority.HIGH, 8, sla_target=1.5),\n",
    "        \n",
    "        # Medium priority\n",
    "        PriorityRequest(3, \"Normal: User query\", Priority.MEDIUM, 15, sla_target=5.0),\n",
    "        PriorityRequest(4, \"Standard: Report gen\", Priority.MEDIUM, 12, sla_target=4.0),\n",
    "        \n",
    "        # Low priority\n",
    "        PriorityRequest(5, \"Batch: Data analysis\", Priority.LOW, 20, sla_target=10.0),\n",
    "        PriorityRequest(6, \"Background: Summary\", Priority.LOW, 18, sla_target=8.0),\n",
    "    ]\n",
    "    \n",
    "    # Submit requests with some delay\n",
    "    for i, request in enumerate(requests):\n",
    "        scheduler.submit_request(request)\n",
    "        time.sleep(0.2)  # Small delay between submissions\n",
    "    \n",
    "    # Wait for completion\n",
    "    while len(scheduler.completed_requests) < len(requests):\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    scheduler.stop()\n",
    "    return scheduler\n",
    "\n",
    "# Run test\n",
    "scheduler = test_priority_scheduling()\n",
    "metrics = scheduler.get_metrics_by_priority()\n",
    "\n",
    "# Display results\n",
    "for priority_name, data in metrics.items():\n",
    "    print(f\"\\n{priority_name} Priority:\")\n",
    "    print(f\"  Count: {data['count']}\")\n",
    "    print(f\"  Avg Latency: {data['avg_latency']:.2f}s\")\n",
    "    print(f\"  Max Latency: {data['max_latency']:.2f}s\")\n",
    "    if data['sla_compliance'] is not None:\n",
    "        print(f\"  SLA Compliance: {data['sla_compliance']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing FIFO vs Priority Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_fifo_vs_priority():\n",
    "    # Create test requests\n",
    "    test_requests = [\n",
    "        # Mix of priorities arriving in non-optimal order\n",
    "        PriorityRequest(1, \"Low priority task\", Priority.LOW, 20),\n",
    "        PriorityRequest(2, \"Another low task\", Priority.LOW, 18),\n",
    "        PriorityRequest(3, \"HIGH PRIORITY!\", Priority.HIGH, 5),\n",
    "        PriorityRequest(4, \"Medium task\", Priority.MEDIUM, 12),\n",
    "        PriorityRequest(5, \"URGENT HIGH!\", Priority.HIGH, 8),\n",
    "        PriorityRequest(6, \"Low priority again\", Priority.LOW, 15),\n",
    "    ]\n",
    "    \n",
    "    # Test FIFO (simple continuous batching)\n",
    "    fifo_completion_order = []\n",
    "    fifo_latencies = []\n",
    "    \n",
    "    # Simulate FIFO processing\n",
    "    current_time = 0\n",
    "    for i, request in enumerate(test_requests):\n",
    "        # Processing time proportional to tokens\n",
    "        processing_time = request.max_tokens * 0.1\n",
    "        current_time += processing_time\n",
    "        \n",
    "        fifo_completion_order.append(request.priority.name)\n",
    "        fifo_latencies.append(current_time)\n",
    "    \n",
    "    # Test Priority Scheduling\n",
    "    priority_scheduler = PriorityScheduler(max_batch_size=1)  # Process one at a time for clarity\n",
    "    priority_scheduler.start()\n",
    "    \n",
    "    for request in test_requests:\n",
    "        priority_scheduler.submit_request(request)\n",
    "        time.sleep(0.05)  # Small delay\n",
    "    \n",
    "    # Wait for completion\n",
    "    while len(priority_scheduler.completed_requests) < len(test_requests):\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    priority_scheduler.stop()\n",
    "    \n",
    "    # Analyze priority scheduling results\n",
    "    priority_completion_order = []\n",
    "    priority_latencies = []\n",
    "    \n",
    "    # Sort by completion time to get order\n",
    "    sorted_requests = sorted(priority_scheduler.completed_requests, \n",
    "                           key=lambda r: r.completion_time)\n",
    "    \n",
    "    for request in sorted_requests:\n",
    "        priority_completion_order.append(request.priority.name)\n",
    "        priority_latencies.append(request.latency)\n",
    "    \n",
    "    return {\n",
    "        'fifo_order': fifo_completion_order,\n",
    "        'fifo_latencies': fifo_latencies,\n",
    "        'priority_order': priority_completion_order,\n",
    "        'priority_latencies': priority_latencies\n",
    "    }\n",
    "\n",
    "# Compare methods\n",
    "comparison = compare_fifo_vs_priority()\n",
    "\n",
    "print(\"FIFO Completion Order:\", comparison['fifo_order'])\n",
    "print(\"Priority Completion Order:\", comparison['priority_order'])\n",
    "\n",
    "print(f\"\\nAverage Latencies:\")\n",
    "print(f\"FIFO: {np.mean(comparison['fifo_latencies']):.2f}s\")\n",
    "print(f\"Priority: {np.mean(comparison['priority_latencies']):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing Priority Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate larger workload\n",
    "def simulate_priority_workload(num_requests=30):\n",
    "    scheduler = PriorityScheduler(max_batch_size=6)\n",
    "    scheduler.start()\n",
    "    \n",
    "    # Generate mixed priority workload\n",
    "    priorities = [Priority.HIGH, Priority.MEDIUM, Priority.LOW]\n",
    "    priority_weights = [0.2, 0.5, 0.3]  # 20% high, 50% medium, 30% low\n",
    "    \n",
    "    for i in range(num_requests):\n",
    "        priority = np.random.choice(priorities, p=priority_weights)\n",
    "        max_tokens = np.random.randint(5, 20)\n",
    "        \n",
    "        # Set SLA targets based on priority\n",
    "        sla_targets = {\n",
    "            Priority.HIGH: 2.0,\n",
    "            Priority.MEDIUM: 5.0,\n",
    "            Priority.LOW: 10.0\n",
    "        }\n",
    "        \n",
    "        request = PriorityRequest(\n",
    "            id=i,\n",
    "            prompt=f\"Request {i}\",\n",
    "            priority=priority,\n",
    "            max_tokens=max_tokens,\n",
    "            sla_target=sla_targets[priority]\n",
    "        )\n",
    "        \n",
    "        scheduler.submit_request(request)\n",
    "        time.sleep(0.1)  # Arrival rate\n",
    "    \n",
    "    # Wait for completion\n",
    "    while len(scheduler.completed_requests) < num_requests:\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    scheduler.stop()\n",
    "    return scheduler\n",
    "\n",
    "# Run simulation\n",
    "scheduler = simulate_priority_workload(30)\n",
    "metrics = scheduler.get_metrics_by_priority()\n",
    "\n",
    "# Plot results\n",
    "priorities = list(metrics.keys())\n",
    "avg_latencies = [metrics[p]['avg_latency'] for p in priorities]\n",
    "sla_compliance = [metrics[p]['sla_compliance'] or 0 for p in priorities]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(priorities, avg_latencies, color=['red', 'orange', 'green'])\n",
    "plt.ylabel('Average Latency (s)')\n",
    "plt.title('Latency by Priority Level')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(priorities, [s * 100 for s in sla_compliance], color=['red', 'orange', 'green'])\n",
    "plt.ylabel('SLA Compliance (%)')\n",
    "plt.title('SLA Compliance by Priority')\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "for priority_name, data in metrics.items():\n",
    "    print(f\"{priority_name}: {data['avg_latency']:.2f}s avg, {data['sla_compliance']:.1%} SLA compliance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Priority-based scheduling provides several key benefits:\n",
    "\n",
    "1. **SLA Guarantees**: High-priority requests get faster service\n",
    "2. **Fair Resource Allocation**: Different user tiers get appropriate service levels\n",
    "3. **Business Value**: Critical requests are processed first\n",
    "4. **Flexibility**: Can adapt to different business requirements\n",
    "\n",
    "This makes priority scheduling essential for production LLM systems serving multiple user classes with different service level requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
