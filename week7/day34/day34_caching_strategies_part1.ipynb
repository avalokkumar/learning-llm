{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 34: Caching Strategies - Part 1\n",
    "\n",
    "Implementing prompt and response caching to reduce LLM serving costs and improve response times.\n",
    "\n",
    "## Overview\n",
    "1. Simple response caching\n",
    "2. Prompt prefix caching\n",
    "3. Cache performance analysis\n",
    "4. Cost savings measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import hashlib\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Response Cache Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseCache:\n",
    "    def __init__(self, max_size=1000, ttl_seconds=3600):\n",
    "        self.max_size = max_size\n",
    "        self.ttl_seconds = ttl_seconds\n",
    "        self.cache = OrderedDict()\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def _make_key(self, prompt, **params):\n",
    "        \"\"\"Create cache key from prompt and parameters.\"\"\"\n",
    "        key_data = {'prompt': prompt, 'params': params}\n",
    "        key_str = json.dumps(key_data, sort_keys=True)\n",
    "        return hashlib.md5(key_str.encode()).hexdigest()\n",
    "    \n",
    "    def get(self, prompt, **params):\n",
    "        \"\"\"Get cached response if available.\"\"\"\n",
    "        key = self._make_key(prompt, **params)\n",
    "        \n",
    "        if key in self.cache:\n",
    "            entry = self.cache[key]\n",
    "            \n",
    "            # Check TTL\n",
    "            if time.time() - entry['timestamp'] < self.ttl_seconds:\n",
    "                # Move to end (LRU)\n",
    "                self.cache.move_to_end(key)\n",
    "                self.hits += 1\n",
    "                return entry['response']\n",
    "            else:\n",
    "                # Expired, remove\n",
    "                del self.cache[key]\n",
    "        \n",
    "        self.misses += 1\n",
    "        return None\n",
    "    \n",
    "    def put(self, prompt, response, **params):\n",
    "        \"\"\"Store response in cache.\"\"\"\n",
    "        key = self._make_key(prompt, **params)\n",
    "        \n",
    "        # Remove oldest if at capacity\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            self.cache.popitem(last=False)\n",
    "        \n",
    "        self.cache[key] = {\n",
    "            'response': response,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def hit_rate(self):\n",
    "        total = self.hits + self.misses\n",
    "        return self.hits / total if total > 0 else 0\n",
    "    \n",
    "    def stats(self):\n",
    "        return {\n",
    "            'size': len(self.cache),\n",
    "            'hits': self.hits,\n",
    "            'misses': self.misses,\n",
    "            'hit_rate': self.hit_rate\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mock LLM Service with Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockLLMService:\n",
    "    def __init__(self, processing_time=1.0, cost_per_token=0.001):\n",
    "        self.processing_time = processing_time\n",
    "        self.cost_per_token = cost_per_token\n",
    "        self.cache = ResponseCache()\n",
    "        self.total_requests = 0\n",
    "        self.total_cost = 0\n",
    "        self.total_time = 0\n",
    "    \n",
    "    def generate(self, prompt, max_tokens=50, temperature=0.7, use_cache=True):\n",
    "        \"\"\"Generate response with optional caching.\"\"\"\n",
    "        self.total_requests += 1\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Check cache first\n",
    "        if use_cache:\n",
    "            cached_response = self.cache.get(\n",
    "                prompt, \n",
    "                max_tokens=max_tokens, \n",
    "                temperature=temperature\n",
    "            )\n",
    "            if cached_response:\n",
    "                # Cache hit - no processing cost\n",
    "                end_time = time.time()\n",
    "                self.total_time += (end_time - start_time)\n",
    "                return {\n",
    "                    'response': cached_response,\n",
    "                    'cached': True,\n",
    "                    'processing_time': end_time - start_time,\n",
    "                    'cost': 0\n",
    "                }\n",
    "        \n",
    "        # Cache miss - generate response\n",
    "        time.sleep(self.processing_time)  # Simulate processing\n",
    "        \n",
    "        # Generate mock response\n",
    "        response = f\"Generated response for: {prompt[:30]}... (tokens: {max_tokens})\"\n",
    "        \n",
    "        # Calculate cost\n",
    "        input_tokens = len(prompt.split())\n",
    "        output_tokens = max_tokens\n",
    "        cost = (input_tokens + output_tokens) * self.cost_per_token\n",
    "        \n",
    "        self.total_cost += cost\n",
    "        \n",
    "        # Store in cache\n",
    "        if use_cache:\n",
    "            self.cache.put(\n",
    "                prompt, \n",
    "                response, \n",
    "                max_tokens=max_tokens, \n",
    "                temperature=temperature\n",
    "            )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        self.total_time += processing_time\n",
    "        \n",
    "        return {\n",
    "            'response': response,\n",
    "            'cached': False,\n",
    "            'processing_time': processing_time,\n",
    "            'cost': cost\n",
    "        }\n",
    "    \n",
    "    def get_stats(self):\n",
    "        cache_stats = self.cache.stats()\n",
    "        return {\n",
    "            'total_requests': self.total_requests,\n",
    "            'total_cost': self.total_cost,\n",
    "            'total_time': self.total_time,\n",
    "            'avg_cost_per_request': self.total_cost / self.total_requests if self.total_requests > 0 else 0,\n",
    "            'avg_time_per_request': self.total_time / self.total_requests if self.total_requests > 0 else 0,\n",
    "            'cache_hit_rate': cache_stats['hit_rate'],\n",
    "            'cache_size': cache_stats['size']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing Cache Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_workload(service, prompts, num_requests=100, repeat_probability=0.3):\n",
    "    \"\"\"Simulate a realistic workload with repeated requests.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i in range(num_requests):\n",
    "        # Decide whether to repeat a previous prompt or use a new one\n",
    "        if i > 0 and np.random.random() < repeat_probability:\n",
    "            # Repeat a previous prompt\n",
    "            prompt = np.random.choice(prompts)\n",
    "        else:\n",
    "            # Use a new prompt\n",
    "            prompt = f\"{np.random.choice(prompts)} (variation {i})\"\n",
    "        \n",
    "        # Generate response\n",
    "        result = service.generate(prompt, max_tokens=np.random.randint(20, 80))\n",
    "        results.append(result)\n",
    "        \n",
    "        # Small delay between requests\n",
    "        time.sleep(0.01)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Explain machine learning\",\n",
    "    \"How do neural networks work?\",\n",
    "    \"What is deep learning?\",\n",
    "    \"Describe natural language processing\"\n",
    "]\n",
    "\n",
    "# Test with caching\n",
    "print(\"Testing with caching enabled...\")\n",
    "service_with_cache = MockLLMService(processing_time=0.5, cost_per_token=0.001)\n",
    "results_cached = simulate_workload(service_with_cache, test_prompts, num_requests=50)\n",
    "\n",
    "# Test without caching\n",
    "print(\"Testing without caching...\")\n",
    "service_no_cache = MockLLMService(processing_time=0.5, cost_per_token=0.001)\n",
    "results_no_cache = []\n",
    "for i in range(50):\n",
    "    prompt = f\"{np.random.choice(test_prompts)} (variation {i})\"\n",
    "    result = service_no_cache.generate(prompt, use_cache=False)\n",
    "    results_no_cache.append(result)\n",
    "    time.sleep(0.01)\n",
    "\n",
    "# Compare results\n",
    "stats_cached = service_with_cache.get_stats()\n",
    "stats_no_cache = service_no_cache.get_stats()\n",
    "\n",
    "print(\"\\n=== Results Comparison ===\")\n",
    "print(f\"With Cache:\")\n",
    "print(f\"  Total Cost: ${stats_cached['total_cost']:.3f}\")\n",
    "print(f\"  Total Time: {stats_cached['total_time']:.2f}s\")\n",
    "print(f\"  Cache Hit Rate: {stats_cached['cache_hit_rate']:.1%}\")\n",
    "print(f\"  Avg Cost/Request: ${stats_cached['avg_cost_per_request']:.4f}\")\n",
    "\n",
    "print(f\"\\nWithout Cache:\")\n",
    "print(f\"  Total Cost: ${stats_no_cache['total_cost']:.3f}\")\n",
    "print(f\"  Total Time: {stats_no_cache['total_time']:.2f}s\")\n",
    "print(f\"  Avg Cost/Request: ${stats_no_cache['avg_cost_per_request']:.4f}\")\n",
    "\n",
    "print(f\"\\nSavings:\")\n",
    "cost_savings = (stats_no_cache['total_cost'] - stats_cached['total_cost']) / stats_no_cache['total_cost']\n",
    "time_savings = (stats_no_cache['total_time'] - stats_cached['total_time']) / stats_no_cache['total_time']\n",
    "print(f\"  Cost Savings: {cost_savings:.1%}\")\n",
    "print(f\"  Time Savings: {time_savings:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cache Hit Rate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cache_performance(repeat_probabilities, num_requests=100):\n",
    "    \"\"\"Analyze cache performance under different repeat probabilities.\"\"\"\n",
    "    hit_rates = []\n",
    "    cost_savings = []\n",
    "    \n",
    "    for repeat_prob in repeat_probabilities:\n",
    "        # Test with caching\n",
    "        service_cached = MockLLMService(processing_time=0.1, cost_per_token=0.001)\n",
    "        simulate_workload(service_cached, test_prompts, num_requests, repeat_prob)\n",
    "        \n",
    "        # Test without caching\n",
    "        service_no_cache = MockLLMService(processing_time=0.1, cost_per_token=0.001)\n",
    "        for i in range(num_requests):\n",
    "            prompt = f\"Test prompt {i}\"\n",
    "            service_no_cache.generate(prompt, use_cache=False)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        stats_cached = service_cached.get_stats()\n",
    "        stats_no_cache = service_no_cache.get_stats()\n",
    "        \n",
    "        hit_rates.append(stats_cached['cache_hit_rate'])\n",
    "        \n",
    "        savings = (stats_no_cache['total_cost'] - stats_cached['total_cost']) / stats_no_cache['total_cost']\n",
    "        cost_savings.append(savings)\n",
    "    \n",
    "    return hit_rates, cost_savings\n",
    "\n",
    "# Analyze different repeat probabilities\n",
    "repeat_probs = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "hit_rates, cost_savings = analyze_cache_performance(repeat_probs)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(repeat_probs, [h * 100 for h in hit_rates], 'o-')\n",
    "plt.xlabel('Repeat Probability')\n",
    "plt.ylabel('Cache Hit Rate (%)')\n",
    "plt.title('Cache Hit Rate vs Repeat Probability')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(repeat_probs, [s * 100 for s in cost_savings], 'o-', color='green')\n",
    "plt.xlabel('Repeat Probability')\n",
    "plt.ylabel('Cost Savings (%)')\n",
    "plt.title('Cost Savings vs Repeat Probability')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Caching: Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticCache:\n",
    "    def __init__(self, similarity_threshold=0.8, max_size=1000):\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.max_size = max_size\n",
    "        self.cache = []\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def _simple_similarity(self, text1, text2):\n",
    "        \"\"\"Simple word-based similarity (for demo purposes).\"\"\"\n",
    "        words1 = set(text1.lower().split())\n",
    "        words2 = set(text2.lower().split())\n",
    "        \n",
    "        if not words1 or not words2:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = len(words1.intersection(words2))\n",
    "        union = len(words1.union(words2))\n",
    "        \n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    def get(self, prompt):\n",
    "        \"\"\"Get cached response for similar prompt.\"\"\"\n",
    "        for entry in self.cache:\n",
    "            similarity = self._simple_similarity(prompt, entry['prompt'])\n",
    "            if similarity >= self.similarity_threshold:\n",
    "                self.hits += 1\n",
    "                return entry['response']\n",
    "        \n",
    "        self.misses += 1\n",
    "        return None\n",
    "    \n",
    "    def put(self, prompt, response):\n",
    "        \"\"\"Store response in cache.\"\"\"\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            self.cache.pop(0)  # Remove oldest\n",
    "        \n",
    "        self.cache.append({\n",
    "            'prompt': prompt,\n",
    "            'response': response,\n",
    "            'timestamp': time.time()\n",
    "        })\n",
    "    \n",
    "    @property\n",
    "    def hit_rate(self):\n",
    "        total = self.hits + self.misses\n",
    "        return self.hits / total if total > 0 else 0\n",
    "\n",
    "# Test semantic caching\n",
    "semantic_cache = SemanticCache(similarity_threshold=0.6)\n",
    "\n",
    "# Test similar prompts\n",
    "similar_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain machine learning\",\n",
    "    \"Tell me about machine learning\",\n",
    "    \"How does machine learning work?\",\n",
    "    \"What is deep learning?\",  # Different topic\n",
    "    \"Explain deep learning\",\n",
    "]\n",
    "\n",
    "print(\"Testing semantic caching:\")\n",
    "for i, prompt in enumerate(similar_prompts):\n",
    "    cached_response = semantic_cache.get(prompt)\n",
    "    if cached_response:\n",
    "        print(f\"  {i+1}. CACHE HIT: {prompt}\")\n",
    "    else:\n",
    "        print(f\"  {i+1}. CACHE MISS: {prompt}\")\n",
    "        # Simulate generating and caching response\n",
    "        response = f\"Response for: {prompt}\"\n",
    "        semantic_cache.put(prompt, response)\n",
    "\n",
    "print(f\"\\nSemantic cache hit rate: {semantic_cache.hit_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Caching strategies can significantly reduce LLM serving costs:\n",
    "\n",
    "1. **Response Caching**: Eliminates redundant computation for identical requests\n",
    "2. **Semantic Caching**: Handles similar requests with slight variations\n",
    "3. **Cost Savings**: Can achieve 30-70% cost reduction depending on workload patterns\n",
    "4. **Performance**: Dramatically reduces response times for cached content\n",
    "\n",
    "Next, we'll explore rate limiting and multi-tenancy strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
