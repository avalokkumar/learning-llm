{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Day 10: Minimal Transformer Implementation - Part 1\n",
    "\n",
    "## Overview\n",
    "This notebook implements the core components of a minimal transformer from scratch, focusing on:\n",
    "- Minimal attention mechanism\n",
    "- Feed-forward networks\n",
    "- Layer normalization and residual connections\n",
    "- Complete transformer block\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will:\n",
    "1. Understand how to implement attention from scratch\n",
    "2. Build feed-forward networks for transformers\n",
    "3. Combine components into a complete transformer block\n",
    "4. Visualize attention patterns and understand their meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Minimal Attention Implementation\n",
    "\n",
    "Let's start by implementing the core attention mechanism from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimalAttention(nn.Module):\n",
    "    \"\"\"Minimal implementation of scaled dot-product attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Generate Q, K, V\n",
    "        Q = self.w_q(x)  # (batch_size, seq_len, d_model)\n",
    "        K = self.w_k(x)\n",
    "        V = self.w_v(x)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(\n",
    "            Q, K, V, mask\n",
    "        )\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, d_model\n",
    "        )\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.w_o(attention_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feed-Forward Network\n",
    "\n",
    "The feed-forward network applies position-wise transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply first linear transformation with GELU activation\n",
    "        x = self.linear1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Apply second linear transformation\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Complete Transformer Block\n",
    "\n",
    "Now let's combine attention and feed-forward into a complete transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Complete transformer block with attention and feed-forward.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MinimalAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        # Self-attention with residual connection and layer norm (pre-norm)\n",
    "        norm_x = self.norm1(x)\n",
    "        attn_output, attn_weights = self.attention(norm_x, mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        \n",
    "        # Feed-forward with residual connection and layer norm (pre-norm)\n",
    "        norm_x = self.norm2(x)\n",
    "        ff_output = self.feed_forward(norm_x)\n",
    "        x = x + self.dropout(ff_output)\n",
    "        \n",
    "        return x, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing the Components\n",
    "\n",
    "Let's test our transformer components with sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parameters\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "\n",
    "# Create sample input\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Test attention\n",
    "attention = MinimalAttention(d_model, num_heads)\n",
    "attn_output, attn_weights = attention(x)\n",
    "print(f\"Attention output shape: {attn_output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "\n",
    "# Test feed-forward\n",
    "ff = FeedForward(d_model, d_ff)\n",
    "ff_output = ff(x)\n",
    "print(f\"Feed-forward output shape: {ff_output.shape}\")\n",
    "\n",
    "# Test complete transformer block\n",
    "transformer_block = TransformerBlock(d_model, num_heads, d_ff)\n",
    "block_output, block_attn_weights = transformer_block(x)\n",
    "print(f\"Transformer block output shape: {block_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attention Visualization\n",
    "\n",
    "Let's visualize the attention patterns to understand what the model is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(attention_weights, head_idx=0, batch_idx=0):\n",
    "    \"\"\"Visualize attention weights for a specific head and batch.\"\"\"\n",
    "    # Extract attention weights for specific head and batch\n",
    "    attn = attention_weights[batch_idx, head_idx].detach().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(attn, annot=True, fmt='.3f', cmap='Blues', \n",
    "                xticklabels=range(attn.shape[1]),\n",
    "                yticklabels=range(attn.shape[0]))\n",
    "    plt.title(f'Attention Weights - Head {head_idx}, Batch {batch_idx}')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize attention patterns\n",
    "visualize_attention(block_attn_weights, head_idx=0)\n",
    "visualize_attention(block_attn_weights, head_idx=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gradient Flow Analysis\n",
    "\n",
    "Let's analyze how gradients flow through our transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gradient_flow(model, x):\n",
    "    \"\"\"Analyze gradient flow through the model.\"\"\"\n",
    "    # Forward pass\n",
    "    output, _ = model(x)\n",
    "    \n",
    "    # Create a simple loss (sum of all outputs)\n",
    "    loss = output.sum()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Collect gradients\n",
    "    gradients = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            gradients[name] = param.grad.norm().item()\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# Analyze gradient flow\n",
    "x_grad = torch.randn(1, 5, d_model, requires_grad=True)\n",
    "transformer_grad = TransformerBlock(d_model, num_heads, d_ff)\n",
    "\n",
    "gradients = analyze_gradient_flow(transformer_grad, x_grad)\n",
    "\n",
    "# Plot gradient norms\n",
    "plt.figure(figsize=(12, 6))\n",
    "names = list(gradients.keys())\n",
    "values = list(gradients.values())\n",
    "\n",
    "plt.bar(range(len(names)), values)\n",
    "plt.xticks(range(len(names)), names, rotation=45, ha='right')\n",
    "plt.ylabel('Gradient Norm')\n",
    "plt.title('Gradient Flow Through Transformer Block')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Gradient norms:\")\n",
    "for name, norm in gradients.items():\n",
    "    print(f\"{name}: {norm:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Parameter Initialization Analysis\n",
    "\n",
    "Let's examine the importance of proper parameter initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model, init_type='xavier'):\n",
    "    \"\"\"Initialize model weights with different strategies.\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name and param.dim() > 1:\n",
    "            if init_type == 'xavier':\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif init_type == 'kaiming':\n",
    "                nn.init.kaiming_uniform_(param)\n",
    "            elif init_type == 'normal':\n",
    "                nn.init.normal_(param, 0, 0.02)\n",
    "        elif 'bias' in name:\n",
    "            nn.init.zeros_(param)\n",
    "\n",
    "# Test different initialization strategies\n",
    "init_strategies = ['xavier', 'kaiming', 'normal']\n",
    "results = {}\n",
    "\n",
    "for init_type in init_strategies:\n",
    "    model = TransformerBlock(d_model, num_heads, d_ff)\n",
    "    initialize_weights(model, init_type)\n",
    "    \n",
    "    # Forward pass\n",
    "    x_test = torch.randn(1, 10, d_model)\n",
    "    output, _ = model(x_test)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    results[init_type] = {\n",
    "        'output_mean': output.mean().item(),\n",
    "        'output_std': output.std().item(),\n",
    "        'output_range': (output.min().item(), output.max().item())\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "print(\"Initialization Strategy Comparison:\")\n",
    "print(\"-\" * 50)\n",
    "for init_type, stats in results.items():\n",
    "    print(f\"{init_type.upper()}:\")\n",
    "    print(f\"  Mean: {stats['output_mean']:.6f}\")\n",
    "    print(f\"  Std:  {stats['output_std']:.6f}\")\n",
    "    print(f\"  Range: ({stats['output_range'][0]:.3f}, {stats['output_range'][1]:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Key Insights\n",
    "\n",
    "In this notebook, we've implemented and analyzed the core components of a transformer:\n",
    "\n",
    "### Key Components:\n",
    "1. **Minimal Attention**: Scaled dot-product attention with multi-head support\n",
    "2. **Feed-Forward Network**: Position-wise transformations with GELU activation\n",
    "3. **Transformer Block**: Complete block with residual connections and layer normalization\n",
    "\n",
    "### Key Insights:\n",
    "1. **Attention Patterns**: Attention weights show how tokens relate to each other\n",
    "2. **Gradient Flow**: Residual connections help maintain healthy gradient flow\n",
    "3. **Initialization**: Proper weight initialization is crucial for training stability\n",
    "\n",
    "### Next Steps:\n",
    "In Part 2, we'll:\n",
    "- Build a complete minimal transformer model\n",
    "- Implement training and evaluation loops\n",
    "- Test on simple language modeling tasks\n",
    "- Analyze performance and scaling properties"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
