{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6: Scaled Dot-Product Attention - Implementation and Examples\n",
    "\n",
    "This notebook contains all the code examples, visualizations, and hands-on exercises for Day 6 of Week 2.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Q, K, V Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_attention_components():\n",
    "    \"\"\"Explain each component of attention with intuitive examples.\"\"\"\n",
    "    \n",
    "    print(\"Understanding Q, K, V with Intuitive Examples\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Example: \"The cat sat on the mat\"\n",
    "    tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "    \n",
    "    print(\"Example sentence: 'The cat sat on the mat'\")\n",
    "    print(\"\\nIntuitive Understanding:\")\n",
    "    print(\"- Query (Q): 'What should I pay attention to?'\")\n",
    "    print(\"- Key (K): 'What information do I have?'\")\n",
    "    print(\"- Value (V): 'What is the actual content?'\")\n",
    "    \n",
    "    print(\"\\nFor token 'sat':\")\n",
    "    print(\"- Query: Looking for subject and object relationships\")\n",
    "    print(\"- Keys: All tokens offer their relationship information\")\n",
    "    print(\"- Values: Actual semantic content of each token\")\n",
    "    \n",
    "    print(\"\\nAttention weights tell us:\")\n",
    "    print(\"- How much 'sat' should focus on 'cat' (subject)\")\n",
    "    print(\"- How much 'sat' should focus on 'mat' (object)\")\n",
    "    print(\"- Less attention to articles 'the', 'the'\")\n",
    "\n",
    "explain_attention_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Attention Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"Scaled Dot-Product Attention implementation from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: [batch_size, seq_len, d_model]\n",
    "            key: [batch_size, seq_len, d_model]  \n",
    "            value: [batch_size, seq_len, d_model]\n",
    "            mask: [batch_size, seq_len, seq_len] or None\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch_size, seq_len, d_model]\n",
    "            attention_weights: [batch_size, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = query.size()\n",
    "        \n",
    "        # Step 1: Compute attention scores (Q·K^T)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        \n",
    "        # Step 2: Scale by √d_k\n",
    "        scores = scores / np.sqrt(d_model)\n",
    "        \n",
    "        # Step 3: Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Step 4: Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Step 5: Apply attention weights to values\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the implementation\n",
    "def test_basic_attention():\n",
    "    \"\"\"Test basic attention with simple example.\"\"\"\n",
    "    \n",
    "    batch_size, seq_len, d_model = 1, 4, 8\n",
    "    \n",
    "    # Create simple input embeddings\n",
    "    embeddings = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Initialize attention\n",
    "    attention = ScaledDotProductAttention(d_model)\n",
    "    \n",
    "    # Self-attention: Q, K, V are all the same\n",
    "    output, weights = attention(embeddings, embeddings, embeddings)\n",
    "    \n",
    "    print(\"Basic Attention Test:\")\n",
    "    print(f\"Input shape: {embeddings.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Attention weights shape: {weights.shape}\")\n",
    "    print(f\"Attention weights sum (should be ~1.0): {weights.sum(dim=-1)}\")\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "output, weights = test_basic_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Manual Computation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_attention_computation():\n",
    "    \"\"\"Compute attention manually for a small example.\"\"\"\n",
    "    \n",
    "    print(\"Manual Attention Computation\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Small example: 3 tokens, 4 dimensions\n",
    "    seq_len, d_model = 3, 4\n",
    "    \n",
    "    # Create simple Q, K, V matrices\n",
    "    Q = torch.tensor([\n",
    "        [1.0, 0.0, 1.0, 0.0],  # Token 1 query\n",
    "        [0.0, 1.0, 0.0, 1.0],  # Token 2 query  \n",
    "        [1.0, 1.0, 0.0, 0.0]   # Token 3 query\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    K = torch.tensor([\n",
    "        [1.0, 0.0, 0.0, 1.0],  # Token 1 key\n",
    "        [0.0, 1.0, 1.0, 0.0],  # Token 2 key\n",
    "        [1.0, 0.0, 1.0, 1.0]   # Token 3 key\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    V = torch.tensor([\n",
    "        [2.0, 0.0, 1.0, 0.0],  # Token 1 value\n",
    "        [0.0, 2.0, 0.0, 1.0],  # Token 2 value\n",
    "        [1.0, 1.0, 2.0, 2.0]   # Token 3 value\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    print(\"Query matrix Q:\")\n",
    "    print(Q.numpy())\n",
    "    print(\"\\nKey matrix K:\")\n",
    "    print(K.numpy())\n",
    "    print(\"\\nValue matrix V:\")\n",
    "    print(V.numpy())\n",
    "    \n",
    "    # Step 1: Compute Q·K^T\n",
    "    scores = torch.matmul(Q, K.transpose(0, 1))\n",
    "    print(f\"\\nStep 1 - Attention scores (Q·K^T):\")\n",
    "    print(scores.numpy())\n",
    "    \n",
    "    # Step 2: Scale by √d_k\n",
    "    scaled_scores = scores / np.sqrt(d_model)\n",
    "    print(f\"\\nStep 2 - Scaled scores (÷√{d_model} = ÷{np.sqrt(d_model):.2f}):\")\n",
    "    print(scaled_scores.numpy())\n",
    "    \n",
    "    # Step 3: Apply softmax\n",
    "    attention_weights = F.softmax(scaled_scores, dim=-1)\n",
    "    print(f\"\\nStep 3 - Attention weights (softmax):\")\n",
    "    print(attention_weights.numpy())\n",
    "    \n",
    "    # Verify weights sum to 1\n",
    "    print(f\"\\nWeights sum per row: {attention_weights.sum(dim=-1).numpy()}\")\n",
    "    \n",
    "    # Step 4: Apply to values\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    print(f\"\\nStep 4 - Final output (weights × V):\")\n",
    "    print(output.numpy())\n",
    "    \n",
    "    return Q, K, V, attention_weights, output\n",
    "\n",
    "Q, K, V, manual_weights, manual_output = manual_attention_computation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Attention Pattern Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_patterns():\n",
    "    \"\"\"Create comprehensive attention visualizations.\"\"\"\n",
    "    \n",
    "    # Create a more interesting example\n",
    "    seq_len, d_model = 6, 16\n",
    "    \n",
    "    # Simulate embeddings for: \"The cat sat on the mat\"\n",
    "    tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "    \n",
    "    # Create embeddings with some structure\n",
    "    torch.manual_seed(42)\n",
    "    embeddings = torch.randn(1, seq_len, d_model)\n",
    "    \n",
    "    # Make some tokens more similar (e.g., \"The\" and \"the\")\n",
    "    embeddings[0, 4] = embeddings[0, 0] + 0.1 * torch.randn(d_model)\n",
    "    \n",
    "    # Make \"cat\" and \"mat\" somewhat similar (both nouns)\n",
    "    embeddings[0, 5] = embeddings[0, 1] + 0.3 * torch.randn(d_model)\n",
    "    \n",
    "    # Apply attention\n",
    "    attention = ScaledDotProductAttention(d_model)\n",
    "    output, weights = attention(embeddings, embeddings, embeddings)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Attention heatmap\n",
    "    sns.heatmap(weights[0].detach().numpy(), \n",
    "                xticklabels=tokens, yticklabels=tokens,\n",
    "                annot=True, fmt='.3f', cmap='Blues',\n",
    "                ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Attention Weights Heatmap')\n",
    "    axes[0, 0].set_xlabel('Key (attending to)')\n",
    "    axes[0, 0].set_ylabel('Query (attending from)')\n",
    "    \n",
    "    # 2. Attention weights for specific token\n",
    "    token_idx = 2  # \"sat\"\n",
    "    axes[0, 1].bar(tokens, weights[0, token_idx].detach().numpy())\n",
    "    axes[0, 1].set_title(f'Attention weights for \"{tokens[token_idx]}\"')\n",
    "    axes[0, 1].set_ylabel('Attention Weight')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Raw attention scores (before softmax)\n",
    "    raw_scores = torch.matmul(embeddings, embeddings.transpose(-2, -1)) / np.sqrt(d_model)\n",
    "    sns.heatmap(raw_scores[0].detach().numpy(),\n",
    "                xticklabels=tokens, yticklabels=tokens,\n",
    "                annot=True, fmt='.2f', cmap='RdBu_r', center=0,\n",
    "                ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Raw Attention Scores (before softmax)')\n",
    "    \n",
    "    # 4. Attention entropy (how focused/distributed)\n",
    "    entropy = -torch.sum(weights * torch.log(weights + 1e-9), dim=-1)\n",
    "    axes[1, 1].bar(tokens, entropy[0].detach().numpy())\n",
    "    axes[1, 1].set_title('Attention Entropy (higher = more distributed)')\n",
    "    axes[1, 1].set_ylabel('Entropy')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print interpretation\n",
    "    print(\"Attention Pattern Interpretation:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        top_attention = torch.topk(weights[0, i], 2)\n",
    "        top_tokens = [tokens[idx] for idx in top_attention.indices]\n",
    "        top_weights = top_attention.values\n",
    "        \n",
    "        print(f\"'{token}' attends most to:\")\n",
    "        for j, (att_token, weight) in enumerate(zip(top_tokens, top_weights)):\n",
    "            print(f\"  {j+1}. '{att_token}' (weight: {weight:.3f})\")\n",
    "        print()\n",
    "    \n",
    "    return weights, tokens\n",
    "\n",
    "attention_weights, tokens = visualize_attention_patterns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scaling Factor Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_scaling_importance():\n",
    "    \"\"\"Show why we need the √d_k scaling factor.\"\"\"\n",
    "    \n",
    "    print(\"Why Scale by √d_k?\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Test with different dimensions\n",
    "    dimensions = [4, 16, 64, 256]\n",
    "    seq_len = 4\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for d_model in dimensions:\n",
    "        # Create random Q and K\n",
    "        Q = torch.randn(1, seq_len, d_model)\n",
    "        K = torch.randn(1, seq_len, d_model)\n",
    "        \n",
    "        # Compute scores without scaling\n",
    "        scores_unscaled = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        \n",
    "        # Compute scores with scaling\n",
    "        scores_scaled = scores_unscaled / np.sqrt(d_model)\n",
    "        \n",
    "        # Apply softmax\n",
    "        weights_unscaled = F.softmax(scores_unscaled, dim=-1)\n",
    "        weights_scaled = F.softmax(scores_scaled, dim=-1)\n",
    "        \n",
    "        # Measure how \"sharp\" the attention is (entropy)\n",
    "        entropy_unscaled = -torch.sum(weights_unscaled * torch.log(weights_unscaled + 1e-9), dim=-1).mean()\n",
    "        entropy_scaled = -torch.sum(weights_scaled * torch.log(weights_scaled + 1e-9), dim=-1).mean()\n",
    "        \n",
    "        results[d_model] = {\n",
    "            'scores_std_unscaled': scores_unscaled.std().item(),\n",
    "            'scores_std_scaled': scores_scaled.std().item(),\n",
    "            'entropy_unscaled': entropy_unscaled.item(),\n",
    "            'entropy_scaled': entropy_scaled.item(),\n",
    "            'max_weight_unscaled': weights_unscaled.max().item(),\n",
    "            'max_weight_scaled': weights_scaled.max().item()\n",
    "        }\n",
    "    \n",
    "    # Display results\n",
    "    print(\"Dimension | Scores Std (Unscaled) | Scores Std (Scaled) | Max Weight (Unscaled) | Max Weight (Scaled)\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for d_model, stats in results.items():\n",
    "        print(f\"{d_model:8d} | {stats['scores_std_unscaled']:17.3f} | {stats['scores_std_scaled']:16.3f} | \"\n",
    "              f\"{stats['max_weight_unscaled']:18.3f} | {stats['max_weight_scaled']:17.3f}\")\n",
    "    \n",
    "    print(\"\\nObservations:\")\n",
    "    print(\"- Without scaling: larger dimensions → larger scores → sharper attention\")\n",
    "    print(\"- With scaling: attention sharpness remains consistent across dimensions\")\n",
    "    print(\"- Scaling prevents attention from becoming too concentrated\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "scaling_results = demonstrate_scaling_importance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exercise: Hand Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exercise_hand_computation():\n",
    "    \"\"\"Exercise: Compute attention by hand for very small example.\"\"\"\n",
    "    \n",
    "    print(\"Exercise 1: Hand Computation\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    print(\"Given:\")\n",
    "    print(\"Q = [[1, 0], [0, 1]]\")\n",
    "    print(\"K = [[1, 1], [1, 0]]\") \n",
    "    print(\"V = [[2, 1], [1, 2]]\")\n",
    "    print(\"d_k = 2\")\n",
    "    \n",
    "    print(\"\\nYour task:\")\n",
    "    print(\"1. Compute QK^T\")\n",
    "    print(\"2. Scale by √d_k\")\n",
    "    print(\"3. Apply softmax\")\n",
    "    print(\"4. Multiply by V\")\n",
    "    \n",
    "    # Solution\n",
    "    Q = torch.tensor([[1.0, 0.0], [0.0, 1.0]])\n",
    "    K = torch.tensor([[1.0, 1.0], [1.0, 0.0]])\n",
    "    V = torch.tensor([[2.0, 1.0], [1.0, 2.0]])\n",
    "    \n",
    "    print(\"\\nSolution:\")\n",
    "    \n",
    "    # Step 1\n",
    "    QK = torch.matmul(Q, K.T)\n",
    "    print(f\"1. QK^T = \\n{QK.numpy()}\")\n",
    "    \n",
    "    # Step 2\n",
    "    scaled = QK / np.sqrt(2)\n",
    "    print(f\"2. Scaled = \\n{scaled.numpy()}\")\n",
    "    \n",
    "    # Step 3\n",
    "    weights = F.softmax(scaled, dim=-1)\n",
    "    print(f\"3. Softmax = \\n{weights.numpy()}\")\n",
    "    \n",
    "    # Step 4\n",
    "    output = torch.matmul(weights, V)\n",
    "    print(f\"4. Output = \\n{output.numpy()}\")\n",
    "\n",
    "exercise_hand_computation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercise: Attention Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attention_patterns():\n",
    "    \"\"\"Analyze different types of attention patterns.\"\"\"\n",
    "    \n",
    "    print(\"Types of Attention Patterns\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    seq_len, d_model = 5, 8\n",
    "    \n",
    "    # Create different scenarios\n",
    "    scenarios = {\n",
    "        'uniform': torch.ones(1, seq_len, d_model),  # All tokens identical\n",
    "        'sequential': torch.arange(seq_len * d_model).float().view(1, seq_len, d_model),  # Sequential pattern\n",
    "        'similar_pairs': torch.randn(1, seq_len, d_model)  # Will modify for similarity\n",
    "    }\n",
    "    \n",
    "    # Make pairs similar in the third scenario\n",
    "    scenarios['similar_pairs'][0, 1] = scenarios['similar_pairs'][0, 0] + 0.1 * torch.randn(d_model)\n",
    "    scenarios['similar_pairs'][0, 3] = scenarios['similar_pairs'][0, 2] + 0.1 * torch.randn(d_model)\n",
    "    \n",
    "    attention = ScaledDotProductAttention(d_model)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    for idx, (name, embeddings) in enumerate(scenarios.items()):\n",
    "        output, weights = attention(embeddings, embeddings, embeddings)\n",
    "        \n",
    "        # Visualize attention pattern\n",
    "        sns.heatmap(weights[0].detach().numpy(),\n",
    "                   annot=True, fmt='.2f', cmap='Blues',\n",
    "                   ax=axes[idx])\n",
    "        axes[idx].set_title(f'{name.title()} Embeddings')\n",
    "        axes[idx].set_xlabel('Key Position')\n",
    "        axes[idx].set_ylabel('Query Position')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze patterns\n",
    "    for name, embeddings in scenarios.items():\n",
    "        output, weights = attention(embeddings, embeddings, embeddings)\n",
    "        \n",
    "        # Compute attention statistics\n",
    "        self_attention = torch.diag(weights[0]).mean()  # How much tokens attend to themselves\n",
    "        max_attention = weights[0].max()  # Maximum attention weight\n",
    "        entropy = -torch.sum(weights[0] * torch.log(weights[0] + 1e-9), dim=-1).mean()\n",
    "        \n",
    "        print(f\"\\n{name.title()} Pattern:\")\n",
    "        print(f\"  Average self-attention: {self_attention:.3f}\")\n",
    "        print(f\"  Maximum attention weight: {max_attention:.3f}\")\n",
    "        print(f\"  Average entropy: {entropy:.3f}\")\n",
    "\n",
    "analyze_attention_patterns()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
