{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 7: Multi-Head Attention - Implementation and Examples\n",
    "\n",
    "This notebook contains all the code examples, visualizations, and hands-on exercises for Day 7 of Week 2.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multi-Head Attention Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"Scaled dot-product attention for one head.\"\"\"\n",
    "        d_k = Q.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output, attention_weights\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size, seq_len = query.size(0), query.size(1)\n",
    "        \n",
    "        # 1. Linear projections\n",
    "        Q = self.w_q(query)  # [batch_size, seq_len, d_model]\n",
    "        K = self.w_k(key)\n",
    "        V = self.w_v(value)\n",
    "        \n",
    "        # 2. Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        # Shape: [batch_size, num_heads, seq_len, d_k]\n",
    "        \n",
    "        # 3. Apply attention to each head\n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # 4. Concatenate heads\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # 5. Final linear projection\n",
    "        output = self.w_o(attention_output)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multihead_attention():\n",
    "    \"\"\"Test multi-head attention implementation.\"\"\"\n",
    "    \n",
    "    batch_size, seq_len, d_model = 2, 6, 64\n",
    "    num_heads = 8\n",
    "    \n",
    "    # Create input\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Initialize multi-head attention\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "    \n",
    "    # Forward pass\n",
    "    output, attention_weights = mha(x, x, x)\n",
    "    \n",
    "    print(\"Multi-Head Attention Test:\")\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "    print(f\"Number of heads: {num_heads}\")\n",
    "    print(f\"d_k per head: {d_model // num_heads}\")\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "output, weights = test_multihead_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualizing Different Attention Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_heads():\n",
    "    \"\"\"Visualize how different heads capture different patterns.\"\"\"\n",
    "    \n",
    "    # Create structured input representing: \"The cat sat on the mat\"\n",
    "    tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "    seq_len, d_model = len(tokens), 64\n",
    "    num_heads = 4\n",
    "    \n",
    "    # Create embeddings with linguistic structure\n",
    "    torch.manual_seed(42)\n",
    "    embeddings = torch.randn(1, seq_len, d_model)\n",
    "    \n",
    "    # Add structure to simulate different relationships\n",
    "    # Articles similar\n",
    "    embeddings[0, 4] = embeddings[0, 0] + 0.2 * torch.randn(d_model)\n",
    "    # Nouns similar  \n",
    "    embeddings[0, 5] = embeddings[0, 1] + 0.3 * torch.randn(d_model)\n",
    "    # Verb-noun relationship\n",
    "    embeddings[0, 2] += 0.1 * embeddings[0, 1]  # sat influenced by cat\n",
    "    \n",
    "    # Apply multi-head attention\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "    output, attention_weights = mha(embeddings, embeddings, embeddings)\n",
    "    \n",
    "    # Visualize each head\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for head in range(num_heads):\n",
    "        head_weights = attention_weights[0, head].detach().numpy()\n",
    "        \n",
    "        sns.heatmap(head_weights, \n",
    "                   xticklabels=tokens, yticklabels=tokens,\n",
    "                   annot=True, fmt='.2f', cmap='Blues',\n",
    "                   ax=axes[head])\n",
    "        axes[head].set_title(f'Head {head + 1}')\n",
    "        axes[head].set_xlabel('Key (attending to)')\n",
    "        axes[head].set_ylabel('Query (attending from)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze head specialization\n",
    "    print(\"Head Specialization Analysis:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for head in range(num_heads):\n",
    "        head_weights = attention_weights[0, head]\n",
    "        \n",
    "        # Find strongest attention patterns\n",
    "        max_attention = head_weights.max()\n",
    "        max_pos = torch.argmax(head_weights.flatten())\n",
    "        i, j = max_pos // seq_len, max_pos % seq_len\n",
    "        \n",
    "        # Calculate attention entropy (how distributed)\n",
    "        entropy = -torch.sum(head_weights * torch.log(head_weights + 1e-9), dim=-1).mean()\n",
    "        \n",
    "        print(f\"Head {head + 1}:\")\n",
    "        print(f\"  Strongest: '{tokens[i]}' â†’ '{tokens[j]}' ({max_attention:.3f})\")\n",
    "        print(f\"  Avg entropy: {entropy:.3f} ({'focused' if entropy < 1.5 else 'distributed'})\")\n",
    "        print()\n",
    "    \n",
    "    return attention_weights, tokens\n",
    "\n",
    "head_weights, tokens = visualize_attention_heads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Head Specialization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_head_specialization():\n",
    "    \"\"\"Analyze what different heads learn to focus on.\"\"\"\n",
    "    \n",
    "    # Create different types of sequences to test specialization\n",
    "    test_cases = {\n",
    "        'syntactic': [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"high\"],\n",
    "        'semantic': [\"cat\", \"dog\", \"animal\", \"pet\", \"furry\", \"cute\"],\n",
    "        'positional': [\"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\"]\n",
    "    }\n",
    "    \n",
    "    d_model, num_heads = 48, 6\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for case_name, tokens in test_cases.items():\n",
    "        seq_len = len(tokens)\n",
    "        \n",
    "        # Create embeddings with different structures\n",
    "        torch.manual_seed(123)\n",
    "        embeddings = torch.randn(1, seq_len, d_model)\n",
    "        \n",
    "        if case_name == 'syntactic':\n",
    "            # Make adjectives similar\n",
    "            for i in [1, 2]:  # quick, brown\n",
    "                embeddings[0, i] += 0.3 * embeddings[0, 1]\n",
    "        elif case_name == 'semantic':\n",
    "            # Make semantically related words similar\n",
    "            embeddings[0, 1] = embeddings[0, 0] + 0.2 * torch.randn(d_model)  # cat, dog\n",
    "            embeddings[0, 3] = embeddings[0, 2] + 0.2 * torch.randn(d_model)  # animal, pet\n",
    "        elif case_name == 'positional':\n",
    "            # Create positional patterns\n",
    "            for i in range(seq_len):\n",
    "                embeddings[0, i] += 0.1 * i * torch.ones(d_model)\n",
    "        \n",
    "        # Get attention patterns\n",
    "        output, attention_weights = mha(embeddings, embeddings, embeddings)\n",
    "        \n",
    "        # Analyze each head\n",
    "        head_analysis = {}\n",
    "        for head in range(num_heads):\n",
    "            head_attn = attention_weights[0, head]\n",
    "            \n",
    "            # Measure different properties\n",
    "            self_attention = torch.diag(head_attn).mean().item()\n",
    "            local_attention = sum(head_attn[i, max(0, i-1):min(seq_len, i+2)].sum() \n",
    "                                for i in range(seq_len)) / seq_len\n",
    "            global_attention = head_attn.sum() - local_attention * seq_len\n",
    "            \n",
    "            head_analysis[head] = {\n",
    "                'self_attention': self_attention,\n",
    "                'local_attention': local_attention.item(),\n",
    "                'global_attention': global_attention.item() / seq_len\n",
    "            }\n",
    "        \n",
    "        results[case_name] = head_analysis\n",
    "    \n",
    "    # Display results\n",
    "    print(\"Head Specialization Across Different Sequence Types:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for case_name, head_data in results.items():\n",
    "        print(f\"\\n{case_name.upper()} Sequences:\")\n",
    "        print(\"Head | Self-Attn | Local-Attn | Global-Attn\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        for head, metrics in head_data.items():\n",
    "            print(f\"{head+1:4d} | {metrics['self_attention']:8.3f} | \"\n",
    "                  f\"{metrics['local_attention']:9.3f} | {metrics['global_attention']:10.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "specialization_results = analyze_head_specialization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Concatenation and Projection Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_concatenation_projection():\n",
    "    \"\"\"Show the concatenation and projection process in detail.\"\"\"\n",
    "    \n",
    "    print(\"Multi-Head Concatenation and Projection\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    batch_size, seq_len, d_model = 1, 4, 12\n",
    "    num_heads = 3\n",
    "    d_k = d_model // num_heads  # 4\n",
    "    \n",
    "    # Simulate attention outputs from different heads\n",
    "    torch.manual_seed(42)\n",
    "    head_outputs = []\n",
    "    \n",
    "    for head in range(num_heads):\n",
    "        # Each head outputs [batch_size, seq_len, d_k]\n",
    "        head_output = torch.randn(batch_size, seq_len, d_k)\n",
    "        head_outputs.append(head_output)\n",
    "        print(f\"Head {head + 1} output shape: {head_output.shape}\")\n",
    "    \n",
    "    print(f\"\\nEach head dimension d_k: {d_k}\")\n",
    "    print(f\"Total model dimension d_model: {d_model}\")\n",
    "    \n",
    "    # Step 1: Concatenation\n",
    "    concatenated = torch.cat(head_outputs, dim=-1)\n",
    "    print(f\"\\nAfter concatenation shape: {concatenated.shape}\")\n",
    "    print(\"Concatenated output (first token, all dimensions):\")\n",
    "    print(concatenated[0, 0].detach().numpy().round(3))\n",
    "    \n",
    "    # Step 2: Linear projection\n",
    "    w_o = nn.Linear(d_model, d_model)\n",
    "    final_output = w_o(concatenated)\n",
    "    \n",
    "    print(f\"\\nAfter projection shape: {final_output.shape}\")\n",
    "    print(\"Final output (first token, first 6 dims):\")\n",
    "    print(final_output[0, 0, :6].detach().numpy().round(3))\n",
    "    \n",
    "    # Visualize the process\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Head outputs\n",
    "    for i, head_output in enumerate(head_outputs):\n",
    "        axes[0].imshow(head_output[0].detach().numpy().T, \n",
    "                      cmap='RdBu', aspect='auto')\n",
    "        axes[0].axvline(x=i*seq_len + seq_len - 0.5, color='white', linewidth=2)\n",
    "    axes[0].set_title('Individual Head Outputs')\n",
    "    axes[0].set_xlabel('Sequence Position')\n",
    "    axes[0].set_ylabel('Feature Dimension')\n",
    "    \n",
    "    # Concatenated\n",
    "    axes[1].imshow(concatenated[0].detach().numpy().T, \n",
    "                  cmap='RdBu', aspect='auto')\n",
    "    axes[1].set_title('Concatenated Output')\n",
    "    axes[1].set_xlabel('Sequence Position')\n",
    "    axes[1].set_ylabel('Feature Dimension')\n",
    "    \n",
    "    # Final projection\n",
    "    axes[2].imshow(final_output[0].detach().numpy().T, \n",
    "                  cmap='RdBu', aspect='auto')\n",
    "    axes[2].set_title('After Linear Projection')\n",
    "    axes[2].set_xlabel('Sequence Position')\n",
    "    axes[2].set_ylabel('Feature Dimension')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return concatenated, final_output\n",
    "\n",
    "concat_output, final_output = demonstrate_concatenation_projection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing Single vs Multi-Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_single_vs_multihead():\n",
    "    \"\"\"Compare single-head vs multi-head attention performance.\"\"\"\n",
    "    \n",
    "    seq_len, d_model = 8, 64\n",
    "    \n",
    "    # Create input with complex relationships\n",
    "    torch.manual_seed(42)\n",
    "    x = torch.randn(1, seq_len, d_model)\n",
    "    \n",
    "    # Add multiple types of relationships\n",
    "    # Local dependencies\n",
    "    for i in range(seq_len - 1):\n",
    "        x[0, i+1] += 0.2 * x[0, i]\n",
    "    \n",
    "    # Long-range dependencies  \n",
    "    x[0, -1] += 0.3 * x[0, 0]\n",
    "    x[0, -2] += 0.3 * x[0, 1]\n",
    "    \n",
    "    # Single-head attention\n",
    "    single_head = MultiHeadAttention(d_model, num_heads=1)\n",
    "    single_output, single_weights = single_head(x, x, x)\n",
    "    \n",
    "    # Multi-head attention\n",
    "    multi_head = MultiHeadAttention(d_model, num_heads=8)\n",
    "    multi_output, multi_weights = multi_head(x, x, x)\n",
    "    \n",
    "    # Visualize attention patterns\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Single head\n",
    "    sns.heatmap(single_weights[0, 0].detach().numpy(),\n",
    "               annot=True, fmt='.2f', cmap='Blues', ax=axes[0])\n",
    "    axes[0].set_title('Single Head Attention')\n",
    "    \n",
    "    # Multi-head (average)\n",
    "    avg_multi_weights = multi_weights[0].mean(dim=0)\n",
    "    sns.heatmap(avg_multi_weights.detach().numpy(),\n",
    "               annot=True, fmt='.2f', cmap='Blues', ax=axes[1])\n",
    "    axes[1].set_title('Multi-Head (Average)')\n",
    "    \n",
    "    # Multi-head diversity (std across heads)\n",
    "    std_multi_weights = multi_weights[0].std(dim=0)\n",
    "    sns.heatmap(std_multi_weights.detach().numpy(),\n",
    "               annot=True, fmt='.2f', cmap='Reds', ax=axes[2])\n",
    "    axes[2].set_title('Multi-Head Diversity (Std)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Quantitative comparison\n",
    "    print(\"Single vs Multi-Head Comparison:\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Attention diversity\n",
    "    single_entropy = -torch.sum(single_weights * torch.log(single_weights + 1e-9), dim=-1).mean()\n",
    "    multi_entropy = -torch.sum(multi_weights * torch.log(multi_weights + 1e-9), dim=-1).mean()\n",
    "    \n",
    "    print(f\"Single-head entropy: {single_entropy:.3f}\")\n",
    "    print(f\"Multi-head entropy: {multi_entropy:.3f}\")\n",
    "    \n",
    "    # Output difference\n",
    "    output_diff = torch.norm(single_output - multi_output).item()\n",
    "    print(f\"Output difference norm: {output_diff:.3f}\")\n",
    "    \n",
    "    # Head diversity in multi-head\n",
    "    head_similarities = []\n",
    "    for i in range(8):\n",
    "        for j in range(i+1, 8):\n",
    "            sim = F.cosine_similarity(\n",
    "                multi_weights[0, i].flatten(),\n",
    "                multi_weights[0, j].flatten(),\n",
    "                dim=0\n",
    "            )\n",
    "            head_similarities.append(sim.item())\n",
    "    \n",
    "    avg_head_similarity = np.mean(head_similarities)\n",
    "    print(f\"Average head similarity: {avg_head_similarity:.3f}\")\n",
    "    print(f\"Head diversity: {'High' if avg_head_similarity < 0.5 else 'Low'}\")\n",
    "\n",
    "compare_single_vs_multihead()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
