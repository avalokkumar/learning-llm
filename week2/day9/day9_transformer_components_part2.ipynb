{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 9: Transformer Components - Part 2\n",
    "\n",
    "This notebook explores GELU activation functions and complete transformer block architectures.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GELU Activation Function\n",
    "\n",
    "Let's implement and analyze the GELU activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu_exact(x):\n",
    "    \"\"\"Exact GELU implementation using error function.\"\"\"\n",
    "    return 0.5 * x * (1 + torch.erf(x / math.sqrt(2)))\n",
    "\n",
    "def gelu_approx(x):\n",
    "    \"\"\"Approximate GELU implementation using tanh.\"\"\"\n",
    "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2/math.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "def compare_activations():\n",
    "    \"\"\"Compare different activation functions.\"\"\"\n",
    "    \n",
    "    x = torch.linspace(-4, 4, 1000)\n",
    "    \n",
    "    # Compute different activations\n",
    "    relu_out = F.relu(x)\n",
    "    gelu_exact_out = gelu_exact(x)\n",
    "    gelu_approx_out = gelu_approx(x)\n",
    "    gelu_pytorch_out = F.gelu(x)\n",
    "    swish_out = x * torch.sigmoid(x)\n",
    "    \n",
    "    # Plot activation functions\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Activation functions\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(x, relu_out, label='ReLU', linewidth=2)\n",
    "    plt.plot(x, gelu_exact_out, label='GELU (Exact)', linewidth=2)\n",
    "    plt.plot(x, gelu_approx_out, label='GELU (Approx)', linewidth=2, linestyle='--')\n",
    "    plt.plot(x, swish_out, label='Swish', linewidth=2)\n",
    "    plt.xlabel('Input')\n",
    "    plt.ylabel('Output')\n",
    "    plt.title('Activation Functions Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Derivatives\n",
    "    plt.subplot(2, 2, 2)\n",
    "    x_grad = torch.linspace(-4, 4, 1000, requires_grad=True)\n",
    "    \n",
    "    relu_grad = torch.autograd.grad(F.relu(x_grad).sum(), x_grad, create_graph=True)[0]\n",
    "    gelu_grad = torch.autograd.grad(F.gelu(x_grad).sum(), x_grad, create_graph=True)[0]\n",
    "    swish_grad = torch.autograd.grad((x_grad * torch.sigmoid(x_grad)).sum(), x_grad, create_graph=True)[0]\n",
    "    \n",
    "    plt.plot(x_grad.detach(), relu_grad.detach(), label='ReLU', linewidth=2)\n",
    "    plt.plot(x_grad.detach(), gelu_grad.detach(), label='GELU', linewidth=2)\n",
    "    plt.plot(x_grad.detach(), swish_grad.detach(), label='Swish', linewidth=2)\n",
    "    plt.xlabel('Input')\n",
    "    plt.ylabel('Gradient')\n",
    "    plt.title('Activation Function Gradients')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # GELU approximation accuracy\n",
    "    plt.subplot(2, 2, 3)\n",
    "    error = torch.abs(gelu_exact_out - gelu_approx_out)\n",
    "    plt.plot(x, error, linewidth=2, color='red')\n",
    "    plt.xlabel('Input')\n",
    "    plt.ylabel('Absolute Error')\n",
    "    plt.title('GELU Approximation Error')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # Probabilistic interpretation\n",
    "    plt.subplot(2, 2, 4)\n",
    "    cdf = 0.5 * (1 + torch.erf(x / math.sqrt(2)))\n",
    "    plt.plot(x, cdf, label='Î¦(x) - CDF', linewidth=2)\n",
    "    plt.plot(x, gelu_exact_out / x, label='GELU(x)/x', linewidth=2)\n",
    "    plt.xlabel('Input')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('GELU Probabilistic Interpretation')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Maximum approximation error: {error.max():.6f}\")\n",
    "    print(f\"Mean approximation error: {error.mean():.6f}\")\n",
    "\n",
    "compare_activations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feed-Forward Network Implementation\n",
    "\n",
    "Let's implement the position-wise feed-forward network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, dropout=0.1, activation='gelu'):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.activation == 'gelu':\n",
    "            return self.w_2(self.dropout(F.gelu(self.w_1(x))))\n",
    "        elif self.activation == 'relu':\n",
    "            return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {self.activation}\")\n",
    "\n",
    "def test_ffn_activations():\n",
    "    \"\"\"Test different activations in feed-forward networks.\"\"\"\n",
    "    \n",
    "    d_model = 128\n",
    "    d_ff = 512\n",
    "    seq_len = 32\n",
    "    batch_size = 8\n",
    "    \n",
    "    # Create input\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Create networks with different activations\n",
    "    ffn_gelu = PositionwiseFeedForward(d_model, d_ff, activation='gelu')\n",
    "    ffn_relu = PositionwiseFeedForward(d_model, d_ff, activation='relu')\n",
    "    \n",
    "    # Forward pass\n",
    "    out_gelu = ffn_gelu(x)\n",
    "    out_relu = ffn_relu(x)\n",
    "    \n",
    "    # Analyze outputs\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Output distributions\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.hist(out_gelu.detach().flatten(), bins=50, alpha=0.7, label='GELU', density=True)\n",
    "    plt.hist(out_relu.detach().flatten(), bins=50, alpha=0.7, label='ReLU', density=True)\n",
    "    plt.xlabel('Output Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Output Distributions')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Activation patterns\n",
    "    plt.subplot(2, 2, 2)\n",
    "    gelu_activations = F.gelu(ffn_gelu.w_1(x))\n",
    "    relu_activations = F.relu(ffn_relu.w_1(x))\n",
    "    \n",
    "    gelu_sparsity = (gelu_activations == 0).float().mean()\n",
    "    relu_sparsity = (relu_activations == 0).float().mean()\n",
    "    \n",
    "    plt.bar(['GELU', 'ReLU'], [gelu_sparsity, relu_sparsity])\n",
    "    plt.ylabel('Sparsity (Fraction of Zeros)')\n",
    "    plt.title('Activation Sparsity')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gradient analysis\n",
    "    plt.subplot(2, 2, 3)\n",
    "    x_grad = x.clone().requires_grad_(True)\n",
    "    \n",
    "    loss_gelu = ffn_gelu(x_grad).sum()\n",
    "    loss_gelu.backward()\n",
    "    grad_gelu = x_grad.grad.clone()\n",
    "    \n",
    "    x_grad.grad.zero_()\n",
    "    loss_relu = ffn_relu(x_grad).sum()\n",
    "    loss_relu.backward()\n",
    "    grad_relu = x_grad.grad.clone()\n",
    "    \n",
    "    plt.hist(grad_gelu.flatten(), bins=50, alpha=0.7, label='GELU', density=True)\n",
    "    plt.hist(grad_relu.flatten(), bins=50, alpha=0.7, label='ReLU', density=True)\n",
    "    plt.xlabel('Gradient Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Gradient Distributions')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Statistics comparison\n",
    "    plt.subplot(2, 2, 4)\n",
    "    stats = {\n",
    "        'Mean': [out_gelu.mean().item(), out_relu.mean().item()],\n",
    "        'Std': [out_gelu.std().item(), out_relu.std().item()],\n",
    "        'Grad Norm': [grad_gelu.norm().item(), grad_relu.norm().item()]\n",
    "    }\n",
    "    \n",
    "    x_pos = np.arange(len(stats))\n",
    "    width = 0.35\n",
    "    \n",
    "    gelu_values = [stats[key][0] for key in stats.keys()]\n",
    "    relu_values = [stats[key][1] for key in stats.keys()]\n",
    "    \n",
    "    plt.bar(x_pos - width/2, gelu_values, width, label='GELU')\n",
    "    plt.bar(x_pos + width/2, relu_values, width, label='ReLU')\n",
    "    plt.xlabel('Statistic')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Statistical Comparison')\n",
    "    plt.xticks(x_pos, stats.keys())\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"GELU - Mean: {out_gelu.mean():.4f}, Std: {out_gelu.std():.4f}\")\n",
    "    print(f\"ReLU - Mean: {out_relu.mean():.4f}, Std: {out_relu.std():.4f}\")\n",
    "    print(f\"GELU Sparsity: {gelu_sparsity:.4f}\")\n",
    "    print(f\"ReLU Sparsity: {relu_sparsity:.4f}\")\n",
    "\n",
    "test_ffn_activations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Complete Transformer Block\n",
    "\n",
    "Let's implement and test complete transformer blocks with different configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "\n",
    "class SimpleAttention(nn.Module):\n",
    "    \"\"\"Simplified attention for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        Q = self.w_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        return self.w_o(context)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Complete transformer block.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1, norm_first=True):\n",
    "        super().__init__()\n",
    "        self.norm_first = norm_first\n",
    "        \n",
    "        self.attention = SimpleAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.norm_first:  # Pre-norm\n",
    "            # Self-attention with residual\n",
    "            attn_out = self.attention(self.norm1(x))\n",
    "            x = x + self.dropout(attn_out)\n",
    "            \n",
    "            # Feed-forward with residual\n",
    "            ff_out = self.feed_forward(self.norm2(x))\n",
    "            x = x + self.dropout(ff_out)\n",
    "        else:  # Post-norm\n",
    "            # Self-attention with residual and norm\n",
    "            attn_out = self.attention(x)\n",
    "            x = self.norm1(x + self.dropout(attn_out))\n",
    "            \n",
    "            # Feed-forward with residual and norm\n",
    "            ff_out = self.feed_forward(x)\n",
    "            x = self.norm2(x + self.dropout(ff_out))\n",
    "        \n",
    "        return x\n",
    "\n",
    "def compare_transformer_blocks():\n",
    "    \"\"\"Compare pre-norm vs post-norm transformer blocks.\"\"\"\n",
    "    \n",
    "    d_model = 128\n",
    "    num_heads = 8\n",
    "    d_ff = 512\n",
    "    seq_len = 32\n",
    "    batch_size = 4\n",
    "    num_layers = 6\n",
    "    \n",
    "    # Create models\n",
    "    pre_norm_blocks = nn.Sequential(*[\n",
    "        TransformerBlock(d_model, num_heads, d_ff, norm_first=True)\n",
    "        for _ in range(num_layers)\n",
    "    ])\n",
    "    \n",
    "    post_norm_blocks = nn.Sequential(*[\n",
    "        TransformerBlock(d_model, num_heads, d_ff, norm_first=False)\n",
    "        for _ in range(num_layers)\n",
    "    ])\n",
    "    \n",
    "    # Create input\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Track activations through layers\n",
    "    pre_norm_activations = []\n",
    "    post_norm_activations = []\n",
    "    \n",
    "    # Pre-norm forward pass\n",
    "    current_x = x.clone()\n",
    "    pre_norm_activations.append(current_x.norm(dim=-1).mean())\n",
    "    \n",
    "    for block in pre_norm_blocks:\n",
    "        current_x = block(current_x)\n",
    "        pre_norm_activations.append(current_x.norm(dim=-1).mean())\n",
    "    \n",
    "    # Post-norm forward pass\n",
    "    current_x = x.clone()\n",
    "    post_norm_activations.append(current_x.norm(dim=-1).mean())\n",
    "    \n",
    "    for block in post_norm_blocks:\n",
    "        current_x = block(current_x)\n",
    "        post_norm_activations.append(current_x.norm(dim=-1).mean())\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Activation norms\n",
    "    plt.subplot(2, 2, 1)\n",
    "    layers = list(range(num_layers + 1))\n",
    "    plt.plot(layers, [x.item() for x in pre_norm_activations], 'o-', label='Pre-norm', linewidth=2)\n",
    "    plt.plot(layers, [x.item() for x in post_norm_activations], 's-', label='Post-norm', linewidth=2)\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('Activation Norm')\n",
    "    plt.title('Activation Norms Through Layers')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gradient analysis\n",
    "    plt.subplot(2, 2, 2)\n",
    "    \n",
    "    # Compute gradients\n",
    "    x_grad = x.clone().requires_grad_(True)\n",
    "    pre_norm_out = pre_norm_blocks(x_grad)\n",
    "    pre_norm_loss = pre_norm_out.sum()\n",
    "    pre_norm_loss.backward()\n",
    "    pre_norm_grad = x_grad.grad.norm().item()\n",
    "    \n",
    "    x_grad = x.clone().requires_grad_(True)\n",
    "    post_norm_out = post_norm_blocks(x_grad)\n",
    "    post_norm_loss = post_norm_out.sum()\n",
    "    post_norm_loss.backward()\n",
    "    post_norm_grad = x_grad.grad.norm().item()\n",
    "    \n",
    "    plt.bar(['Pre-norm', 'Post-norm'], [pre_norm_grad, post_norm_grad])\n",
    "    plt.ylabel('Input Gradient Norm')\n",
    "    plt.title('Gradient Flow to Input')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Output distributions\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.hist(pre_norm_out.detach().flatten(), bins=50, alpha=0.7, label='Pre-norm', density=True)\n",
    "    plt.hist(post_norm_out.detach().flatten(), bins=50, alpha=0.7, label='Post-norm', density=True)\n",
    "    plt.xlabel('Output Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Output Distributions')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Parameter count\n",
    "    plt.subplot(2, 2, 4)\n",
    "    pre_norm_params = sum(p.numel() for p in pre_norm_blocks.parameters())\n",
    "    post_norm_params = sum(p.numel() for p in post_norm_blocks.parameters())\n",
    "    \n",
    "    plt.bar(['Pre-norm', 'Post-norm'], [pre_norm_params, post_norm_params])\n",
    "    plt.ylabel('Parameter Count')\n",
    "    plt.title('Model Size Comparison')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Pre-norm final activation norm: {pre_norm_activations[-1]:.4f}\")\n",
    "    print(f\"Post-norm final activation norm: {post_norm_activations[-1]:.4f}\")\n",
    "    print(f\"Pre-norm gradient norm: {pre_norm_grad:.4f}\")\n",
    "    print(f\"Post-norm gradient norm: {post_norm_grad:.4f}\")\n",
    "    print(f\"Pre-norm parameters: {pre_norm_params:,}\")\n",
    "    print(f\"Post-norm parameters: {post_norm_params:,}\")\n",
    "\n",
    "compare_transformer_blocks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary and Key Insights\n",
    "\n",
    "This notebook demonstrated the key transformer components:\n",
    "\n",
    "1. **GELU Activation**: Provides smooth, probabilistic gating with better gradient flow than ReLU\n",
    "2. **Feed-Forward Networks**: Position-wise transformations that provide model capacity\n",
    "3. **Pre-norm vs Post-norm**: Pre-norm generally provides better training stability\n",
    "\n",
    "These components work together to create stable, trainable transformer architectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
