{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 9: Transformer Components - Part 1\n",
    "\n",
    "This notebook explores residual connections and layer normalization - two critical components that make transformer training stable and effective.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Residual Connections Implementation\n",
    "\n",
    "Let's implement and visualize how residual connections work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLayer(nn.Module):\n",
    "    \"\"\"A simple linear layer for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.tanh(self.linear(x))\n",
    "\n",
    "class ResidualLayer(nn.Module):\n",
    "    \"\"\"Layer with residual connection.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.layer = SimpleLayer(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.layer(x)  # Residual connection\n",
    "\n",
    "class DeepNetwork(nn.Module):\n",
    "    \"\"\"Deep network with optional residual connections.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_layers, use_residual=True):\n",
    "        super().__init__()\n",
    "        self.use_residual = use_residual\n",
    "        \n",
    "        if use_residual:\n",
    "            self.layers = nn.ModuleList([\n",
    "                ResidualLayer(d_model) for _ in range(num_layers)\n",
    "            ])\n",
    "        else:\n",
    "            self.layers = nn.ModuleList([\n",
    "                SimpleLayer(d_model) for _ in range(num_layers)\n",
    "            ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        activations = [x]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            activations.append(x)\n",
    "        \n",
    "        return x, activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient Flow Analysis\n",
    "\n",
    "Let's analyze how residual connections affect gradient flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gradient_flow():\n",
    "    \"\"\"Compare gradient flow with and without residual connections.\"\"\"\n",
    "    \n",
    "    d_model = 64\n",
    "    num_layers = 10\n",
    "    batch_size = 32\n",
    "    \n",
    "    # Create networks\n",
    "    net_with_residual = DeepNetwork(d_model, num_layers, use_residual=True)\n",
    "    net_without_residual = DeepNetwork(d_model, num_layers, use_residual=False)\n",
    "    \n",
    "    # Create input\n",
    "    x = torch.randn(batch_size, d_model)\n",
    "    target = torch.randn(batch_size, d_model)\n",
    "    \n",
    "    # Forward pass and compute loss\n",
    "    def compute_gradients(network, x, target):\n",
    "        network.zero_grad()\n",
    "        output, _ = network(x)\n",
    "        loss = F.mse_loss(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        gradients = []\n",
    "        for layer in network.layers:\n",
    "            if hasattr(layer, 'layer'):\n",
    "                # Residual layer\n",
    "                grad_norm = layer.layer.linear.weight.grad.norm().item()\n",
    "            else:\n",
    "                # Simple layer\n",
    "                grad_norm = layer.linear.weight.grad.norm().item()\n",
    "            gradients.append(grad_norm)\n",
    "        \n",
    "        return gradients, loss.item()\n",
    "    \n",
    "    # Compute gradients\n",
    "    grads_with_residual, loss_with = compute_gradients(net_with_residual, x, target)\n",
    "    grads_without_residual, loss_without = compute_gradients(net_without_residual, x, target)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    layers = list(range(1, num_layers + 1))\n",
    "    plt.plot(layers, grads_with_residual, 'o-', label='With Residual', linewidth=2)\n",
    "    plt.plot(layers, grads_without_residual, 's-', label='Without Residual', linewidth=2)\n",
    "    plt.xlabel('Layer Number')\n",
    "    plt.ylabel('Gradient Norm')\n",
    "    plt.title('Gradient Flow Comparison')\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(['With Residual', 'Without Residual'], [loss_with, loss_without])\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Average gradient norm with residual: {np.mean(grads_with_residual):.6f}\")\n",
    "    print(f\"Average gradient norm without residual: {np.mean(grads_without_residual):.6f}\")\n",
    "    print(f\"Loss with residual: {loss_with:.6f}\")\n",
    "    print(f\"Loss without residual: {loss_without:.6f}\")\n",
    "\n",
    "analyze_gradient_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Layer Normalization Implementation\n",
    "\n",
    "Now let's implement and compare different normalization techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"Custom Layer Normalization implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Compute mean and std across the feature dimension\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        \n",
    "        # Normalize\n",
    "        normalized = (x - mean) / (std + self.eps)\n",
    "        \n",
    "        # Scale and shift\n",
    "        return self.gamma * normalized + self.beta\n",
    "\n",
    "def compare_normalizations():\n",
    "    \"\"\"Compare different normalization techniques.\"\"\"\n",
    "    \n",
    "    batch_size = 8\n",
    "    seq_len = 16\n",
    "    d_model = 64\n",
    "    \n",
    "    # Create input with varying scales\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    x[:, :, :32] *= 10  # Make first half of features larger\n",
    "    x[:, :, 32:] *= 0.1  # Make second half smaller\n",
    "    \n",
    "    # Apply different normalizations\n",
    "    layer_norm = LayerNorm(d_model)\n",
    "    batch_norm = nn.BatchNorm1d(d_model)\n",
    "    \n",
    "    # Layer normalization\n",
    "    x_layer_norm = layer_norm(x)\n",
    "    \n",
    "    # Batch normalization (need to reshape)\n",
    "    x_reshaped = x.view(-1, d_model).transpose(0, 1)\n",
    "    x_batch_norm = batch_norm(x_reshaped).transpose(0, 1).view(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    \n",
    "    # Original input\n",
    "    im1 = axes[0, 0].imshow(x[0].detach().numpy(), cmap='RdBu', aspect='auto')\n",
    "    axes[0, 0].set_title('Original Input')\n",
    "    axes[0, 0].set_xlabel('Feature Dimension')\n",
    "    axes[0, 0].set_ylabel('Sequence Position')\n",
    "    plt.colorbar(im1, ax=axes[0, 0])\n",
    "    \n",
    "    # Layer normalized\n",
    "    im2 = axes[0, 1].imshow(x_layer_norm[0].detach().numpy(), cmap='RdBu', aspect='auto')\n",
    "    axes[0, 1].set_title('Layer Normalized')\n",
    "    axes[0, 1].set_xlabel('Feature Dimension')\n",
    "    axes[0, 1].set_ylabel('Sequence Position')\n",
    "    plt.colorbar(im2, ax=axes[0, 1])\n",
    "    \n",
    "    # Batch normalized\n",
    "    im3 = axes[0, 2].imshow(x_batch_norm[0].detach().numpy(), cmap='RdBu', aspect='auto')\n",
    "    axes[0, 2].set_title('Batch Normalized')\n",
    "    axes[0, 2].set_xlabel('Feature Dimension')\n",
    "    axes[0, 2].set_ylabel('Sequence Position')\n",
    "    plt.colorbar(im3, ax=axes[0, 2])\n",
    "    \n",
    "    # Statistics comparison\n",
    "    axes[1, 0].hist(x[0].flatten().detach().numpy(), bins=50, alpha=0.7, label='Original')\n",
    "    axes[1, 0].set_title('Original Distribution')\n",
    "    axes[1, 0].set_xlabel('Value')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    axes[1, 1].hist(x_layer_norm[0].flatten().detach().numpy(), bins=50, alpha=0.7, label='Layer Norm', color='orange')\n",
    "    axes[1, 1].set_title('Layer Norm Distribution')\n",
    "    axes[1, 1].set_xlabel('Value')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    axes[1, 2].hist(x_batch_norm[0].flatten().detach().numpy(), bins=50, alpha=0.7, label='Batch Norm', color='green')\n",
    "    axes[1, 2].set_title('Batch Norm Distribution')\n",
    "    axes[1, 2].set_xlabel('Value')\n",
    "    axes[1, 2].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"Statistics Comparison:\")\n",
    "    print(f\"Original - Mean: {x.mean():.4f}, Std: {x.std():.4f}\")\n",
    "    print(f\"Layer Norm - Mean: {x_layer_norm.mean():.4f}, Std: {x_layer_norm.std():.4f}\")\n",
    "    print(f\"Batch Norm - Mean: {x_batch_norm.mean():.4f}, Std: {x_batch_norm.std():.4f}\")\n",
    "\n",
    "compare_normalizations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Stability Analysis\n",
    "\n",
    "Let's analyze how these components affect training stability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerComponent(nn.Module):\n",
    "    \"\"\"Basic transformer component with configurable normalization and residual connections.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, use_residual=True, use_layer_norm=True):\n",
    "        super().__init__()\n",
    "        self.use_residual = use_residual\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, d_model * 4)\n",
    "        self.linear2 = nn.Linear(d_model * 4, d_model)\n",
    "        \n",
    "        if use_layer_norm:\n",
    "            self.norm = LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        # Apply layer norm first (pre-norm)\n",
    "        if self.use_layer_norm:\n",
    "            x = self.norm(x)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        x = F.gelu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        # Add residual connection\n",
    "        if self.use_residual:\n",
    "            x = x + residual\n",
    "        \n",
    "        return x\n",
    "\n",
    "def training_stability_experiment():\n",
    "    \"\"\"Experiment to show training stability with different configurations.\"\"\"\n",
    "    \n",
    "    d_model = 128\n",
    "    seq_len = 32\n",
    "    batch_size = 16\n",
    "    num_steps = 100\n",
    "    \n",
    "    # Different configurations\n",
    "    configs = [\n",
    "        {'use_residual': True, 'use_layer_norm': True, 'name': 'Residual + LayerNorm'},\n",
    "        {'use_residual': True, 'use_layer_norm': False, 'name': 'Residual Only'},\n",
    "        {'use_residual': False, 'use_layer_norm': True, 'name': 'LayerNorm Only'},\n",
    "        {'use_residual': False, 'use_layer_norm': False, 'name': 'Neither'}\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config in configs:\n",
    "        # Create model\n",
    "        model = nn.Sequential(*[\n",
    "            TransformerComponent(d_model, config['use_residual'], config['use_layer_norm'])\n",
    "            for _ in range(4)\n",
    "        ])\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        losses = []\n",
    "        grad_norms = []\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            # Generate random data\n",
    "            x = torch.randn(batch_size, seq_len, d_model)\n",
    "            target = torch.randn(batch_size, seq_len, d_model)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = F.mse_loss(output, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Compute gradient norm\n",
    "            total_norm = 0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** (1. / 2)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            grad_norms.append(total_norm)\n",
    "        \n",
    "        results[config['name']] = {'losses': losses, 'grad_norms': grad_norms}\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Loss curves\n",
    "    for name, data in results.items():\n",
    "        axes[0].plot(data['losses'], label=name, linewidth=2)\n",
    "    axes[0].set_xlabel('Training Step')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_yscale('log')\n",
    "    \n",
    "    # Gradient norms\n",
    "    for name, data in results.items():\n",
    "        axes[1].plot(data['grad_norms'], label=name, linewidth=2)\n",
    "    axes[1].set_xlabel('Training Step')\n",
    "    axes[1].set_ylabel('Gradient Norm')\n",
    "    axes[1].set_title('Gradient Norms')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(\"Final Training Statistics:\")\n",
    "    for name, data in results.items():\n",
    "        final_loss = data['losses'][-1]\n",
    "        avg_grad_norm = np.mean(data['grad_norms'][-10:])\n",
    "        print(f\"{name:20s}: Loss = {final_loss:.6f}, Avg Grad Norm = {avg_grad_norm:.6f}\")\n",
    "\n",
    "training_stability_experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
