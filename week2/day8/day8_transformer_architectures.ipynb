{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 8: Transformer Architectures - Implementation and Analysis\n",
    "\n",
    "This notebook contains practical implementations and exercises for the three main transformer architectures:\n",
    "- Encoder-Only (BERT-style)\n",
    "- Decoder-Only (GPT-style) \n",
    "- Encoder-Decoder (T5-style)\n",
    "\n",
    "We'll also explore masked attention mechanisms and compare architectural properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Tuple\n",
    "import math\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Encoder-Only Architecture (BERT-style)\n",
    "\n",
    "The encoder-only architecture uses bidirectional attention to build rich contextual representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderOnlyTransformer(nn.Module):\n",
    "    \"\"\"BERT-style encoder-only transformer.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, d_model: int = 512, n_heads: int = 8, \n",
    "                 n_layers: int = 6, d_ff: int = 2048, max_seq_len: int = 512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, n_layers)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        # Embedding + positional encoding\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Transformer encoder (bidirectional attention)\n",
    "        x = self.transformer(x, src_key_padding_mask=mask)\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_seq_len: int = 512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test encoder-only model\n",
    "vocab_size = 1000\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "encoder_model = EncoderOnlyTransformer(vocab_size, d_model=256, n_heads=8, n_layers=4)\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = encoder_model(input_ids)\n",
    "    print(f\"Encoder-only output shape: {output.shape}\")\n",
    "    print(f\"Each token has bidirectional context from all other tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decoder-Only Architecture (GPT-style)\n",
    "\n",
    "The decoder-only architecture uses causal (masked) attention for autoregressive generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    \"\"\"GPT-style decoder-only transformer.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, d_model: int = 512, n_heads: int = 8, \n",
    "                 n_layers: int = 6, d_ff: int = 2048, max_seq_len: int = 512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerDecoder(decoder_layer, n_layers)\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # Create causal mask\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "        \n",
    "        # Embedding + positional encoding\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Transformer decoder (causal attention)\n",
    "        # Note: Using x as both tgt and memory for decoder-only architecture\n",
    "        x = self.transformer(x, x, tgt_mask=causal_mask, memory_mask=causal_mask)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = self.output_projection(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test decoder-only model\n",
    "decoder_model = DecoderOnlyTransformer(vocab_size, d_model=256, n_heads=8, n_layers=4)\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = decoder_model(input_ids)\n",
    "    print(f\"Decoder-only output shape: {logits.shape}\")\n",
    "    print(f\"Each token can only attend to previous tokens (causal masking)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoder-Decoder Architecture (T5-style)\n",
    "\n",
    "The encoder-decoder architecture combines bidirectional encoding with causal decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderTransformer(nn.Module):\n",
    "    \"\"\"T5-style encoder-decoder transformer.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, d_model: int = 512, n_heads: int = 8, \n",
    "                 n_layers: int = 6, d_ff: int = 2048, max_seq_len: int = 512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.src_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, n_layers)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, n_layers)\n",
    "        \n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, \n",
    "                src_mask: Optional[torch.Tensor] = None,\n",
    "                tgt_mask: Optional[torch.Tensor] = None):\n",
    "        \n",
    "        # Encode source sequence (bidirectional)\n",
    "        src_emb = self.src_embedding(src) * math.sqrt(self.d_model)\n",
    "        src_emb = self.pos_encoding(src_emb)\n",
    "        memory = self.encoder(src_emb, src_key_padding_mask=src_mask)\n",
    "        \n",
    "        # Decode target sequence (causal + cross-attention)\n",
    "        tgt_len = tgt.size(1)\n",
    "        if tgt_mask is None:\n",
    "            tgt_mask = torch.triu(torch.ones(tgt_len, tgt_len), diagonal=1).bool()\n",
    "        \n",
    "        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt_emb = self.pos_encoding(tgt_emb)\n",
    "        \n",
    "        output = self.decoder(tgt_emb, memory, tgt_mask=tgt_mask)\n",
    "        logits = self.output_projection(output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test encoder-decoder model\n",
    "enc_dec_model = EncoderDecoderTransformer(vocab_size, d_model=256, n_heads=8, n_layers=4)\n",
    "src_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "tgt_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = enc_dec_model(src_ids, tgt_ids)\n",
    "    print(f\"Encoder-decoder output shape: {logits.shape}\")\n",
    "    print(f\"Decoder attends to encoder outputs via cross-attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Masked Attention Visualization\n",
    "\n",
    "Let's visualize the different attention patterns used by each architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_patterns():\n",
    "    \"\"\"Visualize attention patterns for different architectures.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # BERT-style (bidirectional)\n",
    "    bert_pattern = torch.ones(4, 4)\n",
    "    sns.heatmap(bert_pattern.numpy(), annot=True, fmt='.0f', \n",
    "                cmap='Blues', ax=axes[0], cbar=False)\n",
    "    axes[0].set_title('BERT: Bidirectional\\n(All-to-All Attention)')\n",
    "    axes[0].set_xlabel('Key Positions')\n",
    "    axes[0].set_ylabel('Query Positions')\n",
    "    \n",
    "    # GPT-style (causal)\n",
    "    gpt_pattern = torch.tril(torch.ones(4, 4))\n",
    "    sns.heatmap(gpt_pattern.numpy(), annot=True, fmt='.0f',\n",
    "                cmap='Greens', ax=axes[1], cbar=False)\n",
    "    axes[1].set_title('GPT: Causal\\n(Lower Triangular)')\n",
    "    axes[1].set_xlabel('Key Positions')\n",
    "    axes[1].set_ylabel('Query Positions')\n",
    "    \n",
    "    # T5-style (cross-attention visualization)\n",
    "    t5_pattern = torch.ones(3, 4)  # 3 decoder tokens, 4 encoder tokens\n",
    "    sns.heatmap(t5_pattern.numpy(), annot=True, fmt='.0f',\n",
    "                cmap='Oranges', ax=axes[2], cbar=False)\n",
    "    axes[2].set_title('T5: Cross-Attention\\n(Decoder → Encoder)')\n",
    "    axes[2].set_xlabel('Encoder Positions')\n",
    "    axes[2].set_ylabel('Decoder Positions')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_attention_patterns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Architecture Comparison\n",
    "\n",
    "Let's compare the three architectures across different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_architectures():\n",
    "    \"\"\"Compare the three transformer architectures.\"\"\"\n",
    "    \n",
    "    print(\"Transformer Architecture Comparison\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    architectures = {\n",
    "        'Encoder-Only (BERT)': {\n",
    "            'attention_type': 'Bidirectional',\n",
    "            'use_cases': ['Classification', 'NER', 'Question Answering'],\n",
    "            'examples': ['BERT', 'RoBERTa', 'DeBERTa'],\n",
    "            'strengths': ['Rich bidirectional context', 'Good for understanding tasks'],\n",
    "            'limitations': ['Cannot generate text', 'No causal modeling']\n",
    "        },\n",
    "        'Decoder-Only (GPT)': {\n",
    "            'attention_type': 'Causal/Autoregressive',\n",
    "            'use_cases': ['Text Generation', 'Language Modeling', 'Few-shot Learning'],\n",
    "            'examples': ['GPT-2/3/4', 'LLaMA', 'PaLM'],\n",
    "            'strengths': ['Excellent generation', 'Scalable', 'Versatile'],\n",
    "            'limitations': ['No bidirectional context', 'Less efficient for understanding']\n",
    "        },\n",
    "        'Encoder-Decoder (T5)': {\n",
    "            'attention_type': 'Bidirectional + Cross-attention',\n",
    "            'use_cases': ['Translation', 'Summarization', 'Seq2Seq'],\n",
    "            'examples': ['T5', 'BART', 'mT5'],\n",
    "            'strengths': ['Best for seq2seq', 'Flexible input/output lengths'],\n",
    "            'limitations': ['More complex', 'Higher memory usage']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for arch_name, details in architectures.items():\n",
    "        print(f\"\\n{arch_name}:\")\n",
    "        print(f\"  Attention: {details['attention_type']}\")\n",
    "        print(f\"  Use cases: {', '.join(details['use_cases'])}\")\n",
    "        print(f\"  Examples: {', '.join(details['examples'])}\")\n",
    "        print(f\"  Strengths: {', '.join(details['strengths'])}\")\n",
    "        print(f\"  Limitations: {', '.join(details['limitations'])}\")\n",
    "\n",
    "compare_architectures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practical Implementation Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def practical_implementation_example():\n",
    "    \"\"\"Show practical implementation considerations.\"\"\"\n",
    "    \n",
    "    print(\"Practical Implementation Considerations\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Model size comparison\n",
    "    configs = {\n",
    "        'BERT-Base': {'layers': 12, 'd_model': 768, 'heads': 12, 'params': '110M'},\n",
    "        'GPT-2': {'layers': 12, 'd_model': 768, 'heads': 12, 'params': '117M'},\n",
    "        'T5-Base': {'layers': 12, 'd_model': 768, 'heads': 12, 'params': '220M'}\n",
    "    }\n",
    "    \n",
    "    print(\"Model Size Comparison:\")\n",
    "    print(\"Model      | Layers | d_model | Heads | Parameters\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for model, config in configs.items():\n",
    "        print(f\"{model:10s} | {config['layers']:6d} | {config['d_model']:7d} | \"\n",
    "              f\"{config['heads']:5d} | {config['params']:>10s}\")\n",
    "    \n",
    "    print(\"\\nMemory Usage Considerations:\")\n",
    "    seq_len = 512\n",
    "    batch_size = 8\n",
    "    d_model = 768\n",
    "    \n",
    "    # Attention memory (dominant factor)\n",
    "    attention_memory = batch_size * seq_len * seq_len * 4 / (1024**2)  # MB\n",
    "    \n",
    "    print(f\"Attention matrices: {attention_memory:.1f} MB\")\n",
    "    print(f\"Scales quadratically with sequence length\")\n",
    "    print(f\"Encoder-decoder uses ~2x memory (self + cross attention)\")\n",
    "    \n",
    "    print(\"\\nTraining Considerations:\")\n",
    "    print(\"- BERT: Masked Language Modeling + Next Sentence Prediction\")\n",
    "    print(\"- GPT: Causal Language Modeling\")\n",
    "    print(\"- T5: Span Corruption (text-to-text)\")\n",
    "\n",
    "practical_implementation_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Architecture Decision Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def architecture_decision_guide():\n",
    "    \"\"\"Guide for choosing the right architecture.\"\"\"\n",
    "    \n",
    "    print(\"Architecture Decision Guide\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Task-specific recommendations\n",
    "    task_recommendations = {\n",
    "        'Text Classification': 'Encoder-Only (BERT)',\n",
    "        'Sentiment Analysis': 'Encoder-Only (BERT)',\n",
    "        'Named Entity Recognition': 'Encoder-Only (BERT)',\n",
    "        'Question Answering': 'Encoder-Only (BERT)',\n",
    "        'Text Generation': 'Decoder-Only (GPT)',\n",
    "        'Language Modeling': 'Decoder-Only (GPT)',\n",
    "        'Code Generation': 'Decoder-Only (GPT)',\n",
    "        'Chat/Dialogue': 'Decoder-Only (GPT)',\n",
    "        'Machine Translation': 'Encoder-Decoder (T5)',\n",
    "        'Summarization': 'Encoder-Decoder (T5)',\n",
    "        'Text-to-Text Tasks': 'Encoder-Decoder (T5)',\n",
    "        'Paraphrasing': 'Encoder-Decoder (T5)'\n",
    "    }\n",
    "    \n",
    "    print(\"Task-Specific Recommendations:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for task, recommendation in task_recommendations.items():\n",
    "        print(f\"{task:25s}: {recommendation}\")\n",
    "    \n",
    "    print(\"\\nKey Decision Factors:\")\n",
    "    print(\"1. Generation vs Understanding\")\n",
    "    print(\"2. Bidirectional vs Causal context\")\n",
    "    print(\"3. Input-output relationship\")\n",
    "    print(\"4. Computational efficiency\")\n",
    "    print(\"5. Available training data\")\n",
    "\n",
    "architecture_decision_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercises\n",
    "\n",
    "Try these exercises to deepen your understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Implement a simple masked language model head for BERT\n",
    "class MLMHead(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.decoder = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = F.gelu(hidden_states)\n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "        return self.decoder(hidden_states)\n",
    "\n",
    "# Exercise 2: Create a simple text generation function for GPT\n",
    "def generate_text(model, input_ids, max_length=20, temperature=1.0):\n",
    "    \"\"\"Simple greedy generation for decoder-only model.\"\"\"\n",
    "    model.eval()\n",
    "    generated = input_ids.clone()\n",
    "    \n",
    "    for _ in range(max_length - input_ids.size(1)):\n",
    "        with torch.no_grad():\n",
    "            logits = model(generated)\n",
    "            next_token_logits = logits[:, -1, :] / temperature\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "print(\"Exercise implementations completed!\")\n",
    "print(\"Try modifying the architectures and experimenting with different configurations.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
