{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Day 14: Regularization, Monitoring, and Early Stopping - Part 2\n",
    "\n",
    "## Overview\n",
    "In this notebook, we'll build on Part 1 to implement monitoring and early stopping, two crucial techniques for robust model training. We will focus on:\n",
    "- Setting up a comprehensive logging system for training metrics.\n",
    "- Implementing an early stopping mechanism to prevent overfitting.\n",
    "- Combining regularization, monitoring, and early stopping into a final, robust training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Re-defining Model and Data (for standalone execution)\n",
    "\n",
    "Let's quickly redefine the model and dataset from Part 1 so this notebook can be run independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Definition ---\n",
    "class SimpleLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, batch_first=True, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = torch.triu(torch.ones(x.size(1), x.size(1)) * float('-inf'), diagonal=1).to(x.device)\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x, mask=mask)\n",
    "        return self.output\n",
    "\n",
    "# --- Dataset Definition ---\n",
    "class OverfitDataset(Dataset):\n",
    "    def __init__(self, vocab_size=100, seq_len=32, size=200):\n",
    "        self.patterns = []\n",
    "        for i in range(10):\n",
    "            pat = torch.randint(0, vocab_size, (seq_len + 1,))...\n",
    "            self.patterns.append(pat)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 200\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.patterns[idx % len(self.patterns)]\n",
    "        return seq[:-1], seq[1:]\n",
    "\n",
    "# --- Dataloaders ---\n",
    "vocab_size = 100\n",
    "seq_len = 32\n",
    "dataset = OverfitDataset(vocab_size, seq_len)\n",
    "train_dataset, val_dataset = random_split(dataset, [150, 50])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comprehensive Logging with TensorBoard\n",
    "\n",
    "Let's create a logger to track various metrics during training and visualize them with TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingLogger:\n",
    "    \"\"\"A simple logger for TensorBoard and console.\"\"\"\n",
    "    def __init__(self, log_dir='runs'):\n",
    "        self.log_dir = f'{log_dir}/{int(time.time())}'\n",
    "        self.writer = SummaryWriter(self.log_dir)\n",
    "        print(f'TensorBoard log directory: {self.log_dir}')\n",
    "\n",
    "    def log_scalar(self, tag, value, step):\n",
    "        self.writer.add_scalar(tag, value, step)\n",
    "\n",
    "    def log_histogram(self, tag, values, step):\n",
    "        self.writer.add_histogram(tag, values, step)\n",
    "\n",
    "    def log_hyperparameters(self, hparams):\n",
    "        self.writer.add_hparams(hparams, {})",
    "\n",
    "    def close(self):\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing Early Stopping\n",
    "\n",
    "Now, we'll create a class to handle early stopping, which saves the best model and stops training when performance on the validation set stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=5, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Complete Training Loop with Monitoring and Early Stopping\n",
    "\n",
    "Let's combine everything into a final, robust training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_train(model, train_loader, val_loader, epochs, lr, weight_decay, patience):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Initialize logger and early stopping\n",
    "    logger = TrainingLogger()\n",
    "    early_stopper = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    hparams = {'lr': lr, 'weight_decay': weight_decay, 'patience': patience, 'dropout': model.transformer.layers[0].dropout.p}\n",
    "    logger.log_hyperparameters(hparams)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss = evaluate(model, val_loader, criterion)\n",
    "        \n",
    "        # Log metrics\n",
    "        logger.log_scalar('Loss/train', avg_train_loss, epoch)\n",
    "        logger.log_scalar('Loss/validation', val_loss, epoch)\n",
    "        logger.log_scalar('Learning Rate', optimizer.param_groups[0]['lr'], epoch)\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                logger.log_histogram(f'Parameters/{name}', param.data, epoch)\n",
    "                if param.grad is not None:\n",
    "                    logger.log_histogram(f'Gradients/{name}', param.grad.data, epoch)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Time: {elapsed:.2f}s')\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopper(val_loss, model)\n",
    "        if early_stopper.early_stop:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "            \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    logger.close()\n",
    "    return model\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x)\n",
    "            loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Running the Robust Training Loop\n",
    "\n",
    "Let's run our final training loop and see early stopping in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with regularization\n",
    "final_model = SimpleLanguageModel(vocab_size=vocab_size, dropout=0.1)\n",
    "\n",
    "print('Training with monitoring and early stopping...')\n",
    "best_model = robust_train(\n",
    "    model=final_model, \n",
    "    train_loader=train_dataloader, \n",
    "    val_loader=val_dataloader, \n",
    "    epochs=100, # Set a high number of epochs\n",
    "    lr=0.001, \n",
    "    weight_decay=0.01,\n",
    "    patience=7 # Set patience for early stopping\n",
    ")\n",
    "\n",
    "print('\\nTo view TensorBoard logs, run the following command in your terminal:')\n",
    "print(f'tensorboard --logdir=runs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Insights\n",
    "\n",
    "In this notebook, we have implemented a robust training pipeline by incorporating:\n",
    "\n",
    "1.  **Regularization**: Using dropout and weight decay to prevent overfitting and improve model generalization.\n",
    "2.  **Monitoring**: Setting up a comprehensive logger with TensorBoard to track key metrics like loss, learning rate, and parameter distributions. This provides crucial insights into the training dynamics.\n",
    "3.  **Early Stopping**: Implementing a mechanism to automatically stop training when the model's performance on a validation set ceases to improve, which saves computational resources and retrieves the best-performing model state.\n",
    "\n",
    "By combining these techniques, we can train models more effectively, diagnose issues like overfitting, and ensure that we are selecting the best possible model based on its ability to generalize to unseen data. This forms the foundation of a production-ready training script for any deep learning model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
