{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Day 14: Regularization, Monitoring, and Early Stopping - Part 1\n",
    "\n",
    "## Overview\n",
    "In this notebook, we'll implement and explore key techniques for preventing overfitting and ensuring optimal model performance. We will focus on:\n",
    "- Implementing regularization (Dropout and Weight Decay)\n",
    "- Creating a model with configurable regularization\n",
    "- Setting up a synthetic dataset to demonstrate overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Language Model with Regularization\n",
    "\n",
    "Let's define a simple language model with configurable dropout for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLanguageModel(nn.Module):\n",
    "    \"\"\"A simple transformer-based language model with dropout.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=4, dim_feedforward=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output.bias.data.zero_()\n",
    "        self.output.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(self, src, src_mask=None):\n",
    "        if src_mask is None:\n",
    "            src_mask = generate_square_subsequent_mask(src.size(1)).to(src.device)\n",
    "        \n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_decoder(src, memory=src, tgt_mask=src_mask) # Using src as memory for decoder-only style\n",
    "        output = self.output(output)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Synthetic Dataset for Overfitting Demonstration\n",
    "\n",
    "Let's create a small, repetitive dataset to easily demonstrate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverfitDataset(Dataset):\n",
    "    \"\"\"A small, repetitive dataset designed to cause overfitting.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=100, seq_len=32, size=200):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.size = size\n",
    "        \n",
    "        # Create a few fixed, repetitive patterns\n",
    "        self.patterns = []\n",
    "        for i in range(10):\n",
    "            pattern_type = i % 3\n",
    "            if pattern_type == 0: # Sequential\n",
    "                start = torch.randint(0, vocab_size - seq_len, (1,)).item()\n",
    "                self.patterns.append(torch.arange(start, start + seq_len + 1))\n",
    "            elif pattern_type == 1: # Alternating\n",
    "                tok1, tok2 = torch.randint(0, vocab_size, (2,)).tolist()\n",
    "                pat = torch.tensor([tok1 if j % 2 == 0 else tok2 for j in range(seq_len + 1)])\n",
    "                self.patterns.append(pat)\n",
    "            else: # Repeating blocks\n",
    "                block = torch.randint(0, vocab_size, (4,))\n",
    "                pat = block.repeat((seq_len // 4) + 2)[:seq_len+1]\n",
    "                self.patterns.append(pat)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Return one of the fixed patterns\n",
    "        seq = self.patterns[idx % len(self.patterns)]\n",
    "        x = seq[:-1]\n",
    "        y = seq[1:]\n",
    "        return x, y\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "vocab_size = 100\n",
    "seq_len = 32\n",
    "dataset = OverfitDataset(vocab_size, seq_len, size=200)\n",
    "train_size = 150\n",
    "val_size = 50\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Training and Evaluation Loop\n",
    "\n",
    "Let's define a training loop. We will add more features to it in Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x)\n",
    "            loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs, lr, weight_decay=0.0):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        val_loss = evaluate(model, val_loader, criterion)\n",
    "        \n",
    "        history['train_loss'].append(avg_epoch_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Time: {elapsed:.2f}s')\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Demonstrating Overfitting\n",
    "\n",
    "Let's train a model with no regularization to observe overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with no dropout\n",
    "overfit_model = SimpleLanguageModel(vocab_size=vocab_size, dropout=0.0)\n",
    "\n",
    "print(\"Training a model with no regularization to induce overfitting...\")\n",
    "# Use a high learning rate and many epochs to speed up overfitting\n",
    "overfit_history = train(\n",
    "    model=overfit_model, \n",
    "    train_loader=train_dataloader, \n",
    "    val_loader=val_dataloader, \n",
    "    epochs=50, \n",
    "    lr=0.001,\n",
    "    weight_decay=0.0\n",
    ")\n",
    "\n",
    "def plot_loss_curves(history, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_curves(overfit_history, 'Overfitting: No Regularization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Applying Regularization\n",
    "\n",
    "Now, let's train a model with dropout and weight decay to mitigate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with dropout\n",
    "regularized_model = SimpleLanguageModel(vocab_size=vocab_size, dropout=0.2)\n",
    "\n",
    "print(\"\\nTraining a model with Dropout and Weight Decay...\")\n",
    "regularized_history = train(\n",
    "    model=regularized_model, \n",
    "    train_loader=train_dataloader, \n",
    "    val_loader=val_dataloader, \n",
    "    epochs=50, \n",
    "    lr=0.001, \n",
    "    weight_decay=0.01 # Add weight decay\n",
    ")\n",
    "\n",
    "plot_loss_curves(regularized_history, 'With Regularization (Dropout + Weight Decay)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Summary\n",
    "\n",
    "In this part, we have:\n",
    "1. Created a simple language model with configurable dropout.\n",
    "2. Designed a synthetic dataset to easily demonstrate overfitting.\n",
    "3. Trained a model without regularization and observed the validation loss increasing while training loss decreases, a clear sign of overfitting.\n",
    "4. Trained another model with dropout and weight decay, observing that the gap between training and validation loss is much smaller, indicating better generalization.\n",
    "\n",
    "In Part 2, we will explore monitoring with a logging system and implement early stopping to automatically find the best model checkpoint."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
