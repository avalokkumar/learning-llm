{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Day 13: Training Loop Details - Part 2\n",
    "\n",
    "## Overview\n",
    "In this notebook, we'll continue exploring key components of an efficient training loop, focusing on:\n",
    "- Learning rate scheduling\n",
    "- Putting it all together in a complete training loop\n",
    "- Monitoring and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, OneCycleLR\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Learning Rate Scheduling\n",
    "\n",
    "Let's implement different learning rate schedules commonly used for training language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_warmup_lr_scheduler(optimizer, warmup_steps, total_steps):\n",
    "    \"\"\"Linear warmup followed by linear decay.\"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        return max(\n",
    "            0.0, float(total_steps - current_step) / float(max(1, total_steps - warmup_steps))\n",
    "        )\n",
    "    \n",
    "    return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "def get_cosine_warmup_lr_scheduler(optimizer, warmup_steps, total_steps):\n",
    "    \"\"\"Linear warmup followed by cosine decay.\"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    \n",
    "    return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "def plot_lr_schedule(scheduler, steps):\n",
    "    \"\"\"Plot learning rate schedule.\"\"\"\n",
    "    lrs = []\n",
    "    for i in range(steps):\n",
    "        scheduler.step()\n",
    "        lrs.append(scheduler.get_last_lr()[0])\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(lrs)\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy model and optimizer\n",
    "dummy_model = nn.Linear(10, 10)\n",
    "optimizer = torch.optim.AdamW(dummy_model.parameters(), lr=0.001)\n",
    "\n",
    "# Example parameters\n",
    "total_steps = 1000\n",
    "warmup_steps = 100  # 10% warmup\n",
    "\n",
    "# Create schedulers\n",
    "linear_scheduler = get_linear_warmup_lr_scheduler(optimizer, warmup_steps, total_steps)\n",
    "plot_lr_schedule(linear_scheduler, total_steps)\n",
    "\n",
    "# Reset optimizer for next scheduler\n",
    "optimizer = torch.optim.AdamW(dummy_model.parameters(), lr=0.001)\n",
    "cosine_scheduler = get_cosine_warmup_lr_scheduler(optimizer, warmup_steps, total_steps)\n",
    "plot_lr_schedule(cosine_scheduler, total_steps)\n",
    "\n",
    "# One-cycle policy\n",
    "optimizer = torch.optim.AdamW(dummy_model.parameters(), lr=0.001)\n",
    "onecycle_scheduler = OneCycleLR(\n",
    "    optimizer, max_lr=0.01, total_steps=total_steps,\n",
    "    pct_start=0.3, anneal_strategy='cos'\n",
    ")\n",
    "plot_lr_schedule(onecycle_scheduler, total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Complete Training Loop\n",
    "\n",
    "Now let's put everything together into a complete training loop with all the components we've discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's define a simple model and dataset if running this notebook separately\n",
    "# If you're running this after Part 1, you can skip these definitions\n",
    "\n",
    "try:\n",
    "    # Check if model and dataset are defined from Part 1\n",
    "    SimpleLanguageModel\n",
    "    train_dataloader\n",
    "    val_dataloader\n",
    "    vocab_size\n",
    "except NameError:\n",
    "    print(\"Defining model and dataset for standalone execution...\")\n",
    "    \n",
    "    # Define a simple model\n",
    "    class SimpleLanguageModel(nn.Module):\n",
    "        def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=4):\n",
    "            super().__init__()\n",
    "            self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "            encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, batch_first=True)\n",
    "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "            self.output = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # Create causal mask\n",
    "            mask = torch.triu(torch.ones(x.size(1), x.size(1)) * float('-inf'), diagonal=1)\n",
    "            mask = mask.to(x.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            x = self.embedding(x)\n",
    "            x = self.transformer(x, mask=mask)\n",
    "            x = self.output(x)\n",
    "            return x\n",
    "    \n",
    "    # Create synthetic dataset\n",
    "    vocab_size = 1000\n",
    "    seq_len = 64\n",
    "    dataset_size = 1000\n",
    "    batch_size = 32\n",
    "    \n",
    "    # Simple dataset that returns random data\n",
    "    class SyntheticDataset(Dataset):\n",
    "        def __init__(self, size):\n",
    "            self.size = size\n",
    "        \n",
    "        def __len__(self):\n",
    "            return self.size\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            x = torch.randint(0, vocab_size, (seq_len,))\n",
    "            y = torch.randint(0, vocab_size, (seq_len,))\n",
    "            return x, y\n",
    "    \n",
    "    # Create dataloaders\n",
    "    dataset = SyntheticDataset(dataset_size)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingLogger:\n",
    "    \"\"\"Logger for tracking and visualizing training progress.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.learning_rates = []\n",
    "        self.grad_norms = []\n",
    "        self.times = []\n",
    "    \n",
    "    def log_step(self, train_loss, val_loss=None, lr=None, grad_norm=None, time_taken=None):\n",
    "        self.train_losses.append(train_loss)\n",
    "        if val_loss is not None:\n",
    "            self.val_losses.append(val_loss)\n",
    "        if lr is not None:\n",
    "            self.learning_rates.append(lr)\n",
    "        if grad_norm is not None:\n",
    "            self.grad_norms.append(grad_norm)\n",
    "        if time_taken is not None:\n",
    "            self.times.append(time_taken)\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.train_losses, label='Train Loss')\n",
    "        if self.val_losses:\n",
    "            plt.plot(self.val_losses, label='Val Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Steps' if len(self.val_losses) == 0 else 'Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_learning_rates(self):\n",
    "        if not self.learning_rates:\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.learning_rates)\n",
    "        plt.title('Learning Rate Schedule')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_grad_norms(self):\n",
    "        if not self.grad_norms:\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.grad_norms)\n",
    "        plt.title('Gradient Norm')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Norm')\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_training_loop(model, train_dataloader, val_dataloader, epochs=5, \n",
    "                           lr=0.001, warmup_ratio=0.1, max_grad_norm=1.0, \n",
    "                           accumulation_steps=4, use_amp=True):\n",
    "    \"\"\"Complete training loop with all components.\"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    \n",
    "    # Create gradient scaler for AMP\n",
    "    scaler = GradScaler() if use_amp else None\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Calculate steps\n",
    "    steps_per_epoch = len(train_dataloader) // accumulation_steps\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "    warmup_steps = int(total_steps * warmup_ratio)\n",
    "    \n",
    "    # Create scheduler\n",
    "    scheduler = get_cosine_warmup_lr_scheduler(optimizer, warmup_steps, total_steps)\n",
    "    \n",
    "    # Create logger\n",
    "    logger = TrainingLogger()\n",
    "    \n",
    "    # Training loop\n",
    "    global_step = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for batch_idx, (x, y) in enumerate(train_dataloader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Forward pass with autocast if using AMP\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    output = model(x)\n",
    "                    loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
    "                    # Normalize loss for gradient accumulation\n",
    "                    loss = loss / accumulation_steps\n",
    "            else:\n",
    "                output = model(x)\n",
    "                loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
    "                # Normalize loss for gradient accumulation\n",
    "                loss = loss / accumulation_steps\n",
    "            \n",
    "            # Backward pass with gradient scaling if using AMP\n",
    "            if use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            \n",
    "            # Update weights after accumulation steps\n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_dataloader):\n",
    "                # Clip gradients\n",
    "                if use_amp:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                \n",
    "                # Calculate gradient norm for logging\n",
    "                grad_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), 2) \n",
    "                                                  for p in model.parameters() if p.grad is not None]), 2)\n",
    "                \n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                \n",
    "                # Step optimizer and update scaler if using AMP\n",
    "                if use_amp:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # Step scheduler\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Log learning rate and gradient norm\n",
    "                current_lr = scheduler.get_last_lr()[0]\n",
    "                logger.log_step(loss.item() * accumulation_steps, lr=current_lr, grad_norm=grad_norm.item())\n",
    "                \n",
    "                # Reset gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Increment global step\n",
    "                global_step += 1\n",
    "            \n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_dataloader)}, \"\n",
    "                      f\"Loss: {loss.item() * accumulation_steps:.4f}, \"\n",
    "                      f\"LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = evaluate(model, val_dataloader, criterion, use_amp)\n",
    "        \n",
    "        # Print epoch stats\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Time: {elapsed:.2f}s\")\n",
    "        \n",
    "        # Log epoch results\n",
    "        logger.log_step(avg_loss, val_loss=val_loss, time_taken=elapsed)\n",
    "    \n",
    "    # Plot training progress\n",
    "    logger.plot_losses()\n",
    "    logger.plot_learning_rates()\n",
    "    logger.plot_grad_norms()\n",
    "    \n",
    "    return model, logger\n",
    "\n",
    "def evaluate(model, dataloader, criterion, use_amp=True):\n",
    "    \"\"\"Evaluate model on dataloader.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Forward pass with autocast if using AMP\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    output = model(x)\n",
    "                    loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
    "            else:\n",
    "                output = model(x)\n",
    "                loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the Model\n",
    "\n",
    "Let's train our model using the complete training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = SimpleLanguageModel(vocab_size)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Train model (uncomment to run - may take some time)\n",
    "# trained_model, logger = complete_training_loop(\n",
    "#     model, train_dataloader, val_dataloader,\n",
    "#     epochs=3,\n",
    "#     lr=0.0005,\n",
    "#     warmup_ratio=0.1,\n",
    "#     max_grad_norm=1.0,\n",
    "#     accumulation_steps=2,\n",
    "#     use_amp=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Checkpointing and Resuming Training\n",
    "\n",
    "Let's implement checkpointing and resuming training, which is essential for long training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, scaler, epoch, global_step, loss, path):\n",
    "    \"\"\"Save training checkpoint.\"\"\"\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'scaler_state_dict': scaler.state_dict() if scaler else None,\n",
    "        'epoch': epoch,\n",
    "        'global_step': global_step,\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint saved to {path}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, scaler, path):\n",
    "    \"\"\"Load training checkpoint.\"\"\"\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if scheduler and checkpoint['scheduler_state_dict']:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    if scaler and checkpoint['scaler_state_dict']:\n",
    "        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    global_step = checkpoint['global_step']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f\"Checkpoint loaded from {path}, resuming from epoch {epoch+1}, step {global_step}\")\n",
    "    return epoch, global_step, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary and Key Insights\n",
    "\n",
    "In this notebook, we've explored and implemented key components of an efficient and stable training loop for language models:\n",
    "\n",
    "1. **Learning Rate Scheduling**:\n",
    "   - Linear warmup + decay\n",
    "   - Cosine warmup + decay\n",
    "   - One-cycle policy\n",
    "\n",
    "2. **Complete Training Loop**:\n",
    "   - Mixed precision training\n",
    "   - Gradient clipping and accumulation\n",
    "   - AdamW optimizer with weight decay\n",
    "   - Learning rate scheduling\n",
    "   - Monitoring and logging\n",
    "\n",
    "3. **Checkpointing**:\n",
    "   - Saving model, optimizer, scheduler, and scaler states\n",
    "   - Resuming training from checkpoints\n",
    "\n",
    "These techniques are essential for training large language models efficiently and stably, especially when working with limited computational resources or when training needs to be resumed after interruptions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
