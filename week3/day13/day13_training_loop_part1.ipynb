{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Day 13: Training Loop Details - Part 1\n",
    "\n",
    "## Overview\n",
    "In this notebook, we'll implement and explore key components of an efficient and stable training loop for language models, focusing on:\n",
    "- Mixed precision training (AMP)\n",
    "- Gradient clipping and accumulation\n",
    "- AdamW optimizer configuration\n",
    "- Learning rate scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Language Model\n",
    "\n",
    "Let's start by defining a simple language model to use for our training loop examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLanguageModel(nn.Module):\n",
    "    \"\"\"A simple transformer-based language model for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=4, dim_feedforward=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # Transformer decoder layer\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Transformer decoder\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights with Xavier uniform.\"\"\"\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output.bias.data.zero_()\n",
    "        self.output.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(self, src, src_mask=None):\n",
    "        # Embed tokens\n",
    "        src = self.embedding(src) * math.sqrt(self.embedding.embedding_dim)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        # Create causal mask if not provided\n",
    "        if src_mask is None:\n",
    "            src_mask = generate_square_subsequent_mask(src.size(1)).to(src.device)\n",
    "        \n",
    "        # Pass through transformer decoder\n",
    "        # Using None for memory since we're using decoder-only architecture\n",
    "        output = self.transformer_decoder(src, None, tgt_mask=src_mask)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        output = self.output(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for sequences.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sine to even indices, cosine to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension and register as buffer (not a parameter)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to input\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"Generate a square mask for the sequence.\"\"\"\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Synthetic Dataset\n",
    "\n",
    "Let's create a synthetic dataset for our training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataset(Dataset):\n",
    "    \"\"\"Synthetic dataset for language modeling.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=1000, seq_len=64, size=10000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.size = size\n",
    "        \n",
    "        # Generate synthetic data\n",
    "        # We'll create sequences with some patterns to make them learnable\n",
    "        self.data = []\n",
    "        for _ in range(size):\n",
    "            # Create a sequence with some patterns\n",
    "            seq = torch.randint(0, vocab_size, (seq_len + 1,))\n",
    "            \n",
    "            # Add some patterns (e.g., repeated tokens, sequential tokens)\n",
    "            pattern_start = torch.randint(0, seq_len - 10, (1,)).item()\n",
    "            pattern_type = torch.randint(0, 3, (1,)).item()\n",
    "            \n",
    "            if pattern_type == 0:  # Repeated token\n",
    "                token = torch.randint(0, vocab_size, (1,)).item()\n",
    "                seq[pattern_start:pattern_start+5] = token\n",
    "            elif pattern_type == 1:  # Sequential tokens\n",
    "                start_token = torch.randint(0, vocab_size-5, (1,)).item()\n",
    "                for i in range(5):\n",
    "                    seq[pattern_start+i] = start_token + i\n",
    "            else:  # Alternating tokens\n",
    "                token1 = torch.randint(0, vocab_size, (1,)).item()\n",
    "                token2 = torch.randint(0, vocab_size, (1,)).item()\n",
    "                for i in range(6):\n",
    "                    seq[pattern_start+i] = token1 if i % 2 == 0 else token2\n",
    "            \n",
    "            self.data.append(seq)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data[idx]\n",
    "        \n",
    "        # Input: all tokens except the last one\n",
    "        # Target: all tokens except the first one\n",
    "        x = seq[:-1]\n",
    "        y = seq[1:]\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "# Create dataset and split into train/val\n",
    "vocab_size = 1000\n",
    "seq_len = 64\n",
    "dataset_size = 5000\n",
    "\n",
    "dataset = SyntheticDataset(vocab_size, seq_len, dataset_size)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Check a sample batch\n",
    "x_sample, y_sample = next(iter(train_dataloader))\n",
    "print(f\"Input shape: {x_sample.shape}\")\n",
    "print(f\"Target shape: {y_sample.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mixed Precision Training (AMP)\n",
    "\n",
    "Let's implement mixed precision training using PyTorch's Automatic Mixed Precision (AMP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_amp(model, train_dataloader, val_dataloader, epochs=5, lr=0.001):\n",
    "    \"\"\"Train a model using mixed precision.\"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Create gradient scaler for AMP\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, (x, y) in enumerate(train_dataloader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Forward pass with autocast (mixed precision)\n",
    "            with autocast():\n",
    "                output = model(x)\n",
    "                loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
    "            \n",
    "            # Backward pass with gradient scaling\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Update weights with gradient scaling\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_dataloader)}, \"\n",
    "                      f\"Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = evaluate(model, val_dataloader, criterion)\n",
    "        \n",
    "        # Print epoch stats\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Time: {elapsed:.2f}s\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    \"\"\"Evaluate model on dataloader.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Forward pass with autocast\n",
    "            with autocast():\n",
    "                output = model(x)\n",
    "                loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Clipping and Accumulation\n",
    "\n",
    "Now let's implement gradient clipping and accumulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_grad_clip_accum(model, train_dataloader, val_dataloader, epochs=5, lr=0.001, \n",
    "                              max_grad_norm=1.0, accumulation_steps=4):\n",
    "    \"\"\"Train with gradient clipping and accumulation.\"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Create gradient scaler for AMP\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Effective batch size\n",
    "    effective_batch_size = batch_size * accumulation_steps\n",
    "    print(f\"Effective batch size: {effective_batch_size}\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for batch_idx, (x, y) in enumerate(train_dataloader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Forward pass with autocast\n",
    "            with autocast():\n",
    "                output = model(x)\n",
    "                loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
    "                # Normalize loss for gradient accumulation\n",
    "                loss = loss / accumulation_steps\n",
    "            \n",
    "            # Backward pass with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Update weights after accumulation steps\n",
    "            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_dataloader):\n",
    "                # Clip gradients\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                \n",
    "                # Step optimizer and update scaler\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_dataloader)}, \"\n",
    "                      f\"Loss: {loss.item() * accumulation_steps:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = evaluate(model, val_dataloader, criterion)\n",
    "        \n",
    "        # Print epoch stats\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Time: {elapsed:.2f}s\")\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
