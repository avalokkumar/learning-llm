{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Day 12: Tokenization at Scale & Sequence Preparation\n",
    "\n",
    "## Overview\n",
    "In this notebook, we'll explore tokenization at scale and efficient sequence preparation for language model training. We'll focus on:\n",
    "- Implementing efficient tokenization with HuggingFace Tokenizers\n",
    "- Comparing different tokenization algorithms\n",
    "- Implementing sequence packing and masking for efficient training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import Counter\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "from tokenizers.models import BPE, WordPiece, Unigram\n",
    "from tokenizers.trainers import BpeTrainer, WordPieceTrainer, UnigramTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace, ByteLevel\n",
    "\n",
    "# Set random seeds and device\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation and Tokenizer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare data\n",
    "text_urls = {\n",
    "    \"fiction\": \"https://www.gutenberg.org/files/1342/1342-0.txt\",\n",
    "    \"science\": \"https://www.gutenberg.org/files/2009/2009-0.txt\"\n",
    "}\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "all_text_path = \"data/all_texts.txt\"\n",
    "with open(all_text_path, \"w\", encoding=\"utf-8\") as f_all:\n",
    "    for name, url in text_urls.items():\n",
    "        text = requests.get(url).text\n",
    "        f_all.write(text)\n",
    "\n",
    "# Train a BPE tokenizer\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=False)\n",
    "trainer = BpeTrainer(vocab_size=8000, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer.train([all_text_path], trainer)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "os.makedirs(\"tokenizers\", exist_ok=True)\n",
    "tokenizer.save(\"tokenizers/bpe_tokenizer.json\")\n",
    "print('BPE tokenizer trained and saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sequence Packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_packed_sequences(texts, tokenizer, max_seq_len=128):\n",
    "    token_ids = [tokenizer.encode(text).ids for text in texts]\n",
    "    pad_id = tokenizer.token_to_id(\"[PAD]\") or 0\n",
    "    \n",
    "    packed_sequences, sequence_mappings = [], []\n",
    "    current_sequence, current_mapping, current_length = [], [], 0\n",
    "    \n",
    "    token_ids_with_idx = sorted([(i, ids) for i, ids in enumerate(token_ids)], key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    for orig_idx, ids in token_ids_with_idx:\n",
    "        if current_length + len(ids) > max_seq_len:\n",
    "            padding = [pad_id] * (max_seq_len - current_length)\n",
    "            packed_sequences.append(current_sequence + padding)\n",
    "            sequence_mappings.append(current_mapping + [-1] * len(padding))\n",
    "            current_sequence, current_mapping, current_length = [], [], 0\n",
    "        \n",
    "        current_sequence.extend(ids)\n",
    "        current_mapping.extend([orig_idx] * len(ids))\n",
    "        current_length += len(ids)\n",
    "        \n",
    "    if current_length > 0:\n",
    "        padding = [pad_id] * (max_seq_len - current_length)\n",
    "        packed_sequences.append(current_sequence + padding)\n",
    "        sequence_mappings.append(current_mapping + [-1] * len(padding))\n",
    "        \n",
    "    return packed_sequences, sequence_mappings\n",
    "\n",
    "with open(all_text_path, 'r', encoding='utf-8') as f:\n",
    "    text_content = f.read()\n",
    "sentences = re.split(r'(?<=[.!?])\\s+', text_content)\n",
    "sample_sentences = [s.strip() for s in sentences if s.strip()][:50]\n",
    "\n",
    "packed_sequences, sequence_mappings = create_packed_sequences(sample_sentences, tokenizer)\n",
    "print(f'Created {len(packed_sequences)} packed sequences.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attention Masking for Packed Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_packed_attention_mask(sequence_mapping, seq_len):\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    \n",
    "    # Create a broadcastable mapping for comparison\n",
    "    mapping_tensor = torch.tensor(sequence_mapping, dtype=torch.long).unsqueeze(1)\n",
    "    # Prevent attention between different sequences\n",
    "    inter_sequence_mask = mapping_tensor != mapping_tensor.T\n",
    "    # Prevent attention to/from padding\n",
    "    padding_mask = (mapping_tensor == -1) | (mapping_tensor.T == -1)\n",
    "    \n",
    "    combined_mask = mask | inter_sequence_mask | padding_mask\n",
    "    return combined_mask.float().masked_fill(combined_mask, float('-inf')).masked_fill(~combined_mask, 0.0)\n",
    "\n",
    "# Visualize a mask\n",
    "if packed_sequences:\n",
    "    sample_mask = create_packed_attention_mask(sequence_mappings[0], len(sequence_mappings[0]))\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(sample_mask.numpy(), cmap='Blues_r')\n",
    "    plt.title('Packed Sequence Attention Mask')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Efficient Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackedSequenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, packed_sequences, sequence_mappings):\n",
    "        self.packed_sequences = packed_sequences\n",
    "        self.sequence_mappings = sequence_mappings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.packed_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = torch.tensor(self.packed_sequences[idx], dtype=torch.long)\n",
    "        mapping = self.sequence_mappings[idx]\n",
    "        \n",
    "        input_ids = sequence[:-1]\n",
    "        targets = sequence[1:]\n",
    "        attention_mask = create_packed_attention_mask(mapping[:-1], len(input_ids))\n",
    "        \n",
    "        return {\"input_ids\": input_ids, \"targets\": targets, \"attention_mask\": attention_mask}\n",
    "\n",
    "dataset = PackedSequenceDataset(packed_sequences, sequence_mappings)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Test the dataloader\n",
    "if len(dataloader) > 0:\n",
    "    batch = next(iter(dataloader))\n",
    "    print(f\"Input shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"Attention mask shape: {batch['attention_mask'].shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
