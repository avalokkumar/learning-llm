{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Day 12: Sequence Packing & Masking for Causal LM\n",
    "\n",
    "## Overview\n",
    "In this notebook, we'll explore efficient sequence packing and masking techniques for causal language modeling, focusing on:\n",
    "- Implementing sequence packing algorithms\n",
    "- Creating causal attention masks\n",
    "- Building an efficient data pipeline for transformer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Tokenizer and Sample Data\n",
    "\n",
    "Let's load the BPE tokenizer we created in Part 1 and prepare some sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BPE tokenizer\n",
    "try:\n",
    "    tokenizer = Tokenizer.from_file(\"tokenizers/bpe_tokenizer.json\")\n",
    "    print(\"Loaded BPE tokenizer successfully\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Tokenizer file not found. Please run Part 1 first.\")\n",
    "    # Create a simple fallback tokenizer for demonstration\n",
    "    from tokenizers import Tokenizer\n",
    "    from tokenizers.models import BPE\n",
    "    from tokenizers.trainers import BpeTrainer\n",
    "    from tokenizers.pre_tokenizers import Whitespace\n",
    "    \n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    \n",
    "    # Train on a small sample\n",
    "    sample_texts = [\n",
    "        \"This is a sample text for tokenizer training.\",\n",
    "        \"We need a tokenizer to demonstrate sequence packing.\",\n",
    "        \"Transformers use attention mechanisms for language modeling.\"\n",
    "    ]\n",
    "    \n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=1000,\n",
    "        special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "    )\n",
    "    \n",
    "    # Save sample texts to file\n",
    "    import os\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    with open(\"data/sample.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(sample_texts))\n",
    "    \n",
    "    tokenizer.train([\"data/sample.txt\"], trainer)\n",
    "    print(\"Created fallback tokenizer\")\n",
    "\n",
    "# Load or create sample data\n",
    "try:\n",
    "    with open(\"data/all_texts.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        all_text = f.read()\n",
    "    print(f\"Loaded sample data: {len(all_text)} characters\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Sample data not found. Creating minimal sample.\")\n",
    "    all_text = \"\"\"\n",
    "    Sequence packing is an important technique for efficient transformer training.\n",
    "    It allows us to maximize GPU utilization by combining multiple sequences into a single batch.\n",
    "    Causal masking ensures that the model can only attend to previous tokens in autoregressive language modeling.\n",
    "    Efficient data pipelines are crucial for training large language models at scale.\n",
    "    \"\"\"\n",
    "    print(f\"Created minimal sample: {len(all_text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing Sequence Packing\n",
    "\n",
    "Let's implement a sequence packing algorithm to efficiently combine multiple sequences into a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_packed_sequences(texts, tokenizer, max_seq_len=512):\n",
    "    \"\"\"Pack multiple sequences into fixed-length chunks.\"\"\"\n",
    "    # Tokenize all texts\n",
    "    tokenized_texts = [tokenizer.encode(text) for text in texts]\n",
    "    token_ids = [encoding.ids for encoding in tokenized_texts]\n",
    "    \n",
    "    # Get pad token ID\n",
    "    pad_id = tokenizer.token_to_id(\"[PAD]\")\n",
    "    if pad_id is None:\n",
    "        pad_id = 0  # Fallback\n",
    "    \n",
    "    # Initialize packed sequences\n",
    "    packed_sequences = []\n",
    "    sequence_mappings = []  # To track which original sequence each token belongs to\n",
    "    current_sequence = []\n",
    "    current_mapping = []\n",
    "    current_length = 0\n",
    "    \n",
    "    # Sort sequences by length (descending) for more efficient packing\n",
    "    token_ids_with_idx = [(i, ids) for i, ids in enumerate(token_ids)]\n",
    "    token_ids_with_idx.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    # Pack sequences\n",
    "    for orig_idx, ids in token_ids_with_idx:\n",
    "        # If adding this sequence would exceed max_seq_len, start a new packed sequence\n",
    "        if current_length + len(ids) > max_seq_len:\n",
    "            # Pad current sequence to max_seq_len\n",
    "            padding_needed = max_seq_len - current_length\n",
    "            current_sequence.extend([pad_id] * padding_needed)\n",
    "            current_mapping.extend([-1] * padding_needed)  # -1 indicates padding\n",
    "            \n",
    "            # Add to packed sequences\n",
    "            packed_sequences.append(current_sequence)\n",
    "            sequence_mappings.append(current_mapping)\n",
    "            \n",
    "            # Start new sequence\n",
    "            current_sequence = []\n",
    "            current_mapping = []\n",
    "            current_length = 0\n",
    "        \n",
    "        # Add current sequence\n",
    "        current_sequence.extend(ids)\n",
    "        current_mapping.extend([orig_idx] * len(ids))\n",
    "        current_length += len(ids)\n",
    "    \n",
    "    # Add the last sequence if not empty\n",
    "    if current_length > 0:\n",
    "        # Pad to max_seq_len\n",
    "        padding_needed = max_seq_len - current_length\n",
    "        current_sequence.extend([pad_id] * padding_needed)\n",
    "        current_mapping.extend([-1] * padding_needed)\n",
    "        \n",
    "        packed_sequences.append(current_sequence)\n",
    "        sequence_mappings.append(current_mapping)\n",
    "    \n",
    "    return packed_sequences, sequence_mappings\n",
    "\n",
    "def visualize_packed_sequences(packed_sequences, sequence_mappings):\n",
    "    \"\"\"Visualize packed sequences.\"\"\"\n",
    "    num_sequences = len(packed_sequences)\n",
    "    seq_len = len(packed_sequences[0])\n",
    "    \n",
    "    plt.figure(figsize=(12, num_sequences * 0.5))\n",
    "    \n",
    "    for i, mapping in enumerate(sequence_mappings):\n",
    "        # Create a row for each packed sequence\n",
    "        row = np.array(mapping)\n",
    "        row = row.reshape(1, -1)\n",
    "        \n",
    "        # Plot as heatmap\n",
    "        ax = plt.subplot(num_sequences, 1, i+1)\n",
    "        sns.heatmap(row, cmap='viridis', cbar=False, xticklabels=50, yticklabels=False)\n",
    "        ax.set_title(f\"Packed Sequence {i+1}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into sentences for packing\n",
    "import re\n",
    "sentences = re.split(r'(?<=[.!?])\\s+', all_text)\n",
    "sentences = [s.strip() for s in sentences if len(s.strip()) > 0]\n",
    "\n",
    "# Take a subset for demonstration\n",
    "sample_sentences = sentences[:50]\n",
    "print(f\"Number of sentences: {len(sample_sentences)}\")\n",
    "\n",
    "# Create packed sequences\n",
    "max_seq_len = 128\n",
    "packed_sequences, sequence_mappings = create_packed_sequences(\n",
    "    sample_sentences, tokenizer, max_seq_len\n",
    ")\n",
    "\n",
    "print(f\"Number of packed sequences: {len(packed_sequences)}\")\n",
    "print(f\"Sequence length: {len(packed_sequences[0])}\")\n",
    "\n",
    "# Calculate packing efficiency\n",
    "total_tokens = sum(len(tokenizer.encode(s).ids) for s in sample_sentences)\n",
    "packed_tokens = len(packed_sequences) * max_seq_len\n",
    "packing_efficiency = total_tokens / packed_tokens * 100\n",
    "\n",
    "print(f\"Total tokens in original sentences: {total_tokens}\")\n",
    "print(f\"Total tokens in packed sequences: {packed_tokens}\")\n",
    "print(f\"Packing efficiency: {packing_efficiency:.2f}%\")\n",
    "\n",
    "# Visualize packed sequences\n",
    "visualize_packed_sequences(packed_sequences, sequence_mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Attention Masks for Packed Sequences\n",
    "\n",
    "Now let's create attention masks for our packed sequences, ensuring proper causal masking and preventing attention across different packed examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"Create a causal mask for autoregressive language modeling.\"\"\"\n",
    "    # Create a mask where each position can attend to itself and previous positions\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    # Convert to float and replace True with -inf, False with 0.0\n",
    "    mask = mask.float().masked_fill(mask == 1, float('-inf')).masked_fill(mask == 0, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_packed_attention_mask(sequence_mapping, seq_len):\n",
    "    \"\"\"Create attention mask for packed sequences.\"\"\"\n",
    "    # Start with causal mask\n",
    "    mask = create_causal_mask(seq_len)\n",
    "    \n",
    "    # Modify mask to prevent attention across different sequences\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            # If tokens belong to different sequences or one is padding, mask the attention\n",
    "            if sequence_mapping[i] != sequence_mapping[j] or sequence_mapping[i] == -1 or sequence_mapping[j] == -1:\n",
    "                mask[i, j] = float('-inf')\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def visualize_attention_mask(mask, title=\"Attention Mask\"):\n",
    "    \"\"\"Visualize an attention mask.\"\"\"\n",
    "    # Convert -inf to a small value for visualization\n",
    "    vis_mask = mask.clone()\n",
    "    vis_mask[vis_mask == float('-inf')] = -10\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(vis_mask, cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a standard causal mask\n",
    "seq_len = 64  # Smaller for visualization\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "visualize_attention_mask(causal_mask, \"Standard Causal Mask\")\n",
    "\n",
    "# Create a packed attention mask\n",
    "# Simulate a sequence mapping with 3 sequences\n",
    "sequence_mapping = [0] * 20 + [1] * 25 + [2] * 15 + [-1] * 4  # Three sequences + padding\n",
    "packed_mask = create_packed_attention_mask(sequence_mapping, seq_len)\n",
    "visualize_attention_mask(packed_mask, \"Packed Sequence Attention Mask\")\n",
    "\n",
    "# Compare with a real packed sequence\n",
    "if len(packed_sequences) > 0:\n",
    "    real_mapping = sequence_mappings[0][:seq_len]  # Take first seq_len tokens\n",
    "    real_packed_mask = create_packed_attention_mask(real_mapping, seq_len)\n",
    "    visualize_attention_mask(real_packed_mask, \"Real Packed Sequence Attention Mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building an Efficient Data Pipeline\n",
    "\n",
    "Let's create an efficient data pipeline for training language models with packed sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackedSequenceDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for packed sequences.\"\"\"\n",
    "    \n",
    "    def __init__(self, packed_sequences, sequence_mappings):\n",
    "        self.packed_sequences = packed_sequences\n",
    "        self.sequence_mappings = sequence_mappings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.packed_sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get packed sequence and mapping\n",
    "        sequence = self.packed_sequences[idx]\n",
    "        mapping = self.sequence_mappings[idx]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        sequence_tensor = torch.tensor(sequence, dtype=torch.long)\n",
    "        mapping_tensor = torch.tensor(mapping, dtype=torch.long)\n",
    "        \n",
    "        # Create input and target tensors for language modeling\n",
    "        # Input: all tokens except the last one\n",
    "        # Target: all tokens except the first one\n",
    "        input_tensor = sequence_tensor[:-1]\n",
    "        target_tensor = sequence_tensor[1:]\n",
    "        \n",
    "        # Create attention mask\n",
    "        seq_len = len(input_tensor)\n",
    "        mask = create_packed_attention_mask(mapping[:-1], seq_len)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_tensor,\n",
    "            'targets': target_tensor,\n",
    "            'attention_mask': mask,\n",
    "            'sequence_mapping': mapping_tensor[:-1]\n",
    "        }\n",
    "\n",
    "def collate_packed_sequences(batch):\n",
    "    \"\"\"Collate function for packed sequences.\"\"\"\n",
    "    # Stack all tensors\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    targets = torch.stack([item['targets'] for item in batch])\n",
    "    attention_masks = torch.stack([item['attention_mask'] for item in batch])\n",
    "    sequence_mappings = torch.stack([item['sequence_mapping'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'targets': targets,\n",
    "        'attention_mask': attention_masks,\n",
    "        'sequence_mapping': sequence_mappings\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "dataset = PackedSequenceDataset(packed_sequences, sequence_mappings)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_packed_sequences\n",
    ")\n",
    "\n",
    "# Test the dataloader\n",
    "batch = next(iter(dataloader))\n",
    "print(f\"Input shape: {batch['input_ids'].shape}\")\n",
    "print(f\"Target shape: {batch['targets'].shape}\")\n",
    "print(f\"Attention mask shape: {batch['attention_mask'].shape}\")\n",
    "print(f\"Sequence mapping shape: {batch['sequence_mapping'].shape}\")\n",
    "\n",
    "# Visualize a sample attention mask from the batch\n",
    "sample_mask = batch['attention_mask'][0]\n",
    "visualize_attention_mask(sample_mask, \"Sample Batch Attention Mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Measuring Efficiency Gains\n",
    "\n",
    "Let's measure the efficiency gains from using packed sequences compared to standard padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_standard_sequences(texts, tokenizer, max_seq_len=512):\n",
    "    \"\"\"Create standard padded sequences without packing.\"\"\"\n",
    "    # Tokenize all texts\n",
    "    tokenized_texts = [tokenizer.encode(text) for text in texts]\n",
    "    token_ids = [encoding.ids for encoding in tokenized_texts]\n",
    "    \n",
    "    # Get pad token ID\n",
    "    pad_id = tokenizer.token_to_id(\"[PAD]\")\n",
    "    if pad_id is None:\n",
    "        pad_id = 0  # Fallback\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_sequences = []\n",
    "    for ids in token_ids:\n",
    "        # Truncate if too long\n",
    "        if len(ids) > max_seq_len:\n",
    "            ids = ids[:max_seq_len]\n",
    "        \n",
    "        # Pad if too short\n",
    "        padding_needed = max_seq_len - len(ids)\n",
    "        padded_ids = ids + [pad_id] * padding_needed\n",
    "        padded_sequences.append(padded_ids)\n",
    "    \n",
    "    return padded_sequences\n",
    "\n",
    "# Compare standard padding vs. sequence packing\n",
    "standard_sequences = create_standard_sequences(sample_sentences, tokenizer, max_seq_len)\n",
    "\n",
    "# Calculate efficiency metrics\n",
    "total_tokens = sum(len(tokenizer.encode(s).ids) for s in sample_sentences)\n",
    "standard_tokens = len(standard_sequences) * max_seq_len\n",
    "packed_tokens = len(packed_sequences) * max_seq_len\n",
    "\n",
    "standard_efficiency = total_tokens / standard_tokens * 100\n",
    "packed_efficiency = total_tokens / packed_tokens * 100\n",
    "efficiency_gain = packed_efficiency / standard_efficiency\n",
    "\n",
    "print(f\"Total tokens in original sentences: {total_tokens}\")\n",
    "print(f\"Total tokens with standard padding: {standard_tokens}\")\n",
    "print(f\"Total tokens with sequence packing: {packed_tokens}\")\n",
    "print(f\"Standard padding efficiency: {standard_efficiency:.2f}%\")\n",
    "print(f\"Sequence packing efficiency: {packed_efficiency:.2f}%\")\n",
    "print(f\"Efficiency gain: {efficiency_gain:.2f}x\")\n",
    "\n",
    "# Visualize comparison\n",
    "labels = ['Original', 'Standard Padding', 'Sequence Packing']\n",
    "values = [total_tokens, standard_tokens, packed_tokens]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(labels, values)\n",
    "plt.title('Token Count Comparison')\n",
    "plt.ylabel('Number of Tokens')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add efficiency labels\n",
    "plt.text(1, values[1] * 1.05, f\"{standard_efficiency:.1f}% Efficient\", ha='center')\n",
    "plt.text(2, values[2] * 1.05, f\"{packed_efficiency:.1f}% Efficient\", ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Key Insights\n",
    "\n",
    "In this notebook, we've explored sequence packing and masking techniques for efficient language model training:\n",
    "\n",
    "1. **Sequence Packing**:\n",
    "   - Efficiently combining multiple sequences into fixed-length batches\n",
    "   - Tracking sequence boundaries with mappings\n",
    "   - Significantly improving GPU utilization\n",
    "\n",
    "2. **Attention Masking**:\n",
    "   - Creating causal masks for autoregressive language modeling\n",
    "   - Preventing attention across different packed sequences\n",
    "   - Handling padding tokens appropriately\n",
    "\n",
    "3. **Efficient Data Pipeline**:\n",
    "   - Building a PyTorch dataset for packed sequences\n",
    "   - Creating proper collate functions\n",
    "   - Generating input-target pairs for language modeling\n",
    "\n",
    "4. **Efficiency Analysis**:\n",
    "   - Measuring token utilization with different approaches\n",
    "   - Quantifying the benefits of sequence packing\n",
    "   - Visualizing efficiency gains\n",
    "\n",
    "These techniques are essential for training large language models efficiently, especially when working with limited computational resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
