{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Day 12: Tokenization at Scale & Sequence Preparation - Part 1\n",
    "\n",
    "## Overview\n",
    "In this notebook, we'll explore tokenization at scale and efficient sequence preparation for language model training. We'll focus on:\n",
    "- Implementing efficient tokenization with HuggingFace Tokenizers\n",
    "- Comparing different tokenization algorithms\n",
    "- Analyzing tokenization efficiency and quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import Counter\n",
    "\n",
    "# For tokenization\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "from tokenizers.models import BPE, WordPiece, Unigram\n",
    "from tokenizers.trainers import BpeTrainer, WordPieceTrainer, UnigramTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace, ByteLevel\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Downloading Sample Data\n",
    "\n",
    "Let's download some sample text data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_text(url: str) -> str:\n",
    "    \"\"\"Download text from a URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.encoding = 'utf-8'  # Ensure proper encoding\n",
    "    return response.text\n",
    "\n",
    "def clean_gutenberg_text(text: str) -> str:\n",
    "    \"\"\"Clean Project Gutenberg text by removing headers and footers.\"\"\"\n",
    "    # Find the start of the actual content (after the header)\n",
    "    start_markers = [\n",
    "        \"*** START OF THIS PROJECT GUTENBERG\",\n",
    "        \"***START OF THE PROJECT GUTENBERG\",\n",
    "        \"*** START OF THE PROJECT GUTENBERG\"\n",
    "    ]\n",
    "    \n",
    "    end_markers = [\n",
    "        \"*** END OF THIS PROJECT GUTENBERG\",\n",
    "        \"***END OF THE PROJECT GUTENBERG\",\n",
    "        \"*** END OF THE PROJECT GUTENBERG\",\n",
    "        \"End of the Project Gutenberg\"\n",
    "    ]\n",
    "    \n",
    "    start_pos = len(text)\n",
    "    for marker in start_markers:\n",
    "        pos = text.find(marker)\n",
    "        if pos != -1 and pos < start_pos:\n",
    "            start_pos = pos\n",
    "    \n",
    "    if start_pos != len(text):\n",
    "        # Find the end of the header line\n",
    "        start_pos = text.find(\"\\n\", start_pos) + 1\n",
    "    else:\n",
    "        start_pos = 0\n",
    "    \n",
    "    end_pos = len(text)\n",
    "    for marker in end_markers:\n",
    "        pos = text.find(marker)\n",
    "        if pos != -1 and pos < end_pos:\n",
    "            end_pos = pos\n",
    "    \n",
    "    if end_pos == len(text):\n",
    "        end_pos = len(text)\n",
    "    \n",
    "    # Extract the content between header and footer\n",
    "    content = text[start_pos:end_pos].strip()\n",
    "    \n",
    "    # Additional cleaning\n",
    "    content = re.sub(r'\\r\\n', '\\n', content)  # Normalize line endings\n",
    "    content = re.sub(r'\\n{3,}', '\\n\\n', content)  # Remove excessive newlines\n",
    "    \n",
    "    return content\n",
    "\n",
    "# Download some texts from different domains\n",
    "text_urls = {\n",
    "    \"fiction\": \"https://www.gutenberg.org/files/1342/1342-0.txt\",  # Pride and Prejudice\n",
    "    \"science\": \"https://www.gutenberg.org/files/2009/2009-0.txt\",  # Origin of Species\n",
    "    \"philosophy\": \"https://www.gutenberg.org/files/4280/4280-0.txt\"  # Critique of Pure Reason\n",
    "}\n",
    "\n",
    "# Download and clean the texts\n",
    "texts = {}\n",
    "for name, url in text_urls.items():\n",
    "    try:\n",
    "        print(f\"Downloading {name}...\")\n",
    "        text = download_text(url)\n",
    "        texts[name] = clean_gutenberg_text(text)\n",
    "        print(f\"Downloaded {name}: {len(texts[name])} characters\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {name}: {e}\")\n",
    "\n",
    "# Create a combined corpus\n",
    "all_text = \"\\n\\n\".join(texts.values())\n",
    "print(f\"Total corpus size: {len(all_text)} characters\")\n",
    "\n",
    "# Save texts to files for tokenizer training\n",
    "import os\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "for name, text in texts.items():\n",
    "    with open(f\"data/{name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "with open(\"data/all_texts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing Different Tokenization Algorithms\n",
    "\n",
    "Let's implement and compare different tokenization algorithms using the HuggingFace Tokenizers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe_tokenizer(files, vocab_size=10000):\n",
    "    \"\"\"Train a BPE tokenizer.\"\"\"\n",
    "    # Initialize a tokenizer with BPE model\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    \n",
    "    # Set up pre-tokenizer\n",
    "    tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=False)\n",
    "    \n",
    "    # Set up trainer\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "        min_frequency=2\n",
    "    )\n",
    "    \n",
    "    # Train tokenizer\n",
    "    tokenizer.train(files, trainer)\n",
    "    \n",
    "    # Set up decoder\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "def train_wordpiece_tokenizer(files, vocab_size=10000):\n",
    "    \"\"\"Train a WordPiece tokenizer.\"\"\"\n",
    "    # Initialize a tokenizer with WordPiece model\n",
    "    tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "    \n",
    "    # Set up pre-tokenizer\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    \n",
    "    # Set up trainer\n",
    "    trainer = WordPieceTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "        min_frequency=2\n",
    "    )\n",
    "    \n",
    "    # Train tokenizer\n",
    "    tokenizer.train(files, trainer)\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "def train_unigram_tokenizer(files, vocab_size=10000):\n",
    "    \"\"\"Train a Unigram tokenizer.\"\"\"\n",
    "    # Initialize a tokenizer with Unigram model\n",
    "    tokenizer = Tokenizer(Unigram())\n",
    "    \n",
    "    # Set up pre-tokenizer\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    \n",
    "    # Set up trainer\n",
    "    trainer = UnigramTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "        unk_token=\"[UNK]\"\n",
    "    )\n",
    "    \n",
    "    # Train tokenizer\n",
    "    tokenizer.train(files, trainer)\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tokenizers\n",
    "files = [\"data/all_texts.txt\"]\n",
    "vocab_size = 8000\n",
    "\n",
    "print(\"Training BPE tokenizer...\")\n",
    "start_time = time.time()\n",
    "bpe_tokenizer = train_bpe_tokenizer(files, vocab_size)\n",
    "bpe_time = time.time() - start_time\n",
    "print(f\"BPE tokenizer trained in {bpe_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nTraining WordPiece tokenizer...\")\n",
    "start_time = time.time()\n",
    "wordpiece_tokenizer = train_wordpiece_tokenizer(files, vocab_size)\n",
    "wordpiece_time = time.time() - start_time\n",
    "print(f\"WordPiece tokenizer trained in {wordpiece_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nTraining Unigram tokenizer...\")\n",
    "start_time = time.time()\n",
    "unigram_tokenizer = train_unigram_tokenizer(files, vocab_size)\n",
    "unigram_time = time.time() - start_time\n",
    "print(f\"Unigram tokenizer trained in {unigram_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparing Tokenization Algorithms\n",
    "\n",
    "Let's compare the different tokenization algorithms in terms of efficiency and token distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tokenization_speed(tokenizers, text, num_runs=5):\n",
    "    \"\"\"Compare tokenization speed of different tokenizers.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        # Warm-up\n",
    "        for _ in range(3):\n",
    "            _ = tokenizer.encode(text)\n",
    "        \n",
    "        # Measure time\n",
    "        start_time = time.time()\n",
    "        for _ in range(num_runs):\n",
    "            _ = tokenizer.encode(text)\n",
    "        avg_time = (time.time() - start_time) / num_runs\n",
    "        \n",
    "        results[name] = avg_time\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_sequence_lengths(tokenizers, texts):\n",
    "    \"\"\"Compare sequence lengths produced by different tokenizers.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        domain_lengths = {}\n",
    "        for domain, text in texts.items():\n",
    "            encoding = tokenizer.encode(text)\n",
    "            domain_lengths[domain] = len(encoding.ids)\n",
    "        results[name] = domain_lengths\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_token_distribution(tokenizer, text, top_n=20):\n",
    "    \"\"\"Analyze token distribution for a tokenizer.\"\"\"\n",
    "    encoding = tokenizer.encode(text)\n",
    "    \n",
    "    # Count token frequencies\n",
    "    token_counts = Counter(encoding.ids)\n",
    "    \n",
    "    # Get most common tokens\n",
    "    most_common = token_counts.most_common(top_n)\n",
    "    \n",
    "    # Convert token IDs to strings\n",
    "    token_strings = []\n",
    "    for token_id, count in most_common:\n",
    "        token = tokenizer.decode([token_id])\n",
    "        token_strings.append((token, count))\n",
    "    \n",
    "    return token_strings\n",
    "\n",
    "# Compare tokenization speed\n",
    "tokenizers = {\n",
    "    \"BPE\": bpe_tokenizer,\n",
    "    \"WordPiece\": wordpiece_tokenizer,\n",
    "    \"Unigram\": unigram_tokenizer\n",
    "}\n",
    "\n",
    "# Use a sample of text for speed comparison\n",
    "sample_text = all_text[:100000]  # First 100K characters\n",
    "\n",
    "print(\"Comparing tokenization speed...\")\n",
    "speed_results = compare_tokenization_speed(tokenizers, sample_text)\n",
    "for name, time_taken in speed_results.items():\n",
    "    print(f\"{name}: {time_taken:.6f} seconds\")\n",
    "\n",
    "# Compare sequence lengths\n",
    "print(\"\\nComparing sequence lengths...\")\n",
    "length_results = compare_sequence_lengths(tokenizers, texts)\n",
    "for tokenizer_name, domain_lengths in length_results.items():\n",
    "    print(f\"\\n{tokenizer_name}:\")\n",
    "    for domain, length in domain_lengths.items():\n",
    "        print(f\"  {domain}: {length} tokens\")\n",
    "\n",
    "# Plot tokenization speed comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(speed_results.keys(), speed_results.values())\n",
    "plt.title('Tokenization Speed Comparison')\n",
    "plt.xlabel('Tokenizer')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Plot sequence length comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "domains = list(texts.keys())\n",
    "x = np.arange(len(domains))\n",
    "width = 0.25\n",
    "multiplier = 0\n",
    "\n",
    "for tokenizer_name, domain_lengths in length_results.items():\n",
    "    offset = width * multiplier\n",
    "    plt.bar(x + offset, [domain_lengths[domain] for domain in domains], width, label=tokenizer_name)\n",
    "    multiplier += 1\n",
    "\n",
    "plt.title('Sequence Length Comparison')\n",
    "plt.xlabel('Domain')\n",
    "plt.ylabel('Number of Tokens')\n",
    "plt.xticks(x + width, domains)\n",
    "plt.legend(loc='best')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyzing Token Distribution\n",
    "\n",
    "Let's analyze the token distribution for each tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze token distributions\n",
    "for name, tokenizer in tokenizers.items():\n",
    "    print(f\"\\n{name} - Most common tokens:\")\n",
    "    token_dist = analyze_token_distribution(tokenizer, all_text)\n",
    "    for token, count in token_dist:\n",
    "        # Replace newlines and tabs for display\n",
    "        display_token = token.replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n",
    "        print(f\"'{display_token}': {count}\")\n",
    "\n",
    "# Plot token frequency distribution\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "for i, (name, tokenizer) in enumerate(tokenizers.items()):\n",
    "    # Encode full text\n",
    "    encoding = tokenizer.encode(all_text)\n",
    "    token_counts = Counter(encoding.ids)\n",
    "    \n",
    "    # Get frequencies of top 100 tokens\n",
    "    top_tokens = token_counts.most_common(100)\n",
    "    freqs = [count for _, count in top_tokens]\n",
    "    \n",
    "    # Plot on log scale\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.loglog(range(1, len(freqs)+1), freqs, marker='o', markersize=3)\n",
    "    plt.title(f'{name} Token Distribution')\n",
    "    plt.xlabel('Token Rank')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenization of Out-of-Vocabulary Words\n",
    "\n",
    "Let's examine how each tokenizer handles out-of-vocabulary (OOV) words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_oov_handling(tokenizers, words):\n",
    "    \"\"\"Compare how tokenizers handle OOV words.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        word_tokens = {}\n",
    "        for word in words:\n",
    "            encoding = tokenizer.encode(word)\n",
    "            tokens = tokenizer.decode(encoding.ids).split()\n",
    "            word_tokens[word] = (encoding.ids, tokens)\n",
    "        results[name] = word_tokens\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with some potentially OOV words\n",
    "oov_words = [\n",
    "    \"transformer\",  # Technical term\n",
    "    \"COVID19\",  # Recent term\n",
    "    \"blockchain\",  # Technical term\n",
    "    \"supercalifragilisticexpialidocious\",  # Very long word\n",
    "    \"ðŸ˜Š\",  # Emoji\n",
    "    \"https://example.com\",  # URL\n",
    "    \"#hashtag\"  # Social media tag\n",
    "]\n",
    "\n",
    "oov_results = compare_oov_handling(tokenizers, oov_words)\n",
    "\n",
    "# Display results\n",
    "for word in oov_words:\n",
    "    print(f\"\\nWord: '{word}'\")\n",
    "    for name in tokenizers.keys():\n",
    "        ids, tokens = oov_results[name][word]\n",
    "        print(f\"  {name}: {len(ids)} tokens - {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Saving and Loading Tokenizers\n",
    "\n",
    "Let's save our trained tokenizers for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for tokenizers\n",
    "os.makedirs(\"tokenizers\", exist_ok=True)\n",
    "\n",
    "# Save tokenizers\n",
    "for name, tokenizer in tokenizers.items():\n",
    "    tokenizer.save(f\"tokenizers/{name.lower()}_tokenizer.json\")\n",
    "    print(f\"Saved {name} tokenizer\")\n",
    "\n",
    "# Test loading a tokenizer\n",
    "loaded_tokenizer = Tokenizer.from_file(\"tokenizers/bpe_tokenizer.json\")\n",
    "print(\"\\nLoaded BPE tokenizer successfully\")\n",
    "\n",
    "# Test tokenization with loaded tokenizer\n",
    "test_text = \"This is a test of the loaded tokenizer.\"\n",
    "original_encoding = bpe_tokenizer.encode(test_text)\n",
    "loaded_encoding = loaded_tokenizer.encode(test_text)\n",
    "\n",
    "print(f\"Original tokens: {original_encoding.tokens}\")\n",
    "print(f\"Loaded tokens: {loaded_encoding.tokens}\")\n",
    "print(f\"Tokens match: {original_encoding.ids == loaded_encoding.ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Key Insights\n",
    "\n",
    "In this notebook, we've explored tokenization at scale by:\n",
    "\n",
    "1. **Implementing different tokenization algorithms**:\n",
    "   - BPE (Byte-Pair Encoding)\n",
    "   - WordPiece\n",
    "   - Unigram\n",
    "\n",
    "2. **Comparing tokenization efficiency**:\n",
    "   - Processing speed\n",
    "   - Resulting sequence lengths\n",
    "\n",
    "3. **Analyzing token distributions**:\n",
    "   - Most common tokens\n",
    "   - Frequency distributions\n",
    "\n",
    "4. **Examining OOV handling**:\n",
    "   - How different tokenizers handle unseen words\n",
    "   - Subword decomposition strategies\n",
    "\n",
    "5. **Saving and loading tokenizers**:\n",
    "   - Persistence for consistent tokenization\n",
    "\n",
    "In Part 2, we'll explore sequence packing and masking techniques for efficient training of causal language models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
