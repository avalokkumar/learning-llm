{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Day 11: Language Modeling Datasets & Perplexity\n",
    "\n",
    "## Overview\n",
    "In this notebook, we'll explore language modeling datasets, learn how to calculate perplexity, and train a simple model to see these concepts in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import requests\n",
    "import random\n",
    "import math\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating a Toy Dataset\n",
    "\n",
    "We'll download classic literature from Project Gutenberg to create our toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_text(url: str) -> str:\n",
    "    response = requests.get(url)\n",
    "    response.encoding = 'utf-8'\n",
    "    return response.text\n",
    "\n",
    "def clean_gutenberg_text(text: str) -> str:\n",
    "    start_markers = [\"*** START OF THIS PROJECT GUTENBERG\", \"***START OF THE PROJECT GUTENBERG\", \"*** START OF THE PROJECT GUTENBERG\"]\n",
    "    end_markers = [\"*** END OF THIS PROJECT GUTENBERG\", \"***END OF THE PROJECT GUTENBERG\", \"*** END OF THE PROJECT GUTENBERG\", \"End of the Project Gutenberg\"]\n",
    "    \n",
    "    start_pos = -1\n",
    "    for marker in start_markers:\n",
    "        pos = text.find(marker)\n",
    "        if pos != -1:\n",
    "            start_pos = text.find('\\n', pos) + 1\n",
    "            break\n",
    "    if start_pos == -1: start_pos = 0\n",
    "        \n",
    "    end_pos = len(text)\n",
    "    for marker in end_markers:\n",
    "        pos = text.find(marker)\n",
    "        if pos != -1:\n",
    "            end_pos = pos\n",
    "            break\n",
    "            \n",
    "    content = text[start_pos:end_pos].strip()\n",
    "    content = re.sub(r'\\r\\n', '\\n', content)\n",
    "    content = re.sub(r'\\n{3,}', '\\n\\n', content)\n",
    "    return content\n",
    "\n",
    "book_urls = {\n",
    "    \"alice\": \"https://www.gutenberg.org/files/11/11-0.txt\",\n",
    "    \"sherlock\": \"https://www.gutenberg.org/files/1661/1661-0.txt\",\n",
    "    \"frankenstein\": \"https://www.gutenberg.org/files/84/84-0.txt\"\n",
    "}\n",
    "\n",
    "books = {}\n",
    "for name, url in book_urls.items():\n",
    "    try:\n",
    "        print(f\"Downloading {name}...\")\n",
    "        text = download_text(url)\n",
    "        books[name] = clean_gutenberg_text(text)\n",
    "        print(f\"Downloaded {name}: {len(books[name])} characters\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Tokenization & Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTokenizer:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        all_text = ''.join(texts)\n",
    "        self.chars = sorted(list(set(all_text)))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        return [self.char_to_idx[char] for char in text]\n",
    "\n",
    "    def decode(self, indices: List[int]) -> str:\n",
    "        return ''.join([self.idx_to_char[i] for i in indices])\n",
    "\n",
    "tokenizer = CharacterTokenizer(list(books.values()))\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "\n",
    "def create_train_val_split(text: str, val_ratio: float = 0.1) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    encoded_text = tokenizer.encode(text)\n",
    "    n = int(len(encoded_text) * (1 - val_ratio))\n",
    "    train_data = torch.tensor(encoded_text[:n], dtype=torch.long)\n",
    "    val_data = torch.tensor(encoded_text[n:], dtype=torch.long)\n",
    "    return train_data, val_data\n",
    "\n",
    "processed_data = {name: create_train_val_split(text) for name, text in books.items()}\n",
    "\n",
    "def get_batch(split, book_name, seq_len, batch_size):\n",
    "    data = processed_data[book_name][0] if split == 'train' else processed_data[book_name][1]\n",
    "    ix = torch.randint(len(data) - seq_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+seq_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+seq_len+1] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Language Model and Perplexity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCharLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        logits = self.fc(self.dropout(lstm_out))\n",
    "        return logits\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_perplexity(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    for x, y in data_loader:\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, model.embedding.num_embeddings), y.view(-1), reduction='sum')\n",
    "        total_loss += loss.item()\n",
    "        total_tokens += y.numel()\n",
    "    model.train()\n",
    "    return math.exp(total_loss / total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(book_name, epochs=3, seq_len=64, batch_size=32, lr=0.001):\n",
    "    model = SimpleCharLM(vocab_size=tokenizer.vocab_size).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    history = {'train_ppl': [], 'val_ppl': []}\n",
    "    \n",
    "    # Create a simple dataloader for validation\n",
    "    val_data = processed_data[book_name][1]\n",
    "    val_loader = [(val_data[i:i+seq_len].unsqueeze(0).to(device), val_data[i+1:i+seq_len+1].unsqueeze(0).to(device)) for i in range(0, len(val_data)-seq_len-1, seq_len)]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        # Simplified training loop for demonstration\n",
    "        for _ in range(100): # 100 steps per epoch\n",
    "            xb, yb = get_batch('train', book_name, seq_len, batch_size)\n",
    "            logits = model(xb)\n",
    "            loss = F.cross_entropy(logits.view(-1, tokenizer.vocab_size), yb.view(-1))\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluation\n",
    "        train_ppl = calculate_perplexity(model, [(xb, yb)]) # Approx train PPL\n",
    "        val_ppl = calculate_perplexity(model, val_loader)\n",
    "        history['train_ppl'].append(train_ppl)\n",
    "        history['val_ppl'].append(val_ppl)\n",
    "        print(f'Epoch {epoch+1}, Book: {book_name}, Train PPL: {train_ppl:.2f}, Val PPL: {val_ppl:.2f}')\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Train on one book\n",
    "trained_model, history = train_model('alice')\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history['train_ppl'], label='Train Perplexity')\n",
    "plt.plot(history['val_ppl'], label='Validation Perplexity')\n",
    "plt.title('Perplexity over Training Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
