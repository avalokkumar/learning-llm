{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 5: End-to-End Pipeline - Part 1\n",
    "\n",
    "In this notebook, we'll build a complete preprocessing pipeline that takes raw text and produces transformer-ready inputs. We'll integrate all the components we've learned about in Week 1: tokenization, embeddings, and positional encoding.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Union\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Positional Encoding Components\n",
    "\n",
    "First, let's define our positional encoding classes that we'll use in our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class LearnedPositionalEmbedding(nn.Module):\n",
    "    \"\"\"Learned positional embedding implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_seq_len, d_model):\n",
    "        super().__init__()\n",
    "        self.position_embeddings = nn.Embedding(max_seq_len, d_model)\n",
    "        nn.init.normal_(self.position_embeddings.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Add learned positional embeddings.\"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand(x.size(0), -1)\n",
    "        return x + self.position_embeddings(position_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Complete Preprocessing Pipeline\n",
    "\n",
    "Now, let's implement our complete transformer preprocessing pipeline that integrates tokenization, embeddings, and positional encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPreprocessingPipeline:\n",
    "    \"\"\"Complete preprocessing pipeline for transformer models.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size: int = 1000,\n",
    "                 embedding_dim: int = 128,\n",
    "                 max_seq_len: int = 512,\n",
    "                 pad_token: str = \"[PAD]\",\n",
    "                 cls_token: str = \"[CLS]\",\n",
    "                 sep_token: str = \"[SEP]\",\n",
    "                 unk_token: str = \"[UNK]\",\n",
    "                 pos_encoding_type: str = \"sinusoidal\"):\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Special tokens\n",
    "        self.pad_token = pad_token\n",
    "        self.cls_token = cls_token\n",
    "        self.sep_token = sep_token\n",
    "        self.unk_token = unk_token\n",
    "        \n",
    "        # Initialize components\n",
    "        self._init_tokenizer()\n",
    "        self._init_embeddings()\n",
    "        self._init_positional_encoding(pos_encoding_type)\n",
    "    \n",
    "    def _init_tokenizer(self):\n",
    "        \"\"\"Initialize tokenizer with basic vocabulary.\"\"\"\n",
    "        # Create basic vocabulary\n",
    "        special_tokens = [self.pad_token, self.unk_token, self.cls_token, self.sep_token]\n",
    "        common_words = [\"the\", \"a\", \"an\", \"and\", \"or\", \"but\", \"in\", \"on\", \"at\", \"to\", \"for\", \"of\", \"with\"]\n",
    "        chars = list(\"abcdefghijklmnopqrstuvwxyz0123456789.,!?;:()[]{}\\\"'-_/\\\\@#$%^&*+=<>|`~\")\n",
    "        \n",
    "        vocab_list = special_tokens + common_words + chars\n",
    "        vocab_list = vocab_list[:self.vocab_size]  # Limit to vocab_size\n",
    "        \n",
    "        self.vocab = {token: idx for idx, token in enumerate(vocab_list)}\n",
    "        self.vocab_reverse = {idx: token for token, idx in self.vocab.items()}\n",
    "        \n",
    "        # Special token IDs\n",
    "        self.pad_token_id = self.vocab[self.pad_token]\n",
    "        self.cls_token_id = self.vocab[self.cls_token]\n",
    "        self.sep_token_id = self.vocab[self.sep_token]\n",
    "        self.unk_token_id = self.vocab[self.unk_token]\n",
    "    \n",
    "    def _init_embeddings(self):\n",
    "        \"\"\"Initialize embedding layer.\"\"\"\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, \n",
    "                                    padding_idx=self.pad_token_id)\n",
    "        nn.init.normal_(self.embedding.weight, mean=0, std=0.1)\n",
    "        \n",
    "        # Zero out padding token embedding\n",
    "        self.embedding.weight.data[self.pad_token_id].fill_(0)\n",
    "    \n",
    "    def _init_positional_encoding(self, pos_type):\n",
    "        \"\"\"Initialize positional encoding.\"\"\"\n",
    "        if pos_type == \"sinusoidal\":\n",
    "            self.pos_encoding = SinusoidalPositionalEncoding(\n",
    "                self.embedding_dim, self.max_seq_len)\n",
    "        elif pos_type == \"learned\":\n",
    "            self.pos_encoding = LearnedPositionalEmbedding(\n",
    "                self.max_seq_len, self.embedding_dim)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown positional encoding type: {pos_type}\")\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Simple tokenization (character-level for demo).\"\"\"\n",
    "        # Normalize text\n",
    "        text = text.lower().strip()\n",
    "        \n",
    "        # Simple character-level tokenization\n",
    "        tokens = []\n",
    "        for char in text:\n",
    "            if char in self.vocab:\n",
    "                tokens.append(char)\n",
    "            else:\n",
    "                tokens.append(self.unk_token)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def encode_text(self, text: str, \n",
    "                   add_special_tokens: bool = True,\n",
    "                   max_length: Optional[int] = None,\n",
    "                   padding: bool = True,\n",
    "                   truncation: bool = True) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Complete encoding pipeline.\"\"\"\n",
    "        \n",
    "        if max_length is None:\n",
    "            max_length = self.max_seq_len\n",
    "        \n",
    "        # Step 1: Tokenization\n",
    "        tokens = self.tokenize(text)\n",
    "        \n",
    "        # Step 2: Add special tokens\n",
    "        if add_special_tokens:\n",
    "            tokens = [self.cls_token] + tokens + [self.sep_token]\n",
    "        \n",
    "        # Step 3: Truncation\n",
    "        if truncation and len(tokens) > max_length:\n",
    "            if add_special_tokens:\n",
    "                tokens = tokens[:max_length-1] + [self.sep_token]\n",
    "            else:\n",
    "                tokens = tokens[:max_length]\n",
    "        \n",
    "        # Step 4: Convert to IDs\n",
    "        input_ids = [self.vocab.get(token, self.unk_token_id) for token in tokens]\n",
    "        \n",
    "        # Step 5: Create attention mask\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        \n",
    "        # Step 6: Padding\n",
    "        if padding and len(input_ids) < max_length:\n",
    "            padding_length = max_length - len(input_ids)\n",
    "            input_ids.extend([self.pad_token_id] * padding_length)\n",
    "            attention_mask.extend([0] * padding_length)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'tokens': tokens\n",
    "        }\n",
    "    \n",
    "    def get_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get token embeddings.\"\"\"\n",
    "        return self.embedding(input_ids)\n",
    "    \n",
    "    def add_positional_encoding(self, embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Add positional encoding to embeddings.\"\"\"\n",
    "        return self.pos_encoding(embeddings)\n",
    "    \n",
    "    def process_text(self, text: str, **kwargs) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Complete end-to-end processing.\"\"\"\n",
    "        # Step 1: Encode text to tokens and IDs\n",
    "        encoded = self.encode_text(text, **kwargs)\n",
    "        \n",
    "        # Step 2: Get embeddings\n",
    "        embeddings = self.get_embeddings(encoded['input_ids'].unsqueeze(0))\n",
    "        \n",
    "        # Step 3: Add positional encoding\n",
    "        embeddings_with_pos = self.add_positional_encoding(embeddings)\n",
    "        \n",
    "        # Step 4: Scale embeddings (as in original Transformer)\n",
    "        embeddings_with_pos = embeddings_with_pos * np.sqrt(self.embedding_dim)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'],\n",
    "            'attention_mask': encoded['attention_mask'],\n",
    "            'embeddings': embeddings_with_pos.squeeze(0),\n",
    "            'tokens': encoded['tokens']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing the Pipeline\n",
    "\n",
    "Let's test our pipeline with a comprehensive set of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pipeline_comprehensive():\n",
    "    \"\"\"Comprehensive pipeline testing.\"\"\"\n",
    "    \n",
    "    print(\"Comprehensive Pipeline Testing\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = TransformerPreprocessingPipeline(\n",
    "        vocab_size=200,\n",
    "        embedding_dim=64,\n",
    "        max_seq_len=32,\n",
    "        pos_encoding_type=\"sinusoidal\"\n",
    "    )\n",
    "    \n",
    "    # Test cases\n",
    "    test_texts = [\n",
    "        \"hello world\",\n",
    "        \"this is a longer test sentence\",\n",
    "        \"short\",\n",
    "        \"\",  # Empty string\n",
    "        \"a\" * 100,  # Very long text\n",
    "        \"hello, world! 123\",  # With punctuation and numbers\n",
    "    ]\n",
    "    \n",
    "    for i, text in enumerate(test_texts, 1):\n",
    "        print(f\"\\nTest {i}: '{text[:30]}{'...' if len(text) > 30 else ''}'\")\n",
    "        \n",
    "        try:\n",
    "            result = pipeline.process_text(text, max_length=16, padding=True)\n",
    "            \n",
    "            print(f\"  Input IDs shape: {result['input_ids'].shape}\")\n",
    "            print(f\"  Attention mask shape: {result['attention_mask'].shape}\")\n",
    "            print(f\"  Embeddings shape: {result['embeddings'].shape}\")\n",
    "            print(f\"  Tokens: {result['tokens'][:10]}{'...' if len(result['tokens']) > 10 else ''}\")\n",
    "            print(f\"  Non-padding tokens: {result['attention_mask'].sum().item()}\")\n",
    "            \n",
    "            # Validate shapes\n",
    "            assert result['input_ids'].shape[0] == 16, \"Input IDs length mismatch\"\n",
    "            assert result['attention_mask'].shape[0] == 16, \"Attention mask length mismatch\"\n",
    "            assert result['embeddings'].shape == (16, 64), \"Embeddings shape mismatch\"\n",
    "            \n",
    "            print(\"  ✓ All validations passed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# Run comprehensive tests\n",
    "pipeline = test_pipeline_comprehensive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing Pipeline Components\n",
    "\n",
    "Let's visualize the different components of our pipeline to better understand how they work together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pipeline_components(pipeline, text=\"hello world\"):\n",
    "    \"\"\"Visualize each step of the pipeline.\"\"\"\n",
    "    \n",
    "    print(f\"Pipeline Visualization for: '{text}'\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Process text\n",
    "    result = pipeline.process_text(text, max_length=16, padding=True)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Token IDs\n",
    "    axes[0, 0].bar(range(len(result['input_ids'])), result['input_ids'].numpy())\n",
    "    axes[0, 0].set_title('Token IDs')\n",
    "    axes[0, 0].set_xlabel('Position')\n",
    "    axes[0, 0].set_ylabel('Token ID')\n",
    "    \n",
    "    # 2. Attention Mask\n",
    "    axes[0, 1].bar(range(len(result['attention_mask'])), result['attention_mask'].numpy())\n",
    "    axes[0, 1].set_title('Attention Mask')\n",
    "    axes[0, 1].set_xlabel('Position')\n",
    "    axes[0, 1].set_ylabel('Mask Value')\n",
    "    \n",
    "    # 3. Embeddings (first 8 dimensions)\n",
    "    embeddings_subset = result['embeddings'][:, :8].detach().numpy()\n",
    "    im1 = axes[1, 0].imshow(embeddings_subset.T, cmap='RdBu', aspect='auto')\n",
    "    axes[1, 0].set_title('Embeddings (first 8 dims)')\n",
    "    axes[1, 0].set_xlabel('Position')\n",
    "    axes[1, 0].set_ylabel('Embedding Dimension')\n",
    "    plt.colorbar(im1, ax=axes[1, 0])\n",
    "    \n",
    "    # 4. Positional Encoding Pattern\n",
    "    pos_encoding = pipeline.pos_encoding.pe[0, :16, :8].detach().numpy()\n",
    "    im2 = axes[1, 1].imshow(pos_encoding.T, cmap='RdBu', aspect='auto')\n",
    "    axes[1, 1].set_title('Positional Encoding (first 8 dims)')\n",
    "    axes[1, 1].set_xlabel('Position')\n",
    "    axes[1, 1].set_ylabel('Encoding Dimension')\n",
    "    plt.colorbar(im2, ax=axes[1, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed breakdown\n",
    "    print(\"\\nDetailed Breakdown:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    valid_positions = result['attention_mask'].sum().item()\n",
    "    for i in range(min(valid_positions, 10)):  # Show first 10 valid positions\n",
    "        token_id = result['input_ids'][i].item()\n",
    "        token = pipeline.vocab_reverse.get(token_id, f\"ID_{token_id}\")\n",
    "        embedding_norm = torch.norm(result['embeddings'][i]).item()\n",
    "        \n",
    "        print(f\"Position {i}: '{token}' (ID: {token_id}) | Embedding norm: {embedding_norm:.3f}\")\n",
    "\n",
    "# Visualize pipeline components\n",
    "visualize_pipeline_components(pipeline, \"hello world!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
