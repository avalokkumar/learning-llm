{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 5: End-to-End Pipeline - Part 3\n",
    "\n",
    "In this notebook, we'll explore advanced features of our transformer preprocessing pipeline, including batch processing and pipeline statistics.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Union\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Advanced Pipeline Implementation\n",
    "\n",
    "Let's extend our basic pipeline with advanced features like batch processing and statistics tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's define our positional encoding classes\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class LearnedPositionalEmbedding(nn.Module):\n",
    "    \"\"\"Learned positional embedding implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_seq_len, d_model):\n",
    "        super().__init__()\n",
    "        self.position_embeddings = nn.Embedding(max_seq_len, d_model)\n",
    "        nn.init.normal_(self.position_embeddings.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Add learned positional embeddings.\"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand(x.size(0), -1)\n",
    "        return x + self.position_embeddings(position_ids)\n",
    "\n",
    "# Base pipeline class (simplified version)\n",
    "class TransformerPreprocessingPipeline:\n",
    "    \"\"\"Base preprocessing pipeline for transformer models.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size: int = 1000,\n",
    "                 embedding_dim: int = 128,\n",
    "                 max_seq_len: int = 512,\n",
    "                 pad_token: str = \"[PAD]\",\n",
    "                 cls_token: str = \"[CLS]\",\n",
    "                 sep_token: str = \"[SEP]\",\n",
    "                 unk_token: str = \"[UNK]\",\n",
    "                 pos_encoding_type: str = \"sinusoidal\"):\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Special tokens\n",
    "        self.pad_token = pad_token\n",
    "        self.cls_token = cls_token\n",
    "        self.sep_token = sep_token\n",
    "        self.unk_token = unk_token\n",
    "        \n",
    "        # Initialize components\n",
    "        self._init_tokenizer()\n",
    "        self._init_embeddings()\n",
    "        self._init_positional_encoding(pos_encoding_type)\n",
    "    \n",
    "    def _init_tokenizer(self):\n",
    "        \"\"\"Initialize tokenizer with basic vocabulary.\"\"\"\n",
    "        # Create basic vocabulary\n",
    "        special_tokens = [self.pad_token, self.unk_token, self.cls_token, self.sep_token]\n",
    "        common_words = [\"the\", \"a\", \"an\", \"and\", \"or\", \"but\", \"in\", \"on\", \"at\", \"to\", \"for\", \"of\", \"with\"]\n",
    "        chars = list(\"abcdefghijklmnopqrstuvwxyz0123456789.,!?;:()[]{}\\\"'-_/\\\\@#$%^&*+=<>|`~\")\n",
    "        \n",
    "        vocab_list = special_tokens + common_words + chars\n",
    "        vocab_list = vocab_list[:self.vocab_size]  # Limit to vocab_size\n",
    "        \n",
    "        self.vocab = {token: idx for idx, token in enumerate(vocab_list)}\n",
    "        self.vocab_reverse = {idx: token for token, idx in self.vocab.items()}\n",
    "        \n",
    "        # Special token IDs\n",
    "        self.pad_token_id = self.vocab[self.pad_token]\n",
    "        self.cls_token_id = self.vocab[self.cls_token]\n",
    "        self.sep_token_id = self.vocab[self.sep_token]\n",
    "        self.unk_token_id = self.vocab[self.unk_token]\n",
    "    \n",
    "    def _init_embeddings(self):\n",
    "        \"\"\"Initialize embedding layer.\"\"\"\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, \n",
    "                                    padding_idx=self.pad_token_id)\n",
    "        nn.init.normal_(self.embedding.weight, mean=0, std=0.1)\n",
    "        \n",
    "        # Zero out padding token embedding\n",
    "        self.embedding.weight.data[self.pad_token_id].fill_(0)\n",
    "    \n",
    "    def _init_positional_encoding(self, pos_type):\n",
    "        \"\"\"Initialize positional encoding.\"\"\"\n",
    "        if pos_type == \"sinusoidal\":\n",
    "            self.pos_encoding = SinusoidalPositionalEncoding(\n",
    "                self.embedding_dim, self.max_seq_len)\n",
    "        elif pos_type == \"learned\":\n",
    "            self.pos_encoding = LearnedPositionalEmbedding(\n",
    "                self.max_seq_len, self.embedding_dim)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown positional encoding type: {pos_type}\")\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Simple tokenization (character-level for demo).\"\"\"\n",
    "        # Normalize text\n",
    "        text = text.lower().strip()\n",
    "        \n",
    "        # Simple character-level tokenization\n",
    "        tokens = []\n",
    "        for char in text:\n",
    "            if char in self.vocab:\n",
    "                tokens.append(char)\n",
    "            else:\n",
    "                tokens.append(self.unk_token)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def encode_text(self, text: str, \n",
    "                   add_special_tokens: bool = True,\n",
    "                   max_length: Optional[int] = None,\n",
    "                   padding: bool = True,\n",
    "                   truncation: bool = True) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Complete encoding pipeline.\"\"\"\n",
    "        \n",
    "        if max_length is None:\n",
    "            max_length = self.max_seq_len\n",
    "        \n",
    "        # Step 1: Tokenization\n",
    "        tokens = self.tokenize(text)\n",
    "        \n",
    "        # Step 2: Add special tokens\n",
    "        if add_special_tokens:\n",
    "            tokens = [self.cls_token] + tokens + [self.sep_token]\n",
    "        \n",
    "        # Step 3: Truncation\n",
    "        if truncation and len(tokens) > max_length:\n",
    "            if add_special_tokens:\n",
    "                tokens = tokens[:max_length-1] + [self.sep_token]\n",
    "            else:\n",
    "                tokens = tokens[:max_length]\n",
    "        \n",
    "        # Step 4: Convert to IDs\n",
    "        input_ids = [self.vocab.get(token, self.unk_token_id) for token in tokens]\n",
    "        \n",
    "        # Step 5: Create attention mask\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        \n",
    "        # Step 6: Padding\n",
    "        if padding and len(input_ids) < max_length:\n",
    "            padding_length = max_length - len(input_ids)\n",
    "            input_ids.extend([self.pad_token_id] * padding_length)\n",
    "            attention_mask.extend([0] * padding_length)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'tokens': tokens\n",
    "        }\n",
    "    \n",
    "    def get_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get token embeddings.\"\"\"\n",
    "        return self.embedding(input_ids)\n",
    "    \n",
    "    def add_positional_encoding(self, embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Add positional encoding to embeddings.\"\"\"\n",
    "        return self.pos_encoding(embeddings)\n",
    "    \n",
    "    def process_text(self, text: str, **kwargs) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Complete end-to-end processing.\"\"\"\n",
    "        # Step 1: Encode text to tokens and IDs\n",
    "        encoded = self.encode_text(text, **kwargs)\n",
    "        \n",
    "        # Step 2: Get embeddings\n",
    "        embeddings = self.get_embeddings(encoded['input_ids'].unsqueeze(0))\n",
    "        \n",
    "        # Step 3: Add positional encoding\n",
    "        embeddings_with_pos = self.add_positional_encoding(embeddings)\n",
    "        \n",
    "        # Step 4: Scale embeddings (as in original Transformer)\n",
    "        embeddings_with_pos = embeddings_with_pos * np.sqrt(self.embedding_dim)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'],\n",
    "            'attention_mask': encoded['attention_mask'],\n",
    "            'embeddings': embeddings_with_pos.squeeze(0),\n",
    "            'tokens': encoded['tokens']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Pipeline with Batch Processing and Statistics\n",
    "\n",
    "Now, let's extend the base pipeline with advanced features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedTransformerPipeline(TransformerPreprocessingPipeline):\n",
    "    \"\"\"Extended pipeline with advanced features.\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.statistics = {\n",
    "            'processed_texts': 0,\n",
    "            'total_tokens': 0,\n",
    "            'avg_sequence_length': 0,\n",
    "            'unk_token_count': 0,\n",
    "            'token_frequency': {}\n",
    "        }\n",
    "    \n",
    "    def process_batch(self, texts: List[str], **kwargs) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Process multiple texts efficiently.\"\"\"\n",
    "        \n",
    "        batch_results = []\n",
    "        max_len = kwargs.get('max_length', self.max_seq_len)\n",
    "        \n",
    "        for text in texts:\n",
    "            result = self.process_text(text, **kwargs)\n",
    "            batch_results.append(result)\n",
    "        \n",
    "        # Stack results\n",
    "        batch_input_ids = torch.stack([r['input_ids'] for r in batch_results])\n",
    "        batch_attention_mask = torch.stack([r['attention_mask'] for r in batch_results])\n",
    "        batch_embeddings = torch.stack([r['embeddings'] for r in batch_results])\n",
    "        \n",
    "        # Update statistics\n",
    "        self._update_statistics(batch_results)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': batch_input_ids,\n",
    "            'attention_mask': batch_attention_mask,\n",
    "            'embeddings': batch_embeddings,\n",
    "            'batch_size': len(texts)\n",
    "        }\n",
    "    \n",
    "    def _update_statistics(self, results):\n",
    "        \"\"\"Update pipeline statistics based on processed results.\"\"\"\n",
    "        self.statistics['processed_texts'] += len(results)\n",
    "        \n",
    "        total_non_padding = 0\n",
    "        for result in results:\n",
    "            # Count non-padding tokens\n",
    "            non_padding = result['attention_mask'].sum().item()\n",
    "            total_non_padding += non_padding\n",
    "            \n",
    "            # Count unknown tokens\n",
    "            unk_count = (result['input_ids'] == self.unk_token_id).sum().item()\n",
    "            self.statistics['unk_token_count'] += unk_count\n",
    "            \n",
    "            # Update token frequency\n",
    "            for token_id in result['input_ids']:\n",
    "                token_id = token_id.item()\n",
    "                if token_id != self.pad_token_id:\n",
    "                    token = self.vocab_reverse.get(token_id, f\"ID_{token_id}\")\n",
    "                    self.statistics['token_frequency'][token] = \\\n",
    "                        self.statistics['token_frequency'].get(token, 0) + 1\n",
    "        \n",
    "        # Update total tokens and average sequence length\n",
    "        self.statistics['total_tokens'] += total_non_padding\n",
    "        self.statistics['avg_sequence_length'] = (\n",
    "            self.statistics['total_tokens'] / self.statistics['processed_texts']\n",
    "        )\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Get processing statistics.\"\"\"\n",
    "        stats = self.statistics.copy()\n",
    "        \n",
    "        # Get top 10 most frequent tokens\n",
    "        if self.statistics['token_frequency']:\n",
    "            sorted_tokens = sorted(\n",
    "                self.statistics['token_frequency'].items(),\n",
    "                key=lambda x: x[1],\n",
    "                reverse=True\n",
    "            )[:10]\n",
    "            stats['top_tokens'] = dict(sorted_tokens)\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def save_pipeline(self, path: str):\n",
    "        \"\"\"Save pipeline configuration.\"\"\"\n",
    "        config = {\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'max_seq_len': self.max_seq_len,\n",
    "            'vocab': self.vocab,\n",
    "            'statistics': self.statistics\n",
    "        }\n",
    "        \n",
    "        torch.save({\n",
    "            'config': config,\n",
    "            'embedding_weights': self.embedding.state_dict(),\n",
    "            'pos_encoding_weights': self.pos_encoding.state_dict() if hasattr(self.pos_encoding, 'state_dict') else None\n",
    "        }, path)\n",
    "        \n",
    "        print(f\"Pipeline saved to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing Advanced Pipeline Features\n",
    "\n",
    "Let's test the advanced features of our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_advanced_pipeline():\n",
    "    \"\"\"Test advanced pipeline features.\"\"\"\n",
    "    \n",
    "    advanced_pipeline = AdvancedTransformerPipeline(\n",
    "        vocab_size=300,\n",
    "        embedding_dim=64,\n",
    "        max_seq_len=24\n",
    "    )\n",
    "    \n",
    "    # Test batch processing\n",
    "    test_texts = [\n",
    "        \"hello world\",\n",
    "        \"this is a test\",\n",
    "        \"batch processing works\",\n",
    "        \"transformers are powerful\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Advanced Pipeline Testing\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Process batch\n",
    "    batch_result = advanced_pipeline.process_batch(test_texts, max_length=12, padding=True)\n",
    "    \n",
    "    print(f\"Batch processing results:\")\n",
    "    print(f\"  Batch size: {batch_result['batch_size']}\")\n",
    "    print(f\"  Input IDs shape: {batch_result['input_ids'].shape}\")\n",
    "    print(f\"  Embeddings shape: {batch_result['embeddings'].shape}\")\n",
    "    \n",
    "    # Show statistics\n",
    "    stats = advanced_pipeline.get_statistics()\n",
    "    print(f\"\\nPipeline Statistics:\")\n",
    "    for key, value in stats.items():\n",
    "        if key != 'token_frequency' and key != 'top_tokens':\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Show top tokens\n",
    "    if 'top_tokens' in stats:\n",
    "        print(\"\\nTop tokens:\")\n",
    "        for token, count in stats['top_tokens'].items():\n",
    "            print(f\"  '{token}': {count}\")\n",
    "    \n",
    "    # Test with more texts to see statistics evolve\n",
    "    more_texts = [\n",
    "        \"language models are fascinating\",\n",
    "        \"deep learning revolutionized nlp\",\n",
    "        \"transformers use self-attention\",\n",
    "        \"positional encoding is crucial\",\n",
    "        \"end-to-end pipeline works well\",\n",
    "        \"we completed week one successfully\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nProcessing more texts...\")\n",
    "    advanced_pipeline.process_batch(more_texts, max_length=16, padding=True)\n",
    "    \n",
    "    # Show updated statistics\n",
    "    updated_stats = advanced_pipeline.get_statistics()\n",
    "    print(f\"\\nUpdated Pipeline Statistics:\")\n",
    "    for key, value in updated_stats.items():\n",
    "        if key != 'token_frequency' and key != 'top_tokens':\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Show top tokens\n",
    "    if 'top_tokens' in updated_stats:\n",
    "        print(\"\\nTop tokens:\")\n",
    "        for token, count in updated_stats['top_tokens'].items():\n",
    "            print(f\"  '{token}': {count}\")\n",
    "    \n",
    "    # Visualize token frequency\n",
    "    if 'top_tokens' in updated_stats:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        tokens = list(updated_stats['top_tokens'].keys())\n",
    "        counts = list(updated_stats['top_tokens'].values())\n",
    "        \n",
    "        plt.bar(tokens, counts)\n",
    "        plt.title('Top Token Frequencies')\n",
    "        plt.xlabel('Token')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return advanced_pipeline, updated_stats\n",
    "\n",
    "# Test advanced pipeline\n",
    "advanced_pipeline, stats = test_advanced_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Saving and Loading the Pipeline\n",
    "\n",
    "Let's test saving and loading the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pipeline\n",
    "advanced_pipeline.save_pipeline('transformer_pipeline.pt')\n",
    "\n",
    "# Load the pipeline (in a real scenario)\n",
    "def load_pipeline(path):\n",
    "    \"\"\"Load a saved pipeline.\"\"\"\n",
    "    saved_data = torch.load(path)\n",
    "    config = saved_data['config']\n",
    "    \n",
    "    # Create a new pipeline with the saved configuration\n",
    "    pipeline = AdvancedTransformerPipeline(\n",
    "        vocab_size=config['vocab_size'],\n",
    "        embedding_dim=config['embedding_dim'],\n",
    "        max_seq_len=config['max_seq_len']\n",
    "    )\n",
    "    \n",
    "    # Restore vocabulary\n",
    "    pipeline.vocab = config['vocab']\n",
    "    pipeline.vocab_reverse = {idx: token for token, idx in pipeline.vocab.items()}\n",
    "    \n",
    "    # Restore weights\n",
    "    pipeline.embedding.load_state_dict(saved_data['embedding_weights'])\n",
    "    if saved_data['pos_encoding_weights'] and hasattr(pipeline.pos_encoding, 'load_state_dict'):\n",
    "        pipeline.pos_encoding.load_state_dict(saved_data['pos_encoding_weights'])\n",
    "    \n",
    "    # Restore statistics\n",
    "    pipeline.statistics = config['statistics']\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "# Note: In this notebook, we won't actually load the pipeline to avoid\n",
    "# file system issues, but in a real scenario, you would use:\n",
    "# loaded_pipeline = load_pipeline('transformer_pipeline.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary and Next Steps\n",
    "\n",
    "In this notebook, we've extended our transformer preprocessing pipeline with advanced features:\n",
    "\n",
    "1. **Batch Processing**: Efficiently handling multiple texts at once\n",
    "2. **Statistics Tracking**: Monitoring token frequencies, sequence lengths, and unknown token usage\n",
    "3. **Serialization**: Saving and loading pipeline configurations\n",
    "\n",
    "These features make our pipeline more practical for real-world applications. The complete pipeline now provides a robust foundation for transformer models, integrating all the components we've learned about in Week 1.\n",
    "\n",
    "In Week 2, we'll build on this foundation to implement the transformer architecture itself, including self-attention mechanisms and encoder/decoder blocks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
