{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 5: End-to-End Pipeline - Part 2\n",
    "\n",
    "In this notebook, we'll continue exploring our transformer preprocessing pipeline with performance benchmarking and integration with the docs/llm.md flow.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Dict, Optional, Union\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reimplementing Required Classes\n",
    "\n",
    "Let's first reimplement our positional encoding and pipeline classes from part 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class LearnedPositionalEmbedding(nn.Module):\n",
    "    \"\"\"Learned positional embedding implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_seq_len, d_model):\n",
    "        super().__init__()\n",
    "        self.position_embeddings = nn.Embedding(max_seq_len, d_model)\n",
    "        nn.init.normal_(self.position_embeddings.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Add learned positional embeddings.\"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand(x.size(0), -1)\n",
    "        return x + self.position_embeddings(position_ids)\n",
    "\n",
    "class TransformerPreprocessingPipeline:\n",
    "    \"\"\"Complete preprocessing pipeline for transformer models.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size: int = 1000,\n",
    "                 embedding_dim: int = 128,\n",
    "                 max_seq_len: int = 512,\n",
    "                 pad_token: str = \"[PAD]\",\n",
    "                 cls_token: str = \"[CLS]\",\n",
    "                 sep_token: str = \"[SEP]\",\n",
    "                 unk_token: str = \"[UNK]\",\n",
    "                 pos_encoding_type: str = \"sinusoidal\"):\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Special tokens\n",
    "        self.pad_token = pad_token\n",
    "        self.cls_token = cls_token\n",
    "        self.sep_token = sep_token\n",
    "        self.unk_token = unk_token\n",
    "        \n",
    "        # Initialize components\n",
    "        self._init_tokenizer()\n",
    "        self._init_embeddings()\n",
    "        self._init_positional_encoding(pos_encoding_type)\n",
    "    \n",
    "    def _init_tokenizer(self):\n",
    "        \"\"\"Initialize tokenizer with basic vocabulary.\"\"\"\n",
    "        # Create basic vocabulary\n",
    "        special_tokens = [self.pad_token, self.unk_token, self.cls_token, self.sep_token]\n",
    "        common_words = [\"the\", \"a\", \"an\", \"and\", \"or\", \"but\", \"in\", \"on\", \"at\", \"to\", \"for\", \"of\", \"with\"]\n",
    "        chars = list(\"abcdefghijklmnopqrstuvwxyz0123456789.,!?;:()[]{}\\\"'-_/\\\\@#$%^&*+=<>|`~\")\n",
    "        \n",
    "        vocab_list = special_tokens + common_words + chars\n",
    "        vocab_list = vocab_list[:self.vocab_size]  # Limit to vocab_size\n",
    "        \n",
    "        self.vocab = {token: idx for idx, token in enumerate(vocab_list)}\n",
    "        self.vocab_reverse = {idx: token for token, idx in self.vocab.items()}\n",
    "        \n",
    "        # Special token IDs\n",
    "        self.pad_token_id = self.vocab[self.pad_token]\n",
    "        self.cls_token_id = self.vocab[self.cls_token]\n",
    "        self.sep_token_id = self.vocab[self.sep_token]\n",
    "        self.unk_token_id = self.vocab[self.unk_token]\n",
    "    \n",
    "    def _init_embeddings(self):\n",
    "        \"\"\"Initialize embedding layer.\"\"\"\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, \n",
    "                                    padding_idx=self.pad_token_id)\n",
    "        nn.init.normal_(self.embedding.weight, mean=0, std=0.1)\n",
    "        \n",
    "        # Zero out padding token embedding\n",
    "        self.embedding.weight.data[self.pad_token_id].fill_(0)\n",
    "    \n",
    "    def _init_positional_encoding(self, pos_type):\n",
    "        \"\"\"Initialize positional encoding.\"\"\"\n",
    "        if pos_type == \"sinusoidal\":\n",
    "            self.pos_encoding = SinusoidalPositionalEncoding(\n",
    "                self.embedding_dim, self.max_seq_len)\n",
    "        elif pos_type == \"learned\":\n",
    "            self.pos_encoding = LearnedPositionalEmbedding(\n",
    "                self.max_seq_len, self.embedding_dim)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown positional encoding type: {pos_type}\")\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Simple tokenization (character-level for demo).\"\"\"\n",
    "        # Normalize text\n",
    "        text = text.lower().strip()\n",
    "        \n",
    "        # Simple character-level tokenization\n",
    "        tokens = []\n",
    "        for char in text:\n",
    "            if char in self.vocab:\n",
    "                tokens.append(char)\n",
    "            else:\n",
    "                tokens.append(self.unk_token)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def encode_text(self, text: str, \n",
    "                   add_special_tokens: bool = True,\n",
    "                   max_length: Optional[int] = None,\n",
    "                   padding: bool = True,\n",
    "                   truncation: bool = True) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Complete encoding pipeline.\"\"\"\n",
    "        \n",
    "        if max_length is None:\n",
    "            max_length = self.max_seq_len\n",
    "        \n",
    "        # Step 1: Tokenization\n",
    "        tokens = self.tokenize(text)\n",
    "        \n",
    "        # Step 2: Add special tokens\n",
    "        if add_special_tokens:\n",
    "            tokens = [self.cls_token] + tokens + [self.sep_token]\n",
    "        \n",
    "        # Step 3: Truncation\n",
    "        if truncation and len(tokens) > max_length:\n",
    "            if add_special_tokens:\n",
    "                tokens = tokens[:max_length-1] + [self.sep_token]\n",
    "            else:\n",
    "                tokens = tokens[:max_length]\n",
    "        \n",
    "        # Step 4: Convert to IDs\n",
    "        input_ids = [self.vocab.get(token, self.unk_token_id) for token in tokens]\n",
    "        \n",
    "        # Step 5: Create attention mask\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        \n",
    "        # Step 6: Padding\n",
    "        if padding and len(input_ids) < max_length:\n",
    "            padding_length = max_length - len(input_ids)\n",
    "            input_ids.extend([self.pad_token_id] * padding_length)\n",
    "            attention_mask.extend([0] * padding_length)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'tokens': tokens\n",
    "        }\n",
    "    \n",
    "    def get_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get token embeddings.\"\"\"\n",
    "        return self.embedding(input_ids)\n",
    "    \n",
    "    def add_positional_encoding(self, embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Add positional encoding to embeddings.\"\"\"\n",
    "        return self.pos_encoding(embeddings)\n",
    "    \n",
    "    def process_text(self, text: str, **kwargs) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Complete end-to-end processing.\"\"\"\n",
    "        # Step 1: Encode text to tokens and IDs\n",
    "        encoded = self.encode_text(text, **kwargs)\n",
    "        \n",
    "        # Step 2: Get embeddings\n",
    "        embeddings = self.get_embeddings(encoded['input_ids'].unsqueeze(0))\n",
    "        \n",
    "        # Step 3: Add positional encoding\n",
    "        embeddings_with_pos = self.add_positional_encoding(embeddings)\n",
    "        \n",
    "        # Step 4: Scale embeddings (as in original Transformer)\n",
    "        embeddings_with_pos = embeddings_with_pos * np.sqrt(self.embedding_dim)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'],\n",
    "            'attention_mask': encoded['attention_mask'],\n",
    "            'embeddings': embeddings_with_pos.squeeze(0),\n",
    "            'tokens': encoded['tokens']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance Benchmarking\n",
    "\n",
    "Let's benchmark the performance of our pipeline with different text lengths and batch sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_pipeline():\n",
    "    \"\"\"Benchmark pipeline performance.\"\"\"\n",
    "    \n",
    "    pipeline = TransformerPreprocessingPipeline(\n",
    "        vocab_size=1000,\n",
    "        embedding_dim=128,\n",
    "        max_seq_len=64\n",
    "    )\n",
    "    \n",
    "    # Test texts of different lengths\n",
    "    test_cases = [\n",
    "        (\"Short\", \"hello\"),\n",
    "        (\"Medium\", \"this is a medium length sentence for testing\"),\n",
    "        (\"Long\", \"this is a much longer sentence that should test the performance of our pipeline with more tokens and processing overhead\" * 2),\n",
    "    ]\n",
    "    \n",
    "    batch_sizes = [1, 8, 32]\n",
    "    \n",
    "    print(\"Pipeline Performance Benchmark\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, text in test_cases:\n",
    "        print(f\"\\n{name} text ({len(text)} chars):\")\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            # Prepare batch\n",
    "            texts = [text] * batch_size\n",
    "            \n",
    "            # Benchmark\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for text_item in texts:\n",
    "                result = pipeline.process_text(text_item, max_length=32)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            total_time = end_time - start_time\n",
    "            time_per_text = total_time / batch_size\n",
    "            texts_per_second = batch_size / total_time\n",
    "            \n",
    "            print(f\"  Batch size {batch_size:2d}: {time_per_text*1000:6.2f}ms/text | {texts_per_second:6.1f} texts/sec\")\n",
    "            \n",
    "            results.append({\n",
    "                'text_type': name,\n",
    "                'text_length': len(text),\n",
    "                'batch_size': batch_size,\n",
    "                'time_per_text_ms': time_per_text * 1000,\n",
    "                'texts_per_second': texts_per_second\n",
    "            })\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Group by text type\n",
    "    for name in [\"Short\", \"Medium\", \"Long\"]:\n",
    "        data = [r for r in results if r['text_type'] == name]\n",
    "        batch_sizes = [r['batch_size'] for r in data]\n",
    "        times = [r['time_per_text_ms'] for r in data]\n",
    "        plt.plot(batch_sizes, times, 'o-', label=name)\n",
    "    \n",
    "    plt.title('Processing Time per Text vs Batch Size')\n",
    "    plt.xlabel('Batch Size')\n",
    "    plt.ylabel('Time per Text (ms)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_results = benchmark_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integration with docs/llm.md Flow\n",
    "\n",
    "Let's demonstrate how our pipeline matches the flow described in docs/llm.md:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_llm_md_integration():\n",
    "    \"\"\"Demonstrate how pipeline matches docs/llm.md flow.\"\"\"\n",
    "    \n",
    "    print(\"Integration with docs/llm.md Flow\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    pipeline = TransformerPreprocessingPipeline(\n",
    "        vocab_size=500,\n",
    "        embedding_dim=128,\n",
    "        max_seq_len=32\n",
    "    )\n",
    "    \n",
    "    text = \"Natural language processing with transformers\"\n",
    "    \n",
    "    print(f\"Input Text: '{text}'\")\n",
    "    print(\"\\nFollowing docs/llm.md flow:\")\n",
    "    \n",
    "    # Step 1: Input Embeddings (from docs/llm.md)\n",
    "    print(\"\\n1. Input Embeddings:\")\n",
    "    encoded = pipeline.encode_text(text, max_length=16)\n",
    "    embeddings = pipeline.get_embeddings(encoded['input_ids'].unsqueeze(0))\n",
    "    print(f\"   Token embeddings shape: {embeddings.shape}\")\n",
    "    print(f\"   Sample embedding (first token): {embeddings[0, 0, :5].detach().numpy()}\")\n",
    "    \n",
    "    # Step 2: Positional Encoding (from docs/llm.md)\n",
    "    print(\"\\n2. Positional Encoding:\")\n",
    "    embeddings_with_pos = pipeline.add_positional_encoding(embeddings)\n",
    "    print(f\"   With positional encoding shape: {embeddings_with_pos.shape}\")\n",
    "    print(f\"   Position encoding added: ✓\")\n",
    "    \n",
    "    # Step 3: Ready for Transformer Blocks\n",
    "    print(\"\\n3. Ready for Transformer Processing:\")\n",
    "    print(\"   ✓ Input Embeddings: Created dense vector representations\")\n",
    "    print(\"   ✓ Positional Encoding: Added sequence order information\")\n",
    "    print(\"   ✓ Attention Mask: Created for padding tokens\")\n",
    "    print(\"   → Ready for Multi-Head Attention (Week 2)\")\n",
    "    \n",
    "    # Show the complete flow\n",
    "    result = pipeline.process_text(text, max_length=16)\n",
    "    \n",
    "    print(f\"\\nFinal Output Summary:\")\n",
    "    print(f\"   Input IDs: {result['input_ids'].shape}\")\n",
    "    print(f\"   Embeddings: {result['embeddings'].shape}\")\n",
    "    print(f\"   Attention Mask: {result['attention_mask'].shape}\")\n",
    "    print(f\"   Ready for: Encoder/Decoder Transformer Blocks\")\n",
    "    \n",
    "    # Visualize the flow\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create a horizontal flow diagram\n",
    "    steps = ['Raw Text', 'Tokenization', 'Token IDs', 'Embeddings', 'Positional\\nEncoding', 'Model Input']\n",
    "    x = np.arange(len(steps))\n",
    "    y = np.zeros_like(x)\n",
    "    \n",
    "    plt.plot(x, y, 'o-', markersize=15, linewidth=2)\n",
    "    \n",
    "    for i, step in enumerate(steps):\n",
    "        plt.text(i, 0.1, step, ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.title('Transformer Preprocessing Pipeline Flow')\n",
    "    plt.xlim(-0.5, len(steps) - 0.5)\n",
    "    plt.ylim(-0.5, 0.5)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Demonstrate integration\n",
    "integration_result = demonstrate_llm_md_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary and Next Steps\n",
    "\n",
    "In this notebook, we've benchmarked our transformer preprocessing pipeline and demonstrated how it integrates with the flow described in docs/llm.md. The pipeline efficiently converts raw text into transformer-ready inputs through a series of steps:\n",
    "\n",
    "1. Text normalization and tokenization\n",
    "2. Converting tokens to IDs\n",
    "3. Adding special tokens and padding\n",
    "4. Creating embeddings\n",
    "5. Adding positional encoding\n",
    "6. Creating attention masks\n",
    "\n",
    "This completes our Week 1 journey through the fundamentals of transformer preprocessing. In Week 2, we'll build on this foundation to implement the transformer architecture itself, including self-attention mechanisms and encoder/decoder blocks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
