{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 Day 1: Text Normalization and Tokenization - Practical Implementation\n",
    "\n",
    "This notebook contains all the practical code examples for Day 1, implementing text normalization and tokenization techniques covered in the theoretical guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install tiktoken transformers unidecode matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer\n",
    "from unidecode import unidecode\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Normalization Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unicode normalization example\n",
    "text = \"Café – AÑO 2025 — coöperate\"\n",
    "print(f\"Original: {text}\")\n",
    "\n",
    "# Normalize to NFC (compose characters)\n",
    "text_nfc = unicodedata.normalize(\"NFC\", text)\n",
    "print(f\"NFC: {text_nfc}\")\n",
    "\n",
    "# Normalize to NFKC (compatibility)\n",
    "text_nfkc = unicodedata.normalize(\"NFKC\", text)\n",
    "print(f\"NFKC: {text_nfkc}\")\n",
    "\n",
    "# Show byte differences\n",
    "print(f\"\\nOriginal bytes: {text.encode('utf-8')}\")\n",
    "print(f\"NFC bytes: {text_nfc.encode('utf-8')}\")\n",
    "print(f\"NFKC bytes: {text_nfkc.encode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete normalization pipeline\n",
    "def normalize_text(text, method='conservative'):\n",
    "    \"\"\"Normalize text with different strategies.\"\"\"\n",
    "    if method == 'aggressive':\n",
    "        # Aggressive normalization\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "        text = text.lower()\n",
    "        text = unidecode(text)  # Remove all diacritics\n",
    "    elif method == 'conservative':\n",
    "        # Conservative normalization\n",
    "        text = unicodedata.normalize(\"NFC\", text)\n",
    "        text = text.lower()\n",
    "    \n",
    "    # Common whitespace normalization\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# Test different normalization strategies\n",
    "test_text = \"Café — AÑO — coöperate   with\\tmultiple\\nwhitespace\"\n",
    "print(f\"Original: '{test_text}'\")\n",
    "print(f\"Conservative: '{normalize_text(test_text, 'conservative')}'\")\n",
    "print(f\"Aggressive: '{normalize_text(test_text, 'aggressive')}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Byte-Pair Encoding (BPE) with tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 BPE tokenizer example\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"Café – internationalization in 2025!\"\n",
    "\n",
    "# Encode text to token IDs\n",
    "ids = enc.encode(text)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Token IDs: {ids}\")\n",
    "print(f\"Decoded: {enc.decode(ids)}\")\n",
    "\n",
    "# Show individual tokens\n",
    "tokens = [enc.decode([id]) for id in ids]\n",
    "print(f\"Individual tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze tokenization patterns\n",
    "test_words = [\n",
    "    \"internationalization\",\n",
    "    \"unhappiness\", \n",
    "    \"preprocessing\",\n",
    "    \"tokenization\",\n",
    "    \"unchartedness\"\n",
    "]\n",
    "\n",
    "print(\"BPE Tokenization Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "for word in test_words:\n",
    "    ids = enc.encode(word)\n",
    "    tokens = [enc.decode([id]) for id in ids]\n",
    "    print(f\"{word:20} → {tokens} ({len(tokens)} tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. WordPiece with BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT WordPiece tokenizer\n",
    "bert_tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Internationalization is complicated!\"\n",
    "\n",
    "# Tokenize text\n",
    "tokens = bert_tok.tokenize(text)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Full encoding with special tokens\n",
    "encoded = bert_tok(text, return_tensors=\"pt\")\n",
    "print(f\"\\nInput IDs: {encoded['input_ids']}\")\n",
    "print(f\"Attention Mask: {encoded['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare WordPiece vs BPE tokenization\n",
    "comparison_words = [\n",
    "    \"internationalization\",\n",
    "    \"unhappiness\",\n",
    "    \"preprocessing\", \n",
    "    \"tokenization\"\n",
    "]\n",
    "\n",
    "print(\"Tokenization Comparison: WordPiece vs BPE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for word in comparison_words:\n",
    "    # WordPiece (BERT)\n",
    "    wp_tokens = bert_tok.tokenize(word)\n",
    "    \n",
    "    # BPE (GPT-2)\n",
    "    bpe_ids = enc.encode(word)\n",
    "    bpe_tokens = [enc.decode([id]) for id in bpe_ids]\n",
    "    \n",
    "    print(f\"\\n{word}:\")\n",
    "    print(f\"  WordPiece: {wp_tokens} ({len(wp_tokens)} tokens)\")\n",
    "    print(f\"  BPE:       {bpe_tokens} ({len(bpe_tokens)} tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenization Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze vocabulary sizes and token distributions\n",
    "sample_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. \n",
    "Internationalization and localization are important for global applications.\n",
    "Machine learning models require careful preprocessing of textual data.\n",
    "Tokenization strategies significantly impact model performance and efficiency.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize with different methods\n",
    "word_tokens = sample_text.split()\n",
    "char_tokens = list(sample_text.replace(' ', '▁'))  # Use ▁ for spaces\n",
    "bpe_tokens = [enc.decode([id]) for id in enc.encode(sample_text)]\n",
    "wp_tokens = bert_tok.tokenize(sample_text)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = {\n",
    "    'Method': ['Word-level', 'Character-level', 'BPE', 'WordPiece'],\n",
    "    'Token Count': [len(word_tokens), len(char_tokens), len(bpe_tokens), len(wp_tokens)],\n",
    "    'Unique Tokens': [len(set(word_tokens)), len(set(char_tokens)), \n",
    "                     len(set(bpe_tokens)), len(set(wp_tokens))]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"Tokenization Strategy Comparison:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tokenization comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Token count comparison\n",
    "ax1.bar(df['Method'], df['Token Count'], color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])\n",
    "ax1.set_title('Total Token Count by Method')\n",
    "ax1.set_ylabel('Number of Tokens')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Unique tokens comparison\n",
    "ax2.bar(df['Method'], df['Unique Tokens'], color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])\n",
    "ax2.set_title('Unique Token Count by Method')\n",
    "ax2.set_ylabel('Number of Unique Tokens')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Analyze OOV rates\n",
    "def calculate_oov_rate(text, tokenizer_type='bpe'):\n",
    "    \"\"\"Calculate out-of-vocabulary rate for different tokenizers.\"\"\"\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    if tokenizer_type == 'bpe':\n",
    "        # BPE has no OOV due to byte-level encoding\n",
    "        return 0.0\n",
    "    elif tokenizer_type == 'wordpiece':\n",
    "        # WordPiece uses [UNK] for unknown tokens\n",
    "        tokens = bert_tok.tokenize(text)\n",
    "        unk_count = tokens.count('[UNK]')\n",
    "        return unk_count / len(tokens) if tokens else 0.0\n",
    "    elif tokenizer_type == 'word':\n",
    "        # Simulate word-level with limited vocabulary\n",
    "        common_words = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'])\n",
    "        oov_count = sum(1 for word in words if word not in common_words)\n",
    "        return oov_count / len(words) if words else 0.0\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "# Test with different text types\n",
    "texts = {\n",
    "    'Common': \"The quick brown fox jumps over the lazy dog.\",\n",
    "    'Technical': \"Internationalization requires sophisticated preprocessing algorithms.\",\n",
    "    'Rare words': \"The sesquipedalian lexicographer's perspicacious observations.\"\n",
    "}\n",
    "\n",
    "print(\"OOV Rate Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "for text_type, text in texts.items():\n",
    "    print(f\"\\n{text_type} text: '{text}'\")\n",
    "    for tok_type in ['word', 'wordpiece', 'bpe']:\n",
    "        oov_rate = calculate_oov_rate(text, tok_type)\n",
    "        print(f\"  {tok_type:10}: {oov_rate:.2%} OOV rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Token length analysis\n",
    "def analyze_token_lengths(text, method='bpe'):\n",
    "    \"\"\"Analyze token length distribution.\"\"\"\n",
    "    if method == 'bpe':\n",
    "        tokens = [enc.decode([id]) for id in enc.encode(text)]\n",
    "    elif method == 'wordpiece':\n",
    "        tokens = bert_tok.tokenize(text)\n",
    "    elif method == 'word':\n",
    "        tokens = text.split()\n",
    "    else:\n",
    "        tokens = list(text)\n",
    "    \n",
    "    lengths = [len(token) for token in tokens]\n",
    "    return lengths, tokens\n",
    "\n",
    "# Analyze token lengths for different methods\n",
    "test_text = \"Internationalization and preprocessing are fundamental components of natural language processing pipelines.\"\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "methods = ['word', 'char', 'bpe', 'wordpiece']\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold']\n",
    "\n",
    "for i, (method, color) in enumerate(zip(methods, colors)):\n",
    "    ax = axes[i//2, i%2]\n",
    "    \n",
    "    if method == 'char':\n",
    "        lengths = [1] * len(test_text.replace(' ', ''))\n",
    "    else:\n",
    "        lengths, _ = analyze_token_lengths(test_text, method)\n",
    "    \n",
    "    ax.hist(lengths, bins=range(1, max(lengths)+2), alpha=0.7, color=color, edgecolor='black')\n",
    "    ax.set_title(f'{method.title()} Token Lengths')\n",
    "    ax.set_xlabel('Token Length (characters)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Custom Tokenization Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple BPE implementation for educational purposes\n",
    "class SimpleBPE:\n",
    "    def __init__(self, vocab_size=1000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}\n",
    "        self.merges = []\n",
    "    \n",
    "    def get_pairs(self, word):\n",
    "        \"\"\"Get all adjacent pairs in a word.\"\"\"\n",
    "        pairs = set()\n",
    "        prev_char = word[0]\n",
    "        for char in word[1:]:\n",
    "            pairs.add((prev_char, char))\n",
    "            prev_char = char\n",
    "        return pairs\n",
    "    \n",
    "    def train(self, corpus):\n",
    "        \"\"\"Train BPE on a corpus.\"\"\"\n",
    "        # Initialize with characters\n",
    "        vocab = Counter()\n",
    "        for word in corpus:\n",
    "            word_tokens = list(word) + ['</w>']\n",
    "            vocab[' '.join(word_tokens)] += 1\n",
    "        \n",
    "        # Learn merges\n",
    "        for i in range(self.vocab_size - len(set(''.join(corpus)))):\n",
    "            pairs = Counter()\n",
    "            for word, freq in vocab.items():\n",
    "                symbols = word.split()\n",
    "                for pair in self.get_pairs(symbols):\n",
    "                    pairs[pair] += freq\n",
    "            \n",
    "            if not pairs:\n",
    "                break\n",
    "                \n",
    "            best_pair = pairs.most_common(1)[0][0]\n",
    "            self.merges.append(best_pair)\n",
    "            \n",
    "            # Apply merge\n",
    "            new_vocab = {}\n",
    "            bigram = ' '.join(best_pair)\n",
    "            replacement = ''.join(best_pair)\n",
    "            \n",
    "            for word in vocab:\n",
    "                new_word = word.replace(bigram, replacement)\n",
    "                new_vocab[new_word] = vocab[word]\n",
    "            vocab = new_vocab\n",
    "        \n",
    "        # Build final vocabulary\n",
    "        self.vocab = set()\n",
    "        for word in vocab:\n",
    "            self.vocab.update(word.split())\n",
    "    \n",
    "    def encode(self, word):\n",
    "        \"\"\"Encode a word using learned BPE.\"\"\"\n",
    "        word_tokens = list(word) + ['</w>']\n",
    "        \n",
    "        for merge in self.merges:\n",
    "            i = 0\n",
    "            while i < len(word_tokens) - 1:\n",
    "                if (word_tokens[i], word_tokens[i+1]) == merge:\n",
    "                    word_tokens = word_tokens[:i] + [''.join(merge)] + word_tokens[i+2:]\n",
    "                i += 1\n",
    "        \n",
    "        return word_tokens\n",
    "\n",
    "# Test simple BPE\n",
    "corpus = ['low', 'lower', 'lowest', 'newer', 'wider']\n",
    "bpe = SimpleBPE(vocab_size=20)\n",
    "bpe.train(corpus)\n",
    "\n",
    "print(\"Simple BPE Training Results:\")\n",
    "print(f\"Learned merges: {bpe.merges[:5]}...\")  # Show first 5 merges\n",
    "print(f\"Vocabulary size: {len(bpe.vocab)}\")\n",
    "\n",
    "# Test encoding\n",
    "test_words = ['low', 'lower', 'lowest', 'new']\n",
    "for word in test_words:\n",
    "    encoded = bpe.encode(word)\n",
    "    print(f\"{word:8} → {encoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Text Normalization**: Unicode normalization, case handling, and whitespace processing\n",
    "2. **BPE Tokenization**: Using tiktoken for GPT-style byte-level BPE\n",
    "3. **WordPiece Tokenization**: Using transformers library for BERT-style tokenization\n",
    "4. **Comparative Analysis**: Token counts, vocabulary sizes, and OOV rates\n",
    "5. **Visualization**: Understanding tokenization patterns through plots\n",
    "6. **Custom Implementation**: Simple BPE algorithm for educational purposes\n",
    "\n",
    "**Next Steps**: Practice with different tokenizers, experiment with vocabulary sizes, and explore domain-specific tokenization challenges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
