{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: Advanced Tokenization - Part 3: Experiments and Optimization\n",
    "\n",
    "This notebook contains the final part of our tokenization exploration, focusing on vocabulary size experiments, domain-specific tokenizers, and performance benchmarking.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, WordPiece\n",
    "from tokenizers.trainers import BpeTrainer, WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.normalizers import Sequence, NFD, Lowercase, StripAccents\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vocabulary Size Experiments\n",
    "\n",
    "Let's explore how vocabulary size affects tokenization efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bpe_tokenizer_simple(texts, vocab_size=1000):\n",
    "    \"\"\"Create and train a simple BPE tokenizer.\"\"\"\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "    )\n",
    "    tokenizer.train_from_iterator(texts, trainer)\n",
    "    return tokenizer\n",
    "\n",
    "def vocabulary_size_experiment():\n",
    "    \"\"\"Experiment with different vocabulary sizes.\"\"\"\n",
    "    \n",
    "    # Sample training data\n",
    "    training_data = [\n",
    "        \"Natural language processing is fascinating.\",\n",
    "        \"Machine learning models require careful preprocessing.\",\n",
    "        \"Tokenization splits text into meaningful units.\",\n",
    "        \"Subword tokenization handles out-of-vocabulary words.\",\n",
    "        \"BERT uses WordPiece tokenization for efficiency.\",\n",
    "        \"GPT models use BPE tokenization for text generation.\",\n",
    "        \"Transformer architectures have revolutionized NLP.\",\n",
    "        \"Attention mechanisms allow models to focus on relevant parts of input.\",\n",
    "        \"Fine-tuning pre-trained models improves performance on specific tasks.\",\n",
    "        \"Transfer learning leverages knowledge from one domain to another.\"\n",
    "    ] * 20  # Repeat for more training data\n",
    "    \n",
    "    vocab_sizes = [50, 100, 200, 500, 1000]\n",
    "    results = []\n",
    "    \n",
    "    test_sentences = [\n",
    "        \"Preprocessing tokenization algorithms efficiently.\",\n",
    "        \"Natural language understanding requires context.\",\n",
    "        \"Transformers process sequences in parallel.\"\n",
    "    ]\n",
    "    \n",
    "    for vocab_size in vocab_sizes:\n",
    "        tokenizer = create_bpe_tokenizer_simple(training_data, vocab_size)\n",
    "        actual_vocab_size = tokenizer.get_vocab_size()\n",
    "        \n",
    "        for sentence in test_sentences:\n",
    "            encoding = tokenizer.encode(sentence)\n",
    "            \n",
    "            results.append({\n",
    "                'Vocabulary Size': vocab_size,\n",
    "                'Actual Vocab Size': actual_vocab_size,\n",
    "                'Sentence': sentence,\n",
    "                'Sequence Length': len(encoding.ids),\n",
    "                'Tokens': encoding.tokens,\n",
    "                'Compression Ratio': len(sentence) / len(encoding.ids)\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df, x='Vocabulary Size', y='Sequence Length', hue='Sentence', marker='o')\n",
    "    plt.title('Vocabulary Size vs. Sequence Length')\n",
    "    plt.xlabel('Vocabulary Size')\n",
    "    plt.ylabel('Sequence Length (tokens)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df, x='Vocabulary Size', y='Compression Ratio', hue='Sentence', marker='o')\n",
    "    plt.title('Vocabulary Size vs. Compression Ratio')\n",
    "    plt.xlabel('Vocabulary Size')\n",
    "    plt.ylabel('Compression Ratio (chars/token)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show detailed results\n",
    "    print(\"Vocabulary Size vs Sequence Length:\")\n",
    "    for vocab_size in vocab_sizes:\n",
    "        subset = df[df['Vocabulary Size'] == vocab_size]\n",
    "        print(f\"\\nVocab Size: {vocab_size}\")\n",
    "        for _, row in subset.iterrows():\n",
    "            print(f\"  Sentence: '{row['Sentence']}'\")\n",
    "            print(f\"  Sequence Length: {row['Sequence Length']}\")\n",
    "            print(f\"  Tokens: {row['Tokens']}\")\n",
    "            print(f\"  Compression Ratio: {row['Compression Ratio']:.2f}\")\n",
    "            print()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run the experiment\n",
    "vocab_experiment_df = vocabulary_size_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Domain-Specific Tokenizers\n",
    "\n",
    "Let's create tokenizers optimized for specific domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_domain_tokenizer(domain_texts, domain_name, vocab_size=5000):\n",
    "    \"\"\"Create a tokenizer optimized for a specific domain.\"\"\"\n",
    "    \n",
    "    # Initialize with domain-specific settings\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    \n",
    "    # Domain-specific normalization\n",
    "    if domain_name == \"code\":\n",
    "        # Preserve case for code\n",
    "        tokenizer.normalizer = NFD()\n",
    "    elif domain_name == \"biomedical\":\n",
    "        # Preserve scientific notation\n",
    "        tokenizer.normalizer = Sequence([NFD(), Lowercase()])\n",
    "    else:\n",
    "        tokenizer.normalizer = Sequence([NFD(), Lowercase()])\n",
    "    \n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    \n",
    "    # Domain-specific special tokens\n",
    "    special_tokens = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "    \n",
    "    if domain_name == \"code\":\n",
    "        special_tokens.extend([\"[FUNC]\", \"[VAR]\", \"[COMMENT]\"])\n",
    "    elif domain_name == \"biomedical\":\n",
    "        special_tokens.extend([\"[GENE]\", \"[PROTEIN]\", \"[DRUG]\"])\n",
    "    \n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=special_tokens,\n",
    "        min_frequency=2\n",
    "    )\n",
    "    \n",
    "    # Train on domain data\n",
    "    tokenizer.train_from_iterator(domain_texts, trainer)\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "# Example domain-specific texts\n",
    "code_texts = [\n",
    "    \"def tokenize_text(text): return text.split()\",\n",
    "    \"import numpy as np\",\n",
    "    \"class BertTokenizer: def __init__(self): pass\",\n",
    "    \"for i in range(len(tokens)): print(tokens[i])\",\n",
    "    \"if __name__ == '__main__': main()\",\n",
    "    \"def process_batch(batch_size=32, max_length=512):\",\n",
    "    \"model.train(); optimizer.zero_grad()\",\n",
    "    \"loss = criterion(outputs, targets)\",\n",
    "    \"with torch.no_grad(): model.eval()\",\n",
    "    \"return {k: v.item() for k, v in results.items()}\"\n",
    "]\n",
    "\n",
    "biomedical_texts = [\n",
    "    \"The patient was diagnosed with COVID-19.\",\n",
    "    \"BRCA1 and BRCA2 are human genes that produce tumor suppressor proteins.\",\n",
    "    \"The drug dosage was 2.5mg administered twice daily.\",\n",
    "    \"Protein kinases are enzymes that modify other proteins.\",\n",
    "    \"DNA sequencing revealed a mutation in the TP53 gene.\",\n",
    "    \"The patient's hemoglobin A1c level was 7.2%.\",\n",
    "    \"Monoclonal antibodies target specific antigens.\",\n",
    "    \"The study examined the efficacy of mRNA vaccines.\",\n",
    "    \"Cytokine storm is a severe immune reaction.\",\n",
    "    \"PCR tests detect viral genetic material in samples.\"\n",
    "]\n",
    "\n",
    "general_texts = training_texts  # Reuse our previous training data\n",
    "\n",
    "# Create domain-specific tokenizers\n",
    "code_tokenizer = create_domain_tokenizer(code_texts, \"code\", vocab_size=100)\n",
    "biomedical_tokenizer = create_domain_tokenizer(biomedical_texts, \"biomedical\", vocab_size=100)\n",
    "general_tokenizer = create_domain_tokenizer(general_texts, \"general\", vocab_size=100)\n",
    "\n",
    "# Test texts for each domain\n",
    "test_texts = {\n",
    "    \"code\": \"def process_tokens(tokens): return [t.lower() for t in tokens]\",\n",
    "    \"biomedical\": \"The study found that SARS-CoV-2 binds to ACE2 receptors with high affinity.\",\n",
    "    \"general\": \"Natural language processing models can understand and generate human language.\"\n",
    "}\n",
    "\n",
    "# Compare tokenization across domains\n",
    "tokenizers = {\n",
    "    \"Code Tokenizer\": code_tokenizer,\n",
    "    \"Biomedical Tokenizer\": biomedical_tokenizer,\n",
    "    \"General Tokenizer\": general_tokenizer\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for domain, text in test_texts.items():\n",
    "    print(f\"\\n{domain.upper()} TEXT: '{text}'\")\n",
    "    \n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        encoding = tokenizer.encode(text)\n",
    "        \n",
    "        results.append({\n",
    "            'Domain': domain,\n",
    "            'Tokenizer': name,\n",
    "            'Text': text,\n",
    "            'Token Count': len(encoding.ids),\n",
    "            'Tokens': encoding.tokens\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Token count: {len(encoding.ids)}\")\n",
    "        print(f\"  Tokens: {encoding.tokens}\")\n",
    "\n",
    "# Create DataFrame and visualize\n",
    "domain_df = pd.DataFrame(results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=domain_df, x='Domain', y='Token Count', hue='Tokenizer')\n",
    "plt.title('Token Count by Domain and Tokenizer')\n",
    "plt.xlabel('Text Domain')\n",
    "plt.ylabel('Number of Tokens')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Tokenizer')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Benchmarking\n",
    "\n",
    "Let's benchmark the performance of different tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_tokenizers(texts, iterations=50):\n",
    "    \"\"\"Benchmark different tokenizers.\"\"\"\n",
    "    \n",
    "    # Make sure we have the tokenizers defined\n",
    "    try:\n",
    "        gpt2_enc\n",
    "    except NameError:\n",
    "        gpt2_enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        gpt4_enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    tokenizers_to_test = {\n",
    "        'tiktoken GPT-2': gpt2_enc,\n",
    "        'tiktoken GPT-4': gpt4_enc,\n",
    "        'HF BPE': code_tokenizer,  # Reusing our domain tokenizers\n",
    "        'HF WordPiece': biomedical_tokenizer\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, tokenizer in tokenizers_to_test.items():\n",
    "        # Warm-up\n",
    "        for text in texts:\n",
    "            if hasattr(tokenizer, 'encode'):\n",
    "                # tiktoken\n",
    "                tokenizer.encode(text)\n",
    "            else:\n",
    "                # HF tokenizers\n",
    "                tokenizer.encode(text)\n",
    "        \n",
    "        # Benchmark\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for _ in range(iterations):\n",
    "            for text in texts:\n",
    "                if hasattr(tokenizer, 'encode'):\n",
    "                    # tiktoken\n",
    "                    tokenizer.encode(text)\n",
    "                else:\n",
    "                    # HF tokenizers\n",
    "                    tokenizer.encode(text)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        results.append({\n",
    "            'Tokenizer': name,\n",
    "            'Total Time (s)': total_time,\n",
    "            'Avg Time per Text (ms)': (total_time / (iterations * len(texts))) * 1000,\n",
    "            'Texts per Second': (iterations * len(texts)) / total_time\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Benchmark with sample texts of different lengths\n",
    "benchmark_texts = [\n",
    "    \"Short text.\",\n",
    "    \"This is a medium-length sentence for testing tokenization speed.\",\n",
    "    \"This is a much longer text that contains multiple sentences and should provide a good test of tokenization performance across different algorithms and implementations. We want to see how each tokenizer handles longer inputs with various patterns and structures.\",\n",
    "    \"def benchmark_function(input_data, iterations=100): return [process(data) for _ in range(iterations) for data in input_data]\"\n",
    "]\n",
    "\n",
    "perf_results = benchmark_tokenizers(benchmark_texts)\n",
    "\n",
    "print(\"Tokenizer Performance Benchmark:\")\n",
    "print(\"-\" * 50)\n",
    "print(perf_results)\n",
    "\n",
    "# Visualize performance\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot texts per second (higher is better)\n",
    "ax = sns.barplot(data=perf_results, x='Tokenizer', y='Texts per Second')\n",
    "plt.title('Tokenizer Performance: Texts Processed per Second')\n",
    "plt.xlabel('Tokenizer')\n",
    "plt.ylabel('Texts per Second (higher is better)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(perf_results['Texts per Second']):\n",
    "    ax.text(i, v + 5, f\"{v:.0f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot average time per text (lower is better)\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(data=perf_results, x='Tokenizer', y='Avg Time per Text (ms)')\n",
    "plt.title('Tokenizer Performance: Average Time per Text')\n",
    "plt.xlabel('Tokenizer')\n",
    "plt.ylabel('Milliseconds per Text (lower is better)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(perf_results['Avg Time per Text (ms)']):\n",
    "    ax.text(i, v + 0.01, f\"{v:.2f}ms\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways\n",
    "\n",
    "1. **Library Choice**: `tiktoken` is optimized for speed and works well with OpenAI models, while Hugging Face `tokenizers` offers more flexibility and customization options.\n",
    "\n",
    "2. **Vocabulary Size**: There's a clear trade-off between vocabulary size and sequence length. Larger vocabularies result in shorter sequences but require more memory for embeddings.\n",
    "\n",
    "3. **Domain Adaptation**: Custom tokenizers trained on domain-specific data perform better on texts from that domain, with more efficient tokenization and better handling of domain terminology.\n",
    "\n",
    "4. **Performance**: tiktoken generally outperforms other libraries in terms of raw speed, but Hugging Face tokenizers offer more features and customization options.\n",
    "\n",
    "5. **BPE Merges**: Understanding how BPE merges work helps debug tokenization issues and optimize vocabulary for specific use cases.\n",
    "\n",
    "## 8. Next Steps\n",
    "\n",
    "In Day 3, we'll explore how tokens become dense vector representations through embeddings, and how these embeddings capture semantic meaning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
