{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: Advanced Tokenization with tiktoken and tokenizers - Practical Exercises\n",
    "\n",
    "This notebook contains hands-on exercises and implementations for Day 2 of the LLM learning journey.\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement tokenization using tiktoken and Hugging Face tokenizers\n",
    "- Compare different tokenization libraries and their performance\n",
    "- Analyze vocabulary size vs sequence length trade-offs\n",
    "- Create custom domain-specific tokenizers\n",
    "- Benchmark tokenizer performance\n",
    "\n",
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install tiktoken tokenizers transformers matplotlib seaborn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, WordPiece\n",
    "from tokenizers.trainers import BpeTrainer, WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.normalizers import Sequence, NFD, Lowercase, StripAccents\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. tiktoken: OpenAI's Fast Tokenizer\n",
    "\n",
    "Let's start by exploring tiktoken and its different encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get different tiktoken encodings\n",
    "gpt2_enc = tiktoken.get_encoding(\"gpt2\")\n",
    "gpt4_enc = tiktoken.get_encoding(\"cl100k_base\")  # GPT-4\n",
    "codex_enc = tiktoken.get_encoding(\"p50k_base\")   # Code models\n",
    "\n",
    "# Test text\n",
    "text = \"Hello, world! This is advanced tokenization with tiktoken. Let's see how it handles different types of text: code_variable, √©mojis üöÄ, and numbers 12345.\"\n",
    "\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Text length: {len(text)} characters\\n\")\n",
    "\n",
    "# Compare different encodings\n",
    "encodings = {\n",
    "    'GPT-2': gpt2_enc,\n",
    "    'GPT-4': gpt4_enc,\n",
    "    'Codex': codex_enc\n",
    "}\n",
    "\n",
    "for name, enc in encodings.items():\n",
    "    tokens = enc.encode(text)\n",
    "    decoded = enc.decode(tokens)\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Tokens: {len(tokens)}\")\n",
    "    print(f\"  Vocab size: {enc.n_vocab:,}\")\n",
    "    print(f\"  First 10 tokens: {tokens[:10]}\")\n",
    "    print(f\"  Decoded tokens: {[enc.decode([t]) for t in tokens[:10]]}\")\n",
    "    print(f\"  Perfect reconstruction: {decoded == text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Visualizing Token Distribution\n",
    "\n",
    "Let's visualize how different encodings tokenize the same text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tokenization(text, encodings):\n",
    "    \"\"\"Visualize how different encodings tokenize the same text.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for name, enc in encodings.items():\n",
    "        tokens = enc.encode(text)\n",
    "        token_strings = [enc.decode([t]) for t in tokens]\n",
    "        \n",
    "        # Count token lengths\n",
    "        token_lengths = [len(t) for t in token_strings]\n",
    "        \n",
    "        for i, (token, length) in enumerate(zip(token_strings, token_lengths)):\n",
    "            results.append({\n",
    "                'Encoding': name,\n",
    "                'Token Index': i,\n",
    "                'Token': token,\n",
    "                'Length': length\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Plot token lengths\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df, x='Token Index', y='Length', hue='Encoding')\n",
    "    plt.title('Token Length by Position and Encoding')\n",
    "    plt.xlabel('Token Position')\n",
    "    plt.ylabel('Token Length (characters)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot token distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for name in df['Encoding'].unique():\n",
    "        subset = df[df['Encoding'] == name]\n",
    "        sns.histplot(subset['Length'], label=name, alpha=0.6, bins=range(1, 10))\n",
    "    \n",
    "    plt.title('Distribution of Token Lengths')\n",
    "    plt.xlabel('Token Length (characters)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Visualize tokenization for our test text\n",
    "token_df = visualize_tokenization(text, encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Testing with Different Text Types\n",
    "\n",
    "Let's see how tiktoken handles different types of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different text types\n",
    "text_types = {\n",
    "    'English': \"The quick brown fox jumps over the lazy dog.\",\n",
    "    'Code': \"def tokenize(text): return [token for token in text.split()]\",\n",
    "    'Emojis': \"I love üòä using üöÄ emojis üéâ in my text! üëç\",\n",
    "    'Numbers': \"The price is $1,234.56 for 42 items.\",\n",
    "    'Unicode': \"„Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå! –ü—Ä–∏–≤–µ—Ç, –º–∏—Ä! ŸÖÿ±ÿ≠ÿ®ÿß ÿ®ÿßŸÑÿπÿßŸÑŸÖ!\",\n",
    "    'Mixed': \"User@example.com sent a message: 'Hello!' with #hashtag and http://example.com\"\n",
    "}\n",
    "\n",
    "# Use GPT-4 encoding for comparison\n",
    "encoder = gpt4_enc\n",
    "\n",
    "for name, sample in text_types.items():\n",
    "    tokens = encoder.encode(sample)\n",
    "    token_strings = [encoder.decode([t]) for t in tokens]\n",
    "    \n",
    "    print(f\"{name} text:\")\n",
    "    print(f\"  Original: {sample}\")\n",
    "    print(f\"  Tokens: {len(tokens)}\")\n",
    "    print(f\"  Token IDs: {tokens}\")\n",
    "    print(f\"  Token strings: {token_strings}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
