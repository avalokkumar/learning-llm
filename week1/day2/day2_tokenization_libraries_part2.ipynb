{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: Advanced Tokenization with Hugging Face Tokenizers - Part 2\n",
    "\n",
    "This notebook continues our exploration of tokenization libraries, focusing on the Hugging Face `tokenizers` library.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, WordPiece\n",
    "from tokenizers.trainers import BpeTrainer, WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.normalizers import Sequence, NFD, Lowercase, StripAccents\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hugging Face Tokenizers\n",
    "\n",
    "Let's implement and train our own tokenizers using the Hugging Face `tokenizers` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 BPE Tokenizer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bpe_tokenizer(texts, vocab_size=1000):\n",
    "    \"\"\"Create and train a BPE tokenizer.\"\"\"\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    \n",
    "    # Setup trainer\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "    )\n",
    "    \n",
    "    # Train tokenizer\n",
    "    tokenizer.train_from_iterator(texts, trainer)\n",
    "    \n",
    "    # Add special token processing\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=\"[CLS] $A [SEP]\",\n",
    "        pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "        special_tokens=[\n",
    "            (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "            (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "# Example training texts\n",
    "training_texts = [\n",
    "    \"Hello world, this is a test.\",\n",
    "    \"Tokenization is important for NLP.\",\n",
    "    \"BPE creates subword units efficiently.\",\n",
    "    \"Machine learning models need tokenized input.\",\n",
    "    \"Natural language processing requires preprocessing.\",\n",
    "    \"Transformers use attention mechanisms to process text.\",\n",
    "    \"Word embeddings capture semantic relationships between words.\",\n",
    "    \"Contextual embeddings depend on surrounding words.\",\n",
    "    \"BERT is a bidirectional encoder representation from transformers.\",\n",
    "    \"GPT models are autoregressive language models.\"\n",
    "] * 10  # Repeat to get more training data\n",
    "\n",
    "# Create and train BPE tokenizer\n",
    "bpe_tokenizer = create_bpe_tokenizer(training_texts, vocab_size=200)\n",
    "\n",
    "# Test the tokenizer\n",
    "test_text = \"Hello world! This is a test of our BPE tokenizer.\"\n",
    "encoding = bpe_tokenizer.encode(test_text)\n",
    "\n",
    "print(f\"Input: {test_text}\")\n",
    "print(f\"Tokens: {encoding.tokens}\")\n",
    "print(f\"IDs: {encoding.ids}\")\n",
    "print(f\"Vocabulary size: {bpe_tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 WordPiece Tokenizer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordpiece_tokenizer(texts, vocab_size=1000):\n",
    "    \"\"\"Create and train a WordPiece tokenizer (BERT-style).\"\"\"\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "    \n",
    "    # Add normalization\n",
    "    tokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    \n",
    "    # Setup trainer\n",
    "    trainer = WordPieceTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "    )\n",
    "    \n",
    "    # Train tokenizer\n",
    "    tokenizer.train_from_iterator(texts, trainer)\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "# Create and train WordPiece tokenizer\n",
    "wordpiece_tokenizer = create_wordpiece_tokenizer(training_texts, vocab_size=200)\n",
    "\n",
    "# Test the tokenizer\n",
    "encoding = wordpiece_tokenizer.encode(test_text)\n",
    "\n",
    "print(f\"Input: {test_text}\")\n",
    "print(f\"Tokens: {encoding.tokens}\")\n",
    "print(f\"IDs: {encoding.ids}\")\n",
    "print(f\"Vocabulary size: {wordpiece_tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Comparing BPE and WordPiece Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tokenization_approaches(text):\n",
    "    \"\"\"Compare different tokenization approaches on the same text.\"\"\"\n",
    "    \n",
    "    # Character-level tokenization\n",
    "    char_tokens = list(text)\n",
    "    \n",
    "    # Word-level tokenization\n",
    "    word_tokens = text.split()\n",
    "    \n",
    "    # tiktoken (GPT-2)\n",
    "    gpt2_tokens = gpt2_enc.encode(text)\n",
    "    gpt2_token_strings = [gpt2_enc.decode([t]) for t in gpt2_tokens]\n",
    "    \n",
    "    # tiktoken (GPT-4)\n",
    "    gpt4_tokens = gpt4_enc.encode(text)\n",
    "    gpt4_token_strings = [gpt4_enc.decode([t]) for t in gpt4_tokens]\n",
    "    \n",
    "    # Custom BPE\n",
    "    bpe_encoding = bpe_tokenizer.encode(text)\n",
    "    bpe_tokens = bpe_encoding.tokens\n",
    "    \n",
    "    # Custom WordPiece\n",
    "    wp_encoding = wordpiece_tokenizer.encode(text)\n",
    "    wp_tokens = wp_encoding.tokens\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    results = {\n",
    "        'Tokenization': ['Character', 'Word', 'GPT-2', 'GPT-4', 'Custom BPE', 'Custom WordPiece'],\n",
    "        'Token Count': [len(char_tokens), len(word_tokens), len(gpt2_tokens), len(gpt4_tokens), len(bpe_tokens), len(wp_tokens)],\n",
    "        'Compression Ratio': [len(text)/len(char_tokens), len(text)/len(word_tokens), \n",
    "                             len(text)/len(gpt2_tokens), len(text)/len(gpt4_tokens),\n",
    "                             len(text)/len(bpe_tokens), len(text)/len(wp_tokens)]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Token count plot\n",
    "    sns.barplot(data=df, x='Tokenization', y='Token Count', ax=ax1)\n",
    "    ax1.set_title('Token Count by Tokenization Method')\n",
    "    ax1.set_ylabel('Number of Tokens')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Compression ratio plot\n",
    "    sns.barplot(data=df, x='Tokenization', y='Compression Ratio', ax=ax2)\n",
    "    ax2.set_title('Compression Ratio by Tokenization Method')\n",
    "    ax2.set_ylabel('Characters per Token (higher = more efficient)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print token examples\n",
    "    print(\"Token Examples:\")\n",
    "    print(f\"Character: {char_tokens[:10]}...\")\n",
    "    print(f\"Word: {word_tokens}\")\n",
    "    print(f\"GPT-2: {gpt2_token_strings}\")\n",
    "    print(f\"GPT-4: {gpt4_token_strings}\")\n",
    "    print(f\"Custom BPE: {bpe_tokens}\")\n",
    "    print(f\"Custom WordPiece: {wp_tokens}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test with a sample text\n",
    "comparison_text = \"The quick brown fox jumps over the lazy dog. Tokenization splits text into meaningful units.\"\n",
    "comparison_df = compare_tokenization_approaches(comparison_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspecting BPE Merge Operations\n",
    "\n",
    "Let's examine how BPE merges work step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_bpe_merges(tokenizer, text):\n",
    "    \"\"\"Inspect how BPE merges work step by step.\"\"\"\n",
    "    \n",
    "    if hasattr(tokenizer, 'get_vocab'):\n",
    "        vocab = tokenizer.get_vocab()\n",
    "    else:\n",
    "        # For tiktoken\n",
    "        vocab = {tokenizer.decode([i]): i for i in range(tokenizer.n_vocab)}\n",
    "    \n",
    "    # Get encoding\n",
    "    if hasattr(tokenizer, 'encode'):\n",
    "        tokens = tokenizer.encode(text)\n",
    "        token_strings = [tokenizer.decode([t]) for t in tokens]\n",
    "    else:\n",
    "        encoding = tokenizer.encode(text)\n",
    "        tokens = encoding.ids\n",
    "        token_strings = encoding.tokens\n",
    "    \n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Tokens: {token_strings}\")\n",
    "    print(f\"Token IDs: {tokens}\")\n",
    "    \n",
    "    # Show merge patterns\n",
    "    if hasattr(tokenizer, 'decode'):\n",
    "        # For tiktoken\n",
    "        merged_tokens = [t for t in token_strings if len(t) > 1 and not t.startswith('[')]\n",
    "    else:\n",
    "        # For HF tokenizers\n",
    "        merged_tokens = [t for t in token_strings if len(t) > 1 and not t.startswith('[')]\n",
    "        \n",
    "    print(f\"Merged tokens (subwords): {merged_tokens}\")\n",
    "    \n",
    "    return tokens, token_strings\n",
    "\n",
    "# Example with different complexity texts\n",
    "texts_to_analyze = [\n",
    "    \"hello\",\n",
    "    \"hello world\",\n",
    "    \"internationalization\",\n",
    "    \"preprocessing tokenization\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\"\n",
    "]\n",
    "\n",
    "for text in texts_to_analyze:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    inspect_bpe_merges(gpt2_enc, text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
