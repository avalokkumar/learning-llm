{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4: Positional Encodings - Part 2\n",
    "\n",
    "In this notebook, we'll explore Rotary Position Embedding (RoPE) and learned positional embeddings.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import pi, sin, cos\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rotary Position Embedding (RoPE)\n",
    "\n",
    "RoPE is a modern positional encoding that applies rotation to query and key vectors, providing better relative position modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    \"\"\"Rotary Position Embedding (RoPE) implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim, max_seq_len=2048, base=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base\n",
    "        \n",
    "        # Precompute frequency inverse\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        \n",
    "        # Precompute positional encodings\n",
    "        self._precompute_freqs_cis(max_seq_len)\n",
    "    \n",
    "    def _precompute_freqs_cis(self, seq_len):\n",
    "        \"\"\"Precompute complex exponentials for efficiency.\"\"\"\n",
    "        t = torch.arange(seq_len, dtype=torch.float32)\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        \n",
    "        # Create complex exponentials\n",
    "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "        self.register_buffer('freqs_cis', freqs_cis)\n",
    "    \n",
    "    def _reshape_for_broadcast(self, freqs_cis, x):\n",
    "        \"\"\"Reshape frequency tensor for broadcasting.\"\"\"\n",
    "        ndim = x.ndim\n",
    "        assert 0 <= 1 < ndim\n",
    "        assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "        shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "        return freqs_cis.view(*shape)\n",
    "    \n",
    "    def forward(self, x, start_pos=0):\n",
    "        \"\"\"Apply rotary position embedding.\"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        freqs_cis = self.freqs_cis[start_pos:start_pos + seq_len]\n",
    "        \n",
    "        # Convert to complex representation\n",
    "        x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "        \n",
    "        # Reshape for broadcasting\n",
    "        freqs_cis = self._reshape_for_broadcast(freqs_cis, x_complex)\n",
    "        \n",
    "        # Apply rotation\n",
    "        x_rotated = x_complex * freqs_cis\n",
    "        \n",
    "        # Convert back to real representation\n",
    "        x_out = torch.view_as_real(x_rotated).flatten(3)\n",
    "        \n",
    "        return x_out.type_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating RoPE\n",
    "\n",
    "Let's demonstrate how RoPE works and how it affects attention patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_rope():\n",
    "    \"\"\"Demonstrate RoPE properties.\"\"\"\n",
    "    \n",
    "    batch_size, seq_len, d_model = 2, 8, 64\n",
    "    rope = RotaryPositionalEmbedding(d_model)\n",
    "    \n",
    "    # Create sample query and key vectors\n",
    "    queries = torch.randn(batch_size, seq_len, d_model)\n",
    "    keys = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Apply RoPE\n",
    "    queries_rope = rope(queries)\n",
    "    keys_rope = rope(keys)\n",
    "    \n",
    "    print(\"RoPE Demonstration:\")\n",
    "    print(f\"Original queries shape: {queries.shape}\")\n",
    "    print(f\"RoPE queries shape: {queries_rope.shape}\")\n",
    "    \n",
    "    # Show that RoPE preserves relative positions\n",
    "    def compute_attention_pattern(q, k):\n",
    "        \"\"\"Compute attention pattern.\"\"\"\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(d_model)\n",
    "        return torch.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Attention without RoPE\n",
    "    attn_no_rope = compute_attention_pattern(queries[0], keys[0])\n",
    "    \n",
    "    # Attention with RoPE\n",
    "    attn_with_rope = compute_attention_pattern(queries_rope[0], keys_rope[0])\n",
    "    \n",
    "    print(f\"\\nAttention pattern difference: {torch.mean(torch.abs(attn_no_rope - attn_with_rope)):.4f}\")\n",
    "    \n",
    "    # Visualize attention patterns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Without RoPE\n",
    "    im1 = ax1.imshow(attn_no_rope.detach().numpy(), cmap='Blues')\n",
    "    ax1.set_title('Attention WITHOUT RoPE')\n",
    "    ax1.set_xlabel('Key Position')\n",
    "    ax1.set_ylabel('Query Position')\n",
    "    plt.colorbar(im1, ax=ax1)\n",
    "    \n",
    "    # With RoPE\n",
    "    im2 = ax2.imshow(attn_with_rope.detach().numpy(), cmap='Blues')\n",
    "    ax2.set_title('Attention WITH RoPE')\n",
    "    ax2.set_xlabel('Key Position')\n",
    "    ax2.set_ylabel('Query Position')\n",
    "    plt.colorbar(im2, ax=ax2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return queries_rope, keys_rope, attn_no_rope, attn_with_rope\n",
    "\n",
    "# Demonstrate RoPE\n",
    "rope_results = demonstrate_rope()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing RoPE Rotations\n",
    "\n",
    "Let's visualize how RoPE rotates vectors based on their position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rope_rotations():\n",
    "    \"\"\"Visualize how RoPE rotates vectors based on position.\"\"\"\n",
    "    \n",
    "    # Create a simple 2D vector for visualization\n",
    "    vector = torch.tensor([[1.0, 0.0]])\n",
    "    \n",
    "    # Create rotation matrices for different positions\n",
    "    positions = range(8)\n",
    "    rotated_vectors = []\n",
    "    \n",
    "    for pos in positions:\n",
    "        # Create rotation matrix for this position\n",
    "        theta = pos * 0.1  # Simple rotation angle proportional to position\n",
    "        rotation_matrix = torch.tensor([\n",
    "            [np.cos(theta), -np.sin(theta)],\n",
    "            [np.sin(theta), np.cos(theta)]\n",
    "        ])\n",
    "        \n",
    "        # Apply rotation\n",
    "        rotated = torch.matmul(vector, rotation_matrix)\n",
    "        rotated_vectors.append(rotated.numpy()[0])\n",
    "    \n",
    "    # Visualize rotations\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    # Plot unit circle\n",
    "    theta = np.linspace(0, 2*np.pi, 100)\n",
    "    plt.plot(np.cos(theta), np.sin(theta), 'k--', alpha=0.3)\n",
    "    \n",
    "    # Plot vectors\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(positions)))\n",
    "    \n",
    "    for i, (pos, vec) in enumerate(zip(positions, rotated_vectors)):\n",
    "        plt.arrow(0, 0, vec[0], vec[1], head_width=0.05, head_length=0.1, \n",
    "                 fc=colors[i], ec=colors[i], label=f'Position {pos}')\n",
    "    \n",
    "    plt.xlim(-1.2, 1.2)\n",
    "    plt.ylim(-1.2, 1.2)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(y=0, color='k', linewidth=0.5, alpha=0.5)\n",
    "    plt.axvline(x=0, color='k', linewidth=0.5, alpha=0.5)\n",
    "    plt.title('RoPE Vector Rotations by Position')\n",
    "    plt.legend()\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "    \n",
    "    return rotated_vectors\n",
    "\n",
    "# Visualize RoPE rotations\n",
    "rotated_vectors = visualize_rope_rotations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Learned Positional Embeddings\n",
    "\n",
    "Some models use learned positional embeddings instead of fixed mathematical functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedPositionalEmbedding(nn.Module):\n",
    "    \"\"\"Learned positional embedding implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_seq_len, d_model):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Learnable position embeddings\n",
    "        self.position_embeddings = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        # Initialize with small random values\n",
    "        nn.init.normal_(self.position_embeddings.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, x, position_ids=None):\n",
    "        \"\"\"Add learned positional embeddings.\"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand(x.size(0), -1)\n",
    "        \n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        return x + position_embeddings\n",
    "    \n",
    "    def get_embedding(self, position):\n",
    "        \"\"\"Get positional embedding for specific position.\"\"\"\n",
    "        return self.position_embeddings.weight[position].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Different Positional Encoding Methods\n",
    "\n",
    "Let's compare sinusoidal and learned positional encodings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_positional_encodings():\n",
    "    \"\"\"Compare different positional encoding approaches.\"\"\"\n",
    "    \n",
    "    d_model = 64\n",
    "    max_seq_len = 20\n",
    "    \n",
    "    # Initialize different encoders\n",
    "    sinusoidal = SinusoidalPositionalEncoding(d_model, max_seq_len)\n",
    "    learned = LearnedPositionalEmbedding(max_seq_len, d_model)\n",
    "    \n",
    "    # Get encodings for comparison\n",
    "    positions = range(10)\n",
    "    \n",
    "    sin_encodings = []\n",
    "    learned_encodings = []\n",
    "    \n",
    "    for pos in positions:\n",
    "        sin_enc = sinusoidal.get_encoding(pos)\n",
    "        learned_enc = learned.get_embedding(pos)\n",
    "        \n",
    "        sin_encodings.append(sin_enc)\n",
    "        learned_encodings.append(learned_enc)\n",
    "    \n",
    "    sin_encodings = np.array(sin_encodings)\n",
    "    learned_encodings = np.array(learned_encodings)\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Sinusoidal encodings\n",
    "    im1 = ax1.imshow(sin_encodings.T, cmap='RdBu', aspect='auto')\n",
    "    ax1.set_title('Sinusoidal Positional Encodings')\n",
    "    ax1.set_xlabel('Position')\n",
    "    ax1.set_ylabel('Embedding Dimension')\n",
    "    plt.colorbar(im1, ax=ax1)\n",
    "    \n",
    "    # Learned encodings (random initialization)\n",
    "    im2 = ax2.imshow(learned_encodings.T, cmap='RdBu', aspect='auto')\n",
    "    ax2.set_title('Learned Positional Embeddings (Random Init)')\n",
    "    ax2.set_xlabel('Position')\n",
    "    ax2.set_ylabel('Embedding Dimension')\n",
    "    plt.colorbar(im2, ax=ax2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compare norms\n",
    "    sin_norms = np.linalg.norm(sin_encodings, axis=1)\n",
    "    learned_norms = np.linalg.norm(learned_encodings, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(positions, sin_norms, 'o-', label='Sinusoidal')\n",
    "    plt.plot(positions, learned_norms, 'o-', label='Learned')\n",
    "    plt.title('Embedding Norms by Position')\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('L2 Norm')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return sin_encodings, learned_encodings\n",
    "\n",
    "# For this to work, we need to define SinusoidalPositionalEncoding class\n",
    "# Let's assume it's defined in the same way as in part 1\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len=5000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Create division term for frequency scaling\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register as buffer (not a parameter)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len]\n",
    "    \n",
    "    def get_encoding(self, position):\n",
    "        \"\"\"Get positional encoding for specific position.\"\"\"\n",
    "        return self.pe[0, position].detach().numpy()\n",
    "\n",
    "# Compare encodings\n",
    "comparison_results = compare_positional_encodings()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
