{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4: Positional Encodings in Transformer Models\n",
    "\n",
    "In this notebook, we'll explore why transformers need positional information and implement different types of positional encodings, including sinusoidal encodings and Rotary Position Embeddings (RoPE).\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import pi, sin, cos\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Position Problem\n",
    "\n",
    "Transformers process all tokens simultaneously, losing the natural sequence order information that RNNs and LSTMs have built-in. Let's demonstrate why this is a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_position_problem():\n",
    "    \"\"\"Show why transformers need positional encoding.\"\"\"\n",
    "    \n",
    "    # Simulate attention without position information\n",
    "    def simple_attention(queries, keys, values):\n",
    "        \"\"\"Simplified attention mechanism.\"\"\"\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(queries, keys.transpose(-2, -1))\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, values)\n",
    "        return output, attention_weights\n",
    "    \n",
    "    # Create identical embeddings for different positions\n",
    "    embedding_dim = 4\n",
    "    seq_len = 3\n",
    "    \n",
    "    # Same word at different positions\n",
    "    word_embedding = torch.tensor([1.0, 0.5, -0.2, 0.8])\n",
    "    \n",
    "    # Without positional encoding - all positions identical\n",
    "    embeddings_no_pos = word_embedding.unsqueeze(0).repeat(seq_len, 1)\n",
    "    print(\"Embeddings without positional encoding:\")\n",
    "    print(embeddings_no_pos)\n",
    "    \n",
    "    # Attention treats all positions identically\n",
    "    output_no_pos, weights_no_pos = simple_attention(\n",
    "        embeddings_no_pos.unsqueeze(0),\n",
    "        embeddings_no_pos.unsqueeze(0), \n",
    "        embeddings_no_pos.unsqueeze(0)\n",
    "    )\n",
    "    \n",
    "    print(\"\\nAttention weights without position (should be uniform):\")\n",
    "    print(weights_no_pos[0])\n",
    "    \n",
    "    # Let's visualize the attention pattern\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(weights_no_pos[0].detach().numpy(), cmap='Blues')\n",
    "    plt.title('Attention Pattern Without Position Information')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    plt.colorbar(label='Attention Weight')\n",
    "    plt.show()\n",
    "    \n",
    "    return embeddings_no_pos, weights_no_pos\n",
    "\n",
    "# Demonstrate the position problem\n",
    "embeddings_no_pos, weights_no_pos = demonstrate_position_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Problem\n",
    "\n",
    "As we can see above, when we have the same word repeated at different positions, the attention mechanism treats all positions identically. This is because the attention calculation is based solely on the dot product between query and key vectors, which doesn't consider position information.\n",
    "\n",
    "In natural language, word order is crucial. \"Dog bites man\" means something very different from \"Man bites dog.\" Without position information, transformers would treat these sentences as identical bags of words.\n",
    "\n",
    "## 2. Sinusoidal Positional Encoding\n",
    "\n",
    "The original Transformer paper introduced sinusoidal positional encodings using sine and cosine functions of different frequencies. Let's implement and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len=5000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Create division term for frequency scaling\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register as buffer (not a parameter)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len]\n",
    "    \n",
    "    def get_encoding(self, position):\n",
    "        \"\"\"Get positional encoding for specific position.\"\"\"\n",
    "        return self.pe[0, position].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Sinusoidal Positional Encodings\n",
    "\n",
    "Let's create and visualize the sinusoidal positional encodings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create positional encoding\n",
    "d_model = 128\n",
    "max_seq_len = 100\n",
    "pos_encoder = SinusoidalPositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "def visualize_positional_encodings(pos_encoder, max_pos=50, max_dim=64):\n",
    "    \"\"\"Visualize positional encoding patterns.\"\"\"\n",
    "    \n",
    "    # Get encodings for different positions\n",
    "    encodings = []\n",
    "    for pos in range(max_pos):\n",
    "        encoding = pos_encoder.get_encoding(pos)[:max_dim]\n",
    "        encodings.append(encoding)\n",
    "    \n",
    "    encodings = np.array(encodings)\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(encodings.T, cmap='RdBu', aspect='auto')\n",
    "    plt.colorbar(label='Encoding Value')\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('Embedding Dimension')\n",
    "    plt.title('Sinusoidal Positional Encodings')\n",
    "    plt.show()\n",
    "    \n",
    "    # Show specific positions\n",
    "    positions_to_show = [0, 1, 5, 10, 20]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for pos in positions_to_show:\n",
    "        encoding = pos_encoder.get_encoding(pos)[:32]  # Show first 32 dims\n",
    "        plt.plot(encoding, label=f'Position {pos}', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Embedding Dimension')\n",
    "    plt.ylabel('Encoding Value')\n",
    "    plt.title('Positional Encodings for Different Positions')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return encodings\n",
    "\n",
    "# Visualize the encodings\n",
    "encodings = visualize_positional_encodings(pos_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Properties of Sinusoidal Encodings\n",
    "\n",
    "Let's analyze some key properties of sinusoidal positional encodings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sinusoidal_properties(pos_encoder, max_pos=20):\n",
    "    \"\"\"Analyze key properties of sinusoidal positional encodings.\"\"\"\n",
    "    \n",
    "    print(\"Properties of Sinusoidal Positional Encodings:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Property 1: Unique encoding for each position\n",
    "    encodings = []\n",
    "    for pos in range(max_pos):\n",
    "        encoding = pos_encoder.get_encoding(pos)\n",
    "        encodings.append(encoding)\n",
    "    \n",
    "    encodings = np.array(encodings)\n",
    "    \n",
    "    # Check uniqueness\n",
    "    unique_count = len(np.unique(encodings, axis=0))\n",
    "    print(f\"1. Uniqueness: {unique_count}/{max_pos} positions have unique encodings\")\n",
    "    \n",
    "    # Property 2: Relative position information\n",
    "    def relative_position_similarity(pos1, pos2):\n",
    "        enc1 = pos_encoder.get_encoding(pos1)\n",
    "        enc2 = pos_encoder.get_encoding(pos2)\n",
    "        return np.dot(enc1, enc2) / (np.linalg.norm(enc1) * np.linalg.norm(enc2))\n",
    "    \n",
    "    print(\"\\n2. Relative Position Similarities:\")\n",
    "    reference_pos = 5\n",
    "    for offset in [1, 2, 5, 10]:\n",
    "        sim = relative_position_similarity(reference_pos, reference_pos + offset)\n",
    "        print(f\"   Position {reference_pos} vs {reference_pos + offset}: {sim:.4f}\")\n",
    "    \n",
    "    # Property 3: Linear combination property\n",
    "    # PE(pos + k) can be expressed as linear combination of PE(pos) and PE(k)\n",
    "    pos_a, pos_b = 3, 7\n",
    "    pos_sum = pos_a + pos_b\n",
    "    \n",
    "    enc_a = pos_encoder.get_encoding(pos_a)\n",
    "    enc_b = pos_encoder.get_encoding(pos_b)\n",
    "    enc_sum = pos_encoder.get_encoding(pos_sum)\n",
    "    \n",
    "    # This property is approximate for sinusoidal encodings\n",
    "    print(f\"\\n3. Additivity (approximate):\")\n",
    "    print(f\"   ||PE({pos_a}) + PE({pos_b}) - PE({pos_sum})|| = {np.linalg.norm(enc_a + enc_b - enc_sum):.4f}\")\n",
    "    \n",
    "    # Property 4: Fixed norm\n",
    "    norms = np.linalg.norm(encodings, axis=1)\n",
    "    print(f\"\\n4. Norm consistency:\")\n",
    "    print(f\"   Mean norm: {np.mean(norms):.4f}\")\n",
    "    print(f\"   Std dev of norms: {np.std(norms):.4f}\")\n",
    "    \n",
    "    # Property 5: Frequency spectrum\n",
    "    print(f\"\\n5. Frequency spectrum:\")\n",
    "    # Compute average frequency for each dimension\n",
    "    freq_by_dim = np.zeros(pos_encoder.d_model)\n",
    "    for dim in range(pos_encoder.d_model):\n",
    "        # Estimate frequency by counting zero crossings\n",
    "        zero_crossings = np.sum(np.diff(np.signbit(encodings[:, dim])))\n",
    "        freq_by_dim[dim] = zero_crossings / (2 * max_pos)\n",
    "    \n",
    "    # Plot frequency spectrum\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(freq_by_dim)\n",
    "    plt.title('Frequency Spectrum of Sinusoidal Encodings')\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('Frequency (estimated)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return encodings\n",
    "\n",
    "# Analyze properties\n",
    "properties = analyze_sinusoidal_properties(pos_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating Positional Encoding in Action\n",
    "\n",
    "Let's see how positional encoding changes the attention pattern for identical tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_positional_encoding_effect():\n",
    "    \"\"\"Demonstrate how positional encoding affects attention.\"\"\"\n",
    "    \n",
    "    # Simplified attention function\n",
    "    def simple_attention(queries, keys, values):\n",
    "        \"\"\"Simplified attention mechanism.\"\"\"\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(queries, keys.transpose(-2, -1))\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, values)\n",
    "        return output, attention_weights\n",
    "    \n",
    "    # Setup\n",
    "    embedding_dim = 32\n",
    "    seq_len = 5\n",
    "    pos_encoder = SinusoidalPositionalEncoding(embedding_dim, 100)\n",
    "    \n",
    "    # Create identical embeddings (same word repeated)\n",
    "    word_embedding = torch.randn(embedding_dim)\n",
    "    embeddings_no_pos = word_embedding.unsqueeze(0).repeat(seq_len, 1)\n",
    "    \n",
    "    # Add positional encoding\n",
    "    embeddings_with_pos = pos_encoder(embeddings_no_pos.unsqueeze(0)).squeeze(0)\n",
    "    \n",
    "    # Compute attention without positional encoding\n",
    "    output_no_pos, attn_no_pos = simple_attention(\n",
    "        embeddings_no_pos.unsqueeze(0),\n",
    "        embeddings_no_pos.unsqueeze(0),\n",
    "        embeddings_no_pos.unsqueeze(0)\n",
    "    )\n",
    "    \n",
    "    # Compute attention with positional encoding\n",
    "    output_with_pos, attn_with_pos = simple_attention(\n",
    "        embeddings_with_pos.unsqueeze(0),\n",
    "        embeddings_with_pos.unsqueeze(0),\n",
    "        embeddings_with_pos.unsqueeze(0)\n",
    "    )\n",
    "    \n",
    "    # Visualize attention patterns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Without positional encoding\n",
    "    im1 = ax1.imshow(attn_no_pos[0].detach().numpy(), cmap='Blues')\n",
    "    ax1.set_title('Attention WITHOUT Positional Encoding')\n",
    "    ax1.set_xlabel('Key Position')\n",
    "    ax1.set_ylabel('Query Position')\n",
    "    plt.colorbar(im1, ax=ax1)\n",
    "    \n",
    "    # With positional encoding\n",
    "    im2 = ax2.imshow(attn_with_pos[0].detach().numpy(), cmap='Blues')\n",
    "    ax2.set_title('Attention WITH Positional Encoding')\n",
    "    ax2.set_xlabel('Key Position')\n",
    "    ax2.set_ylabel('Query Position')\n",
    "    plt.colorbar(im2, ax=ax2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Without positional encoding, attention is uniform across all positions.\")\n",
    "    print(\"With positional encoding, attention patterns become position-aware.\")\n",
    "    print(\"Notice how positions tend to attend more to themselves and nearby positions.\")\n",
    "    \n",
    "    return embeddings_no_pos, embeddings_with_pos, attn_no_pos, attn_with_pos\n",
    "\n",
    "# Demonstrate the effect\n",
    "embeddings_no_pos, embeddings_with_pos, attn_no_pos, attn_with_pos = demonstrate_positional_encoding_effect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
