{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4: Positional Encodings - Part 3\n",
    "\n",
    "In this notebook, we'll explore practical applications and exercises related to positional encodings.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import pi, sin, cos\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Positional Encoding in Practice\n",
    "\n",
    "Let's implement a simple transformer block with positional encoding to see how it works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's define our positional encoding classes\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len=5000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Create division term for frequency scaling\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register as buffer (not a parameter)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len]\n",
    "\n",
    "class LearnedPositionalEmbedding(nn.Module):\n",
    "    \"\"\"Learned positional embedding implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_seq_len, d_model):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Learnable position embeddings\n",
    "        self.position_embeddings = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        # Initialize with small random values\n",
    "        nn.init.normal_(self.position_embeddings.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, x, position_ids=None):\n",
    "        \"\"\"Add learned positional embeddings.\"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand(x.size(0), -1)\n",
    "        \n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        return x + position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerWithPositionalEncoding(nn.Module):\n",
    "    \"\"\"Simple transformer block with positional encoding.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, nhead, max_seq_len, pos_encoding_type='sinusoidal'):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        if pos_encoding_type == 'sinusoidal':\n",
    "            self.pos_encoding = SinusoidalPositionalEncoding(d_model, max_seq_len)\n",
    "        elif pos_encoding_type == 'learned':\n",
    "            self.pos_encoding = LearnedPositionalEmbedding(max_seq_len, d_model)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown positional encoding type: {pos_encoding_type}\")\n",
    "        \n",
    "        # Simple attention layer\n",
    "        self.attention = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"Forward pass with positional encoding.\"\"\"\n",
    "        # Token embeddings\n",
    "        embeddings = self.token_embedding(token_ids) * np.sqrt(self.d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        embeddings_with_pos = self.pos_encoding(embeddings)\n",
    "        \n",
    "        # Self-attention\n",
    "        attn_output, attn_weights = self.attention(\n",
    "            embeddings_with_pos, embeddings_with_pos, embeddings_with_pos\n",
    "        )\n",
    "        \n",
    "        # Residual connection and normalization\n",
    "        output = self.norm(embeddings_with_pos + attn_output)\n",
    "        \n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating the Effect of Positional Encoding\n",
    "\n",
    "Let's demonstrate how positional encoding affects attention patterns in a transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_position_effect():\n",
    "    \"\"\"Demonstrate how positional encoding affects attention.\"\"\"\n",
    "    \n",
    "    vocab_size = 100\n",
    "    d_model = 64\n",
    "    nhead = 4\n",
    "    max_seq_len = 10\n",
    "    \n",
    "    # Create models with and without positional encoding\n",
    "    model_with_pos = TransformerWithPositionalEncoding(\n",
    "        vocab_size, d_model, nhead, max_seq_len, 'sinusoidal'\n",
    "    )\n",
    "    \n",
    "    # Create a model without positional encoding (for comparison)\n",
    "    class NoPositionalEncoding(nn.Module):\n",
    "        def forward(self, x):\n",
    "            return x\n",
    "    \n",
    "    model_no_pos = TransformerWithPositionalEncoding(\n",
    "        vocab_size, d_model, nhead, max_seq_len, 'sinusoidal'\n",
    "    )\n",
    "    model_no_pos.pos_encoding = NoPositionalEncoding()\n",
    "    \n",
    "    # Test with identical tokens at different positions\n",
    "    token_ids = torch.tensor([[1, 1, 1, 2, 2, 2]])  # Same tokens repeated\n",
    "    \n",
    "    # Forward pass with positional encoding\n",
    "    output_with_pos, attn_with_pos = model_with_pos(token_ids)\n",
    "    \n",
    "    # Forward pass without positional encoding\n",
    "    output_no_pos, attn_no_pos = model_no_pos(token_ids)\n",
    "    \n",
    "    print(\"Effect of Positional Encoding on Attention:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Compare attention patterns\n",
    "    print(\"Attention weights WITH positional encoding:\")\n",
    "    print(attn_with_pos[0, 0].detach().numpy().round(3))  # First head, first batch\n",
    "    \n",
    "    print(\"\\nAttention weights WITHOUT positional encoding:\")\n",
    "    print(attn_no_pos[0, 0].detach().numpy().round(3))\n",
    "    \n",
    "    # Visualize attention patterns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # With positional encoding\n",
    "    im1 = ax1.imshow(attn_with_pos[0, 0].detach().numpy(), cmap='Blues')\n",
    "    ax1.set_title('Attention WITH Positional Encoding')\n",
    "    ax1.set_xlabel('Key Position')\n",
    "    ax1.set_ylabel('Query Position')\n",
    "    plt.colorbar(im1, ax=ax1)\n",
    "    \n",
    "    # Without positional encoding\n",
    "    im2 = ax2.imshow(attn_no_pos[0, 0].detach().numpy(), cmap='Blues')\n",
    "    ax2.set_title('Attention WITHOUT Positional Encoding')\n",
    "    ax2.set_xlabel('Key Position')\n",
    "    ax2.set_ylabel('Query Position')\n",
    "    plt.colorbar(im2, ax=ax2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze the attention patterns\n",
    "    print(\"\\nObservations:\")\n",
    "    print(\"1. Without positional encoding, identical tokens attend to each other equally\")\n",
    "    print(\"2. With positional encoding, we see a more structured attention pattern\")\n",
    "    print(\"3. Tokens tend to attend more to themselves and nearby positions\")\n",
    "    print(\"4. The model can now distinguish between different positions of the same token\")\n",
    "    \n",
    "    return output_with_pos, output_no_pos, attn_with_pos, attn_no_pos\n",
    "\n",
    "# Demonstrate the effect of positional encoding\n",
    "position_effect_results = demonstrate_position_effect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practical Exercises\n",
    "\n",
    "### Exercise 1: Position Encoding Comparison\n",
    "\n",
    "Let's compare different positional encoding methods in terms of how they affect attention patterns and model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding_comparison_experiment():\n",
    "    \"\"\"Compare different positional encoding methods.\"\"\"\n",
    "    \n",
    "    d_model = 128\n",
    "    max_seq_len = 50\n",
    "    \n",
    "    # Initialize encoders\n",
    "    encoders = {\n",
    "        'Sinusoidal': SinusoidalPositionalEncoding(d_model, max_seq_len),\n",
    "        'Learned': LearnedPositionalEmbedding(max_seq_len, d_model)\n",
    "    }\n",
    "    \n",
    "    # Test sequence\n",
    "    seq_len = 20\n",
    "    batch_size = 1\n",
    "    \n",
    "    # Random token embeddings\n",
    "    token_embeddings = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, encoder in encoders.items():\n",
    "        # Apply positional encoding\n",
    "        if name == 'Sinusoidal':\n",
    "            encoded = encoder(token_embeddings)\n",
    "        else:  # Learned\n",
    "            encoded = encoder(token_embeddings)\n",
    "        \n",
    "        # Compute position distinguishability\n",
    "        position_similarities = []\n",
    "        for i in range(seq_len - 1):\n",
    "            for j in range(i + 1, seq_len):\n",
    "                pos_i = encoded[0, i].detach().numpy()\n",
    "                pos_j = encoded[0, j].detach().numpy()\n",
    "                sim = np.dot(pos_i, pos_j) / (np.linalg.norm(pos_i) * np.linalg.norm(pos_j))\n",
    "                position_similarities.append(sim)\n",
    "        \n",
    "        results[name] = {\n",
    "            'avg_similarity': np.mean(position_similarities),\n",
    "            'std_similarity': np.std(position_similarities),\n",
    "            'encoded_shape': encoded.shape\n",
    "        }\n",
    "    \n",
    "    print(\"Positional Encoding Comparison:\")\n",
    "    print(\"=\" * 40)\n",
    "    for name, stats in results.items():\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Average position similarity: {stats['avg_similarity']:.4f}\")\n",
    "        print(f\"  Similarity std deviation: {stats['std_similarity']:.4f}\")\n",
    "        print(f\"  Output shape: {stats['encoded_shape']}\")\n",
    "        print()\n",
    "    \n",
    "    # Visualize similarity distributions\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Compute all pairwise similarities for visualization\n",
    "    all_similarities = {}\n",
    "    for name, encoder in encoders.items():\n",
    "        if name == 'Sinusoidal':\n",
    "            encoded = encoder(token_embeddings)\n",
    "        else:  # Learned\n",
    "            encoded = encoder(token_embeddings)\n",
    "            \n",
    "        # Compute similarity matrix\n",
    "        similarities = np.zeros((seq_len, seq_len))\n",
    "        for i in range(seq_len):\n",
    "            for j in range(seq_len):\n",
    "                pos_i = encoded[0, i].detach().numpy()\n",
    "                pos_j = encoded[0, j].detach().numpy()\n",
    "                sim = np.dot(pos_i, pos_j) / (np.linalg.norm(pos_i) * np.linalg.norm(pos_j))\n",
    "                similarities[i, j] = sim\n",
    "        \n",
    "        all_similarities[name] = similarities\n",
    "    \n",
    "    # Plot similarity matrices\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    for i, (name, similarities) in enumerate(all_similarities.items()):\n",
    "        im = axes[i].imshow(similarities, cmap='viridis', vmin=-1, vmax=1)\n",
    "        axes[i].set_title(f'{name} Position Similarity Matrix')\n",
    "        axes[i].set_xlabel('Position')\n",
    "        axes[i].set_ylabel('Position')\n",
    "        plt.colorbar(im, ax=axes[i])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results, all_similarities\n",
    "\n",
    "# Run the comparison experiment\n",
    "comparison_experiment_results, similarity_matrices = position_encoding_comparison_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Sequence Length Extrapolation\n",
    "\n",
    "Let's test how well different positional encodings handle longer sequences than they were trained on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_length_extrapolation_test():\n",
    "    \"\"\"Test how well positional encodings handle longer sequences.\"\"\"\n",
    "    \n",
    "    d_model = 64\n",
    "    train_seq_len = 20\n",
    "    test_seq_lens = [30, 40, 60, 100]\n",
    "    \n",
    "    # Train on shorter sequences\n",
    "    sinusoidal = SinusoidalPositionalEncoding(d_model, max_seq_len=200)\n",
    "    learned = LearnedPositionalEmbedding(train_seq_len, d_model)\n",
    "    \n",
    "    print(\"Sequence Length Extrapolation Test:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for test_len in test_seq_lens:\n",
    "        print(f\"\\nTesting sequence length: {test_len}\")\n",
    "        \n",
    "        # Create test embeddings\n",
    "        test_embeddings = torch.randn(1, test_len, d_model)\n",
    "        \n",
    "        # Sinusoidal (can handle any length)\n",
    "        try:\n",
    "            sin_encoded = sinusoidal(test_embeddings)\n",
    "            sin_success = True\n",
    "        except Exception as e:\n",
    "            sin_success = False\n",
    "            print(f\"  Sinusoidal failed: {e}\")\n",
    "        \n",
    "        # Learned (limited by training length)\n",
    "        try:\n",
    "            if test_len <= train_seq_len:\n",
    "                learned_encoded = learned(test_embeddings)\n",
    "                learned_success = True\n",
    "            else:\n",
    "                learned_success = False\n",
    "                print(f\"  Learned encoding cannot handle length {test_len} (trained on {train_seq_len})\")\n",
    "        except Exception as e:\n",
    "            learned_success = False\n",
    "            print(f\"  Learned failed: {e}\")\n",
    "        \n",
    "        print(f\"  Sinusoidal: {'✓' if sin_success else '✗'}\")\n",
    "        print(f\"  Learned: {'✓' if learned_success else '✗'}\")\n",
    "    \n",
    "    print(\"\\nConclusion: Sinusoidal encodings can extrapolate to longer sequences,\")\n",
    "    print(\"while learned encodings are limited by their training length.\")\n",
    "    \n",
    "    # Visualize sinusoidal encodings at different positions\n",
    "    if max(test_seq_lens) <= 200:  # Check if we can visualize the longest sequence\n",
    "        positions = [0, 10, 20, 50, 100]\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        for pos in positions:\n",
    "            if pos < 200:  # Make sure position is within range\n",
    "                encoding = sinusoidal.pe[0, pos, :32].detach().numpy()  # First 32 dimensions\n",
    "                plt.plot(encoding, label=f'Position {pos}', alpha=0.7)\n",
    "        \n",
    "        plt.title('Sinusoidal Encodings at Different Positions')\n",
    "        plt.xlabel('Dimension')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "# Run the extrapolation test\n",
    "sequence_length_extrapolation_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Position Problem**: Transformers need explicit positional information due to their parallel processing\n",
    "2. **Sinusoidal Encoding**: Mathematical, deterministic approach that extrapolates well to unseen lengths\n",
    "3. **Learned Encoding**: Flexible approach that adapts to data but is limited by training sequence length\n",
    "4. **RoPE**: Modern approach with better relative position modeling through rotational transformations\n",
    "5. **Relative Encoding**: Focuses on relative rather than absolute positions for more efficient modeling\n",
    "\n",
    "## What's Next\n",
    "\n",
    "In Day 5, we'll build a complete end-to-end pipeline combining everything: text → tokenization → embeddings → positional encoding, creating a full preprocessing pipeline for transformer models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
