{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3: Embeddings - From Tokens to Vectors\n",
    "\n",
    "In this notebook, we'll explore how to transform discrete tokens into dense vector representations that capture semantic meaning. We'll implement both one-hot and learned embeddings, explore similarity measures, and visualize how embeddings encode relationships between words.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. One-hot Embeddings\n",
    "\n",
    "One-hot encoding represents each token as a sparse vector with exactly one 1 and all other elements as 0. Let's implement a simple one-hot embedding class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEmbedding:\n",
    "    \"\"\"Simple one-hot embedding implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = vocab_size  # Same as vocab size for one-hot\n",
    "    \n",
    "    def encode(self, token_ids):\n",
    "        \"\"\"Convert token IDs to one-hot vectors.\"\"\"\n",
    "        if isinstance(token_ids, int):\n",
    "            token_ids = [token_ids]\n",
    "        \n",
    "        one_hot = np.zeros((len(token_ids), self.vocab_size))\n",
    "        for i, token_id in enumerate(token_ids):\n",
    "            if 0 <= token_id < self.vocab_size:\n",
    "                one_hot[i, token_id] = 1.0\n",
    "        \n",
    "        return one_hot\n",
    "    \n",
    "    def similarity(self, token_id1, token_id2):\n",
    "        \"\"\"Compute similarity between two tokens.\"\"\"\n",
    "        vec1 = self.encode([token_id1])[0]\n",
    "        vec2 = self.encode([token_id2])[0]\n",
    "        \n",
    "        # Cosine similarity for one-hot is always 0 (orthogonal) or 1 (identical)\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our one-hot embedding implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small vocabulary for demonstration\n",
    "vocab = {\n",
    "    'the': 0,\n",
    "    'cat': 1,\n",
    "    'dog': 2,\n",
    "    'sat': 3,\n",
    "    'on': 4,\n",
    "    'mat': 5,\n",
    "    'ran': 6,\n",
    "    'in': 7,\n",
    "    'park': 8,\n",
    "    'house': 9\n",
    "}\n",
    "\n",
    "# Create one-hot embedding\n",
    "vocab_size = len(vocab)\n",
    "one_hot_emb = OneHotEmbedding(vocab_size)\n",
    "\n",
    "# Encode some tokens\n",
    "tokens = [vocab['the'], vocab['cat'], vocab['dog'], vocab['mat']]\n",
    "embeddings = one_hot_emb.encode(tokens)\n",
    "\n",
    "print(\"One-hot embeddings:\")\n",
    "for i, token_id in enumerate(tokens):\n",
    "    token_name = [k for k, v in vocab.items() if v == token_id][0]\n",
    "    print(f\"Token '{token_name}' (ID {token_id}): {embeddings[i]}\")\n",
    "\n",
    "# Check similarity\n",
    "sim = one_hot_emb.similarity(vocab['cat'], vocab['dog'])\n",
    "print(f\"\\nSimilarity between 'cat' and 'dog': {sim}\")\n",
    "\n",
    "sim_same = one_hot_emb.similarity(vocab['cat'], vocab['cat'])\n",
    "print(f\"Similarity between 'cat' and itself: {sim_same}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing One-hot Embeddings\n",
    "\n",
    "Let's visualize our one-hot embeddings to understand their structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_one_hot(vocab, one_hot_emb):\n",
    "    # Get embeddings for all words\n",
    "    words = list(vocab.keys())\n",
    "    token_ids = [vocab[word] for word in words]\n",
    "    embeddings = one_hot_emb.encode(token_ids)\n",
    "    \n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(embeddings, cmap='Blues', cbar=True, \n",
    "                xticklabels=range(vocab_size), \n",
    "                yticklabels=words)\n",
    "    plt.title('One-hot Encodings')\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('Word')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create similarity matrix\n",
    "    similarity_matrix = np.zeros((len(words), len(words)))\n",
    "    for i, word1 in enumerate(words):\n",
    "        for j, word2 in enumerate(words):\n",
    "            similarity_matrix[i, j] = one_hot_emb.similarity(vocab[word1], vocab[word2])\n",
    "    \n",
    "    # Plot similarity matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_matrix, annot=True, cmap='YlGnBu', \n",
    "                xticklabels=words, yticklabels=words)\n",
    "    plt.title('One-hot Encoding Similarity Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize one-hot embeddings\n",
    "visualize_one_hot(vocab, one_hot_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of One-hot Embeddings\n",
    "\n",
    "Let's analyze the limitations of one-hot embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_one_hot_limitations(vocab_sizes):\n",
    "    # Analyze memory usage and dimensionality\n",
    "    results = []\n",
    "    for size in vocab_sizes:\n",
    "        # Memory usage (bytes) for float32\n",
    "        memory_bytes = size * size * 4  # 4 bytes per float32\n",
    "        memory_mb = memory_bytes / (1024 * 1024)\n",
    "        \n",
    "        # Sparsity (percentage of zeros)\n",
    "        sparsity = (size - 1) / size * 100\n",
    "        \n",
    "        results.append({\n",
    "            'vocab_size': size,\n",
    "            'memory_mb': memory_mb,\n",
    "            'sparsity': sparsity\n",
    "        })\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Plot results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Memory usage\n",
    "    ax1.plot(df['vocab_size'], df['memory_mb'], marker='o', linewidth=2)\n",
    "    ax1.set_title('Memory Usage vs. Vocabulary Size')\n",
    "    ax1.set_xlabel('Vocabulary Size')\n",
    "    ax1.set_ylabel('Memory (MB)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Sparsity\n",
    "    ax2.plot(df['vocab_size'], df['sparsity'], marker='o', linewidth=2, color='green')\n",
    "    ax2.set_title('Sparsity vs. Vocabulary Size')\n",
    "    ax2.set_xlabel('Vocabulary Size')\n",
    "    ax2.set_ylabel('Sparsity (%)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Analyze one-hot limitations with different vocabulary sizes\n",
    "vocab_sizes = [10, 100, 1000, 10000, 50000]\n",
    "limitation_results = analyze_one_hot_limitations(vocab_sizes)\n",
    "print(limitation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learned Embeddings\n",
    "\n",
    "Learned embeddings map tokens to dense, low-dimensional vectors that can capture semantic relationships. Let's implement a learned embedding layer using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedEmbedding(nn.Module):\n",
    "    \"\"\"Learned embedding layer implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, padding_idx=None):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Initialize embedding matrix\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, \n",
    "            embedding_dim, \n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "        \n",
    "        # Initialize with small random values\n",
    "        nn.init.normal_(self.embedding.weight, mean=0, std=0.1)\n",
    "        \n",
    "        if padding_idx is not None:\n",
    "            # Zero out padding token embedding\n",
    "            self.embedding.weight.data[padding_idx].fill_(0)\n",
    "    \n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"Forward pass through embedding layer.\"\"\"\n",
    "        return self.embedding(token_ids)\n",
    "    \n",
    "    def get_embedding(self, token_id):\n",
    "        \"\"\"Get embedding for a specific token.\"\"\"\n",
    "        return self.embedding.weight[token_id].detach().numpy()\n",
    "    \n",
    "    def similarity(self, token_id1, token_id2):\n",
    "        \"\"\"Compute cosine similarity between two token embeddings.\"\"\"\n",
    "        emb1 = self.get_embedding(token_id1)\n",
    "        emb2 = self.get_embedding(token_id2)\n",
    "        \n",
    "        return cosine_similarity([emb1], [emb2])[0][0]\n",
    "    \n",
    "    def most_similar(self, token_id, top_k=5):\n",
    "        \"\"\"Find most similar tokens to given token.\"\"\"\n",
    "        target_emb = self.get_embedding(token_id)\n",
    "        all_embeddings = self.embedding.weight.detach().numpy()\n",
    "        \n",
    "        similarities = cosine_similarity([target_emb], all_embeddings)[0]\n",
    "        \n",
    "        # Get top-k most similar (excluding the token itself)\n",
    "        similar_indices = np.argsort(similarities)[::-1]\n",
    "        similar_indices = similar_indices[similar_indices != token_id][:top_k]\n",
    "        \n",
    "        return [(idx, similarities[idx]) for idx in similar_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our learned embedding implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create learned embedding\n",
    "embedding_dim = 8  # Small dimension for visualization\n",
    "learned_emb = LearnedEmbedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Test forward pass\n",
    "token_ids = torch.tensor([vocab['the'], vocab['cat'], vocab['dog'], vocab['mat']])\n",
    "embeddings = learned_emb(token_ids)\n",
    "\n",
    "print(f\"Learned embeddings shape: {embeddings.shape}\")\n",
    "print(\"\\nEmbedding values:\")\n",
    "for i, token_id in enumerate(token_ids):\n",
    "    token_name = [k for k, v in vocab.items() if v == token_id.item()][0]\n",
    "    print(f\"Token '{token_name}': {embeddings[i].detach().numpy()}\")\n",
    "\n",
    "# Check similarity\n",
    "sim = learned_emb.similarity(vocab['cat'], vocab['dog'])\n",
    "print(f\"\\nSimilarity between 'cat' and 'dog': {sim:.4f}\")\n",
    "\n",
    "sim_same = learned_emb.similarity(vocab['cat'], vocab['cat'])\n",
    "print(f\"Similarity between 'cat' and itself: {sim_same:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Learned Embeddings\n",
    "\n",
    "Let's visualize our learned embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_learned_embeddings(vocab, learned_emb):\n",
    "    # Get embeddings for all words\n",
    "    words = list(vocab.keys())\n",
    "    token_ids = [vocab[word] for word in words]\n",
    "    \n",
    "    # Get embeddings\n",
    "    embeddings = np.array([learned_emb.get_embedding(token_id) for token_id in token_ids])\n",
    "    \n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(embeddings, cmap='coolwarm', cbar=True, \n",
    "                xticklabels=range(learned_emb.embedding_dim), \n",
    "                yticklabels=words)\n",
    "    plt.title('Learned Embeddings')\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('Word')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create similarity matrix\n",
    "    similarity_matrix = np.zeros((len(words), len(words)))\n",
    "    for i, word1 in enumerate(words):\n",
    "        for j, word2 in enumerate(words):\n",
    "            similarity_matrix[i, j] = learned_emb.similarity(vocab[word1], vocab[word2])\n",
    "    \n",
    "    # Plot similarity matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_matrix, annot=True, fmt='.2f', cmap='YlGnBu', \n",
    "                xticklabels=words, yticklabels=words)\n",
    "    plt.title('Learned Embedding Similarity Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # PCA visualization if we have more than 2 dimensions\n",
    "    if learned_emb.embedding_dim > 2:\n",
    "        pca = PCA(n_components=2)\n",
    "        embeddings_2d = pca.fit_transform(embeddings)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.7)\n",
    "        \n",
    "        # Add labels\n",
    "        for i, word in enumerate(words):\n",
    "            plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]), \n",
    "                        fontsize=12, alpha=0.8)\n",
    "        \n",
    "        plt.title('PCA of Learned Embeddings')\n",
    "        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "# Visualize learned embeddings\n",
    "visualize_learned_embeddings(vocab, learned_emb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
