{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3: Embeddings - Part 4: Pre-trained Embeddings and Practical Exercises\n",
    "\n",
    "In this notebook, we'll explore pre-trained embeddings and work through practical exercises to deepen our understanding of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pre-trained Embeddings\n",
    "\n",
    "Let's explore pre-trained embeddings like Word2Vec and GloVe. For this notebook, we'll simulate pre-trained embeddings with a small vocabulary to demonstrate the concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_word2vec_concepts():\n",
    "    \"\"\"Demonstrate key Word2Vec concepts with simple examples.\"\"\"\n",
    "    \n",
    "    # Simulate Word2Vec-style relationships\n",
    "    # In real Word2Vec, these emerge from training\n",
    "    word_vectors = {\n",
    "        'king': np.array([0.8, 0.2, 0.9, 0.1]),\n",
    "        'queen': np.array([0.7, 0.8, 0.9, 0.2]),\n",
    "        'man': np.array([0.9, 0.1, 0.2, 0.1]),\n",
    "        'woman': np.array([0.8, 0.9, 0.2, 0.2]),\n",
    "        'prince': np.array([0.85, 0.15, 0.8, 0.1]),\n",
    "        'princess': np.array([0.75, 0.85, 0.8, 0.2]),\n",
    "        'uncle': np.array([0.7, 0.1, 0.3, 0.1]),\n",
    "        'aunt': np.array([0.6, 0.9, 0.3, 0.2]),\n",
    "        'cat': np.array([0.4, 0.3, 0.1, 0.8]),\n",
    "        'dog': np.array([0.5, 0.3, 0.2, 0.7])\n",
    "    }\n",
    "    \n",
    "    # Visualize embeddings with PCA\n",
    "    words = list(word_vectors.keys())\n",
    "    embeddings = np.array([word_vectors[word] for word in words])\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.7)\n",
    "    \n",
    "    # Add labels\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]), \n",
    "                    fontsize=12, alpha=0.8)\n",
    "    \n",
    "    plt.title('PCA of Word Vectors')\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Demonstrate vector analogies\n",
    "    def vector_analogy(a, b, c, word_vectors):\n",
    "        \"\"\"Solve analogy: a is to b as c is to ?\"\"\"\n",
    "        # Vector arithmetic: b - a + c\n",
    "        target_vector = word_vectors[b] - word_vectors[a] + word_vectors[c]\n",
    "        \n",
    "        best_match = None\n",
    "        best_similarity = -1\n",
    "        \n",
    "        for word, vector in word_vectors.items():\n",
    "            if word in [a, b, c]:  # Skip input words\n",
    "                continue\n",
    "            \n",
    "            sim = cosine_similarity([target_vector], [vector])[0][0]\n",
    "            if sim > best_similarity:\n",
    "                best_similarity = sim\n",
    "                best_match = word\n",
    "        \n",
    "        return best_match, best_similarity\n",
    "    \n",
    "    # Test analogies\n",
    "    analogies = [\n",
    "        ('king', 'queen', 'man'),  # king:queen :: man:?\n",
    "        ('man', 'woman', 'prince'),  # man:woman :: prince:?\n",
    "        ('uncle', 'aunt', 'king')  # uncle:aunt :: king:?\n",
    "    ]\n",
    "    \n",
    "    print(\"Vector Analogies:\")\n",
    "    for a, b, c in analogies:\n",
    "        result, similarity = vector_analogy(a, b, c, word_vectors)\n",
    "        print(f\"{a}:{b} :: {c}:{result} (similarity: {similarity:.4f})\")\n",
    "    \n",
    "    # Visualize analogies as vectors\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot gender direction\n",
    "    gender_direction = word_vectors['woman'] - word_vectors['man']\n",
    "    royal_direction = word_vectors['king'] - word_vectors['man']\n",
    "    \n",
    "    # Project all words onto these two directions\n",
    "    x_coords = [np.dot(word_vectors[word], royal_direction) for word in words]\n",
    "    y_coords = [np.dot(word_vectors[word], gender_direction) for word in words]\n",
    "    \n",
    "    plt.scatter(x_coords, y_coords, alpha=0.7)\n",
    "    \n",
    "    # Add labels\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, (x_coords[i], y_coords[i]), \n",
    "                    fontsize=12, alpha=0.8)\n",
    "    \n",
    "    plt.title('Word Vectors Projected onto Royal and Gender Directions')\n",
    "    plt.xlabel('Royal Direction')\n",
    "    plt.ylabel('Gender Direction')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(y=0, color='k', linewidth=0.5, alpha=0.5)\n",
    "    plt.axvline(x=0, color='k', linewidth=0.5, alpha=0.5)\n",
    "    plt.show()\n",
    "    \n",
    "    return word_vectors\n",
    "\n",
    "word_vectors = demonstrate_word2vec_concepts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pre-trained Embeddings\n",
    "\n",
    "In practice, you would load pre-trained embeddings from libraries like gensim. Here's how you would do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is for demonstration purposes\n",
    "# Uncomment and run if you have gensim installed\n",
    "\n",
    "# !pip install gensim\n",
    "\n",
    "# import gensim.downloader as api\n",
    "\n",
    "# # Load pre-trained Word2Vec embeddings\n",
    "# word2vec_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# # Load pre-trained GloVe embeddings\n",
    "# glove_model = api.load('glove-wiki-gigaword-100')\n",
    "\n",
    "# # Example usage\n",
    "# print(\"Word2Vec similarity between 'king' and 'queen':\", word2vec_model.similarity('king', 'queen'))\n",
    "# print(\"GloVe similarity between 'king' and 'queen':\", glove_model.similarity('king', 'queen'))\n",
    "\n",
    "# # Word analogy\n",
    "# result = word2vec_model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "# print(\"\\nWord2Vec analogy (king - man + woman):\", result)\n",
    "\n",
    "# result = glove_model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "# print(\"GloVe analogy (king - man + woman):\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practical Exercises\n",
    "\n",
    "### Exercise 1: Embedding Dimension Analysis\n",
    "\n",
    "Let's experiment with different embedding dimensions to understand the trade-offs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_dimension_experiment():\n",
    "    \"\"\"Experiment with different embedding dimensions.\"\"\"\n",
    "    \n",
    "    vocab_size = 100\n",
    "    dimensions = [10, 50, 100, 200, 500]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for dim in dimensions:\n",
    "        # Create embedding layer\n",
    "        embedding = nn.Embedding(vocab_size, dim)\n",
    "        \n",
    "        # Compute some statistics\n",
    "        embeddings = embedding.weight.detach().numpy()\n",
    "        \n",
    "        # Memory usage (approximate)\n",
    "        memory_mb = (vocab_size * dim * 4) / (1024 * 1024)  # 4 bytes per float32\n",
    "        \n",
    "        # Average pairwise similarity\n",
    "        similarities = cosine_similarity(embeddings)\n",
    "        np.fill_diagonal(similarities, 0)  # Remove self-similarity\n",
    "        avg_similarity = np.mean(similarities)\n",
    "        \n",
    "        results[dim] = {\n",
    "            'memory_mb': memory_mb,\n",
    "            'avg_similarity': avg_similarity,\n",
    "            'parameter_count': vocab_size * dim\n",
    "        }\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame([\n",
    "        {'dimension': dim, **stats} \n",
    "        for dim, stats in results.items()\n",
    "    ])\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Embedding Dimension Analysis:\")\n",
    "    print(\"Dim | Memory(MB) | Avg Sim | Parameters\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for dim, stats in results.items():\n",
    "        print(f\"{dim:3d} | {stats['memory_mb']:8.2f} | {stats['avg_similarity']:7.4f} | {stats['parameter_count']:10d}\")\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Memory usage\n",
    "    ax1.plot(df['dimension'], df['memory_mb'], marker='o', linewidth=2)\n",
    "    ax1.set_title('Memory Usage vs. Dimension')\n",
    "    ax1.set_xlabel('Embedding Dimension')\n",
    "    ax1.set_ylabel('Memory (MB)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Average similarity\n",
    "    ax2.plot(df['dimension'], df['avg_similarity'], marker='o', linewidth=2, color='green')\n",
    "    ax2.set_title('Average Similarity vs. Dimension')\n",
    "    ax2.set_xlabel('Embedding Dimension')\n",
    "    ax2.set_ylabel('Average Cosine Similarity')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "dimension_results = embedding_dimension_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Semantic Clustering\n",
    "\n",
    "Let's demonstrate how embeddings can capture semantic clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_clustering_demo():\n",
    "    \"\"\"Demonstrate how embeddings can capture semantic clusters.\"\"\"\n",
    "    \n",
    "    # Create word categories\n",
    "    categories = {\n",
    "        'animals': ['cat', 'dog', 'bird', 'fish', 'lion'],\n",
    "        'colors': ['red', 'blue', 'green', 'yellow', 'purple'],\n",
    "        'numbers': ['one', 'two', 'three', 'four', 'five'],\n",
    "        'actions': ['run', 'jump', 'swim', 'fly', 'walk']\n",
    "    }\n",
    "    \n",
    "    # Build vocabulary\n",
    "    vocab = {}\n",
    "    for category, words in categories.items():\n",
    "        for word in words:\n",
    "            vocab[word] = len(vocab)\n",
    "    \n",
    "    # Create embeddings (in practice, these would be trained)\n",
    "    embedding_dim = 20\n",
    "    embeddings = {}\n",
    "    \n",
    "    # Simulate category-aware embeddings\n",
    "    np.random.seed(42)\n",
    "    category_centers = {\n",
    "        'animals': np.random.normal(0, 1, embedding_dim),\n",
    "        'colors': np.random.normal(2, 1, embedding_dim),\n",
    "        'numbers': np.random.normal(-2, 1, embedding_dim),\n",
    "        'actions': np.random.normal(0, 1, embedding_dim) + np.array([0, 3] + [0]*(embedding_dim-2))\n",
    "    }\n",
    "    \n",
    "    for category, words in categories.items():\n",
    "        center = category_centers[category]\n",
    "        for word in words:\n",
    "            # Add noise around category center\n",
    "            embeddings[word] = center + np.random.normal(0, 0.3, embedding_dim)\n",
    "    \n",
    "    # Compute within-category vs between-category similarities\n",
    "    within_similarities = []\n",
    "    between_similarities = []\n",
    "    \n",
    "    for cat1, words1 in categories.items():\n",
    "        for word1 in words1:\n",
    "            for cat2, words2 in categories.items():\n",
    "                for word2 in words2:\n",
    "                    if word1 != word2:\n",
    "                        sim = cosine_similarity([embeddings[word1]], [embeddings[word2]])[0][0]\n",
    "                        if cat1 == cat2:\n",
    "                            within_similarities.append(sim)\n",
    "                        else:\n",
    "                            between_similarities.append(sim)\n",
    "    \n",
    "    print(\"Semantic Clustering Analysis:\")\n",
    "    print(f\"Within-category similarity: {np.mean(within_similarities):.4f} ± {np.std(within_similarities):.4f}\")\n",
    "    print(f\"Between-category similarity: {np.mean(between_similarities):.4f} ± {np.std(between_similarities):.4f}\")\n",
    "    \n",
    "    # Create similarity matrix\n",
    "    words = list(vocab.keys())\n",
    "    similarity_matrix = np.zeros((len(words), len(words)))\n",
    "    \n",
    "    for i, word1 in enumerate(words):\n",
    "        for j, word2 in enumerate(words):\n",
    "            similarity_matrix[i, j] = cosine_similarity([embeddings[word1]], [embeddings[word2]])[0][0]\n",
    "    \n",
    "    # Plot similarity matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Reorder words by category for better visualization\n",
    "    ordered_words = []\n",
    "    ordered_categories = []\n",
    "    for category, category_words in categories.items():\n",
    "        ordered_words.extend(category_words)\n",
    "        ordered_categories.extend([category] * len(category_words))\n",
    "    \n",
    "    # Reorder similarity matrix\n",
    "    ordered_indices = [words.index(word) for word in ordered_words]\n",
    "    ordered_matrix = similarity_matrix[np.ix_(ordered_indices, ordered_indices)]\n",
    "    \n",
    "    # Create color map for categories\n",
    "    category_colors = {'animals': 'red', 'colors': 'blue', 'numbers': 'green', 'actions': 'orange'}\n",
    "    row_colors = [category_colors[cat] for cat in ordered_categories]\n",
    "    \n",
    "    # Plot clustered heatmap\n",
    "    g = sns.clustermap(ordered_matrix, cmap='YlGnBu', \n",
    "                      row_colors=row_colors, col_colors=row_colors,\n",
    "                      xticklabels=ordered_words, yticklabels=ordered_words,\n",
    "                      figsize=(14, 12))\n",
    "    plt.title('Semantic Similarity Clustering')\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize with PCA\n",
    "    all_embeddings = np.array([embeddings[word] for word in words])\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(all_embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot points by category\n",
    "    for category, category_words in categories.items():\n",
    "        indices = [words.index(word) for word in category_words]\n",
    "        plt.scatter(embeddings_2d[indices, 0], embeddings_2d[indices, 1], \n",
    "                   label=category, alpha=0.7)\n",
    "    \n",
    "    # Add labels\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]), \n",
    "                    fontsize=12, alpha=0.8)\n",
    "    \n",
    "    plt.title('PCA of Semantic Clusters')\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return embeddings, within_similarities, between_similarities\n",
    "\n",
    "clustering_results = semantic_clustering_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **One-hot vs. Learned**: Learned embeddings capture semantic relationships, one-hot cannot\n",
    "2. **Dimensionality**: Balance between expressiveness and computational efficiency\n",
    "3. **Cosine Similarity**: Measures semantic similarity regardless of vector magnitude\n",
    "4. **Context Matters**: Embeddings become meaningful when trained with contextual information\n",
    "5. **Pre-trained Embeddings**: Leverage knowledge from large corpora for downstream tasks\n",
    "6. **Semantic Clustering**: Similar concepts naturally cluster together in embedding space\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In Day 4, we'll explore positional encodings - how transformers understand the order of tokens in sequences. We'll implement sinusoidal encodings and explore modern alternatives like RoPE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
