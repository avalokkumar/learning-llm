{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3: Embeddings - Part 3: Training Embeddings with Context\n",
    "\n",
    "In this notebook, we'll explore how embeddings become meaningful when trained with context and implement a simple contextual embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Embeddings with Context\n",
    "\n",
    "Embeddings become meaningful when trained with context. Let's implement a simple skip-gram-like model for training embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualEmbedding(nn.Module):\n",
    "    \"\"\"Embedding layer trained with contextual information.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Target word embeddings\n",
    "        self.target_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Context word embeddings\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Initialize\n",
    "        nn.init.uniform_(self.target_embeddings.weight, -0.5/embedding_dim, 0.5/embedding_dim)\n",
    "        nn.init.uniform_(self.context_embeddings.weight, -0.5/embedding_dim, 0.5/embedding_dim)\n",
    "    \n",
    "    def forward(self, target_words, context_words):\n",
    "        \"\"\"Forward pass for skip-gram training.\"\"\"\n",
    "        target_emb = self.target_embeddings(target_words)  # [batch_size, emb_dim]\n",
    "        context_emb = self.context_embeddings(context_words)  # [batch_size, emb_dim]\n",
    "        \n",
    "        # Compute similarity scores\n",
    "        scores = torch.sum(target_emb * context_emb, dim=1)  # [batch_size]\n",
    "        return scores\n",
    "    \n",
    "    def get_embedding(self, word_id):\n",
    "        \"\"\"Get final embedding for a word.\"\"\"\n",
    "        return self.target_embeddings.weight[word_id].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Training Data\n",
    "\n",
    "Let's create training data from a small corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(sentences, vocab, window_size=2):\n",
    "    \"\"\"Create skip-gram training data from sentences.\"\"\"\n",
    "    training_pairs = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        for i, target_word in enumerate(words):\n",
    "            if target_word not in vocab:\n",
    "                continue\n",
    "                \n",
    "            target_id = vocab[target_word]\n",
    "            \n",
    "            # Get context words within window\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(words), i + window_size + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                if i != j and words[j] in vocab:\n",
    "                    context_id = vocab[words[j]]\n",
    "                    training_pairs.append((target_id, context_id))\n",
    "    \n",
    "    return training_pairs\n",
    "\n",
    "# Example training setup\n",
    "sentences = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog ran in the park\",\n",
    "    \"cats and dogs are pets\",\n",
    "    \"the park has many trees\",\n",
    "    \"trees provide shade in summer\",\n",
    "    \"the cat and dog played in the park\",\n",
    "    \"birds fly in the sky above trees\",\n",
    "    \"fish swim in the water\",\n",
    "    \"pets like to play with toys\",\n",
    "    \"children play in the park with dogs\"\n",
    "]\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = {}\n",
    "for sentence in sentences:\n",
    "    for word in sentence.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Vocabulary: {vocab}\")\n",
    "\n",
    "# Create training data\n",
    "training_pairs = create_training_data(sentences, vocab, window_size=2)\n",
    "print(f\"Training pairs: {len(training_pairs)}\")\n",
    "print(f\"Sample pairs (target_id, context_id): {training_pairs[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Embeddings\n",
    "\n",
    "Let's train our contextual embeddings using negative sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embeddings(training_pairs, vocab_size, embedding_dim=50, num_epochs=100, batch_size=64, num_negative=5):\n",
    "    \"\"\"Train embeddings using skip-gram with negative sampling.\"\"\"\n",
    "    # Initialize model\n",
    "    model = ContextualEmbedding(vocab_size, embedding_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Convert training pairs to tensors\n",
    "    target_words = torch.tensor([pair[0] for pair in training_pairs])\n",
    "    context_words = torch.tensor([pair[1] for pair in training_pairs])\n",
    "    \n",
    "    # Training loop\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle data\n",
    "        indices = torch.randperm(len(training_pairs))\n",
    "        target_words = target_words[indices]\n",
    "        context_words = context_words[indices]\n",
    "        \n",
    "        total_loss = 0\n",
    "        num_batches = (len(training_pairs) + batch_size - 1) // batch_size\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            # Get batch\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(training_pairs))\n",
    "            \n",
    "            target_batch = target_words[start_idx:end_idx]\n",
    "            context_batch = context_words[start_idx:end_idx]\n",
    "            batch_size_actual = len(target_batch)\n",
    "            \n",
    "            # Generate negative samples\n",
    "            negative_samples = torch.randint(0, vocab_size, (batch_size_actual * num_negative,))\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass for positive samples\n",
    "            positive_scores = model(target_batch, context_batch)\n",
    "            positive_loss = -torch.log(torch.sigmoid(positive_scores)).mean()\n",
    "            \n",
    "            # Forward pass for negative samples\n",
    "            negative_target = target_batch.repeat_interleave(num_negative)\n",
    "            negative_scores = model(negative_target, negative_samples)\n",
    "            negative_loss = -torch.log(torch.sigmoid(-negative_scores)).mean()\n",
    "            \n",
    "            # Total loss\n",
    "            loss = positive_loss + negative_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Track progress\n",
    "        avg_loss = total_loss / num_batches\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train embeddings\n",
    "embedding_dim = 20  # Small dimension for visualization\n",
    "trained_model = train_embeddings(training_pairs, len(vocab), embedding_dim=embedding_dim, num_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Trained Embeddings\n",
    "\n",
    "Let's analyze our trained embeddings to see if they've captured semantic relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_embedding_properties(model, vocab):\n",
    "    \"\"\"Analyze properties of trained embeddings.\"\"\"\n",
    "    # Create reverse vocabulary mapping\n",
    "    vocab_reverse = {v: k for k, v in vocab.items()}\n",
    "    \n",
    "    # Get all embeddings\n",
    "    all_embeddings = []\n",
    "    labels = []\n",
    "    \n",
    "    for word_id in range(len(vocab)):\n",
    "        emb = model.get_embedding(word_id)\n",
    "        all_embeddings.append(emb)\n",
    "        labels.append(vocab_reverse[word_id])\n",
    "    \n",
    "    all_embeddings = np.array(all_embeddings)\n",
    "    \n",
    "    # Compute embedding statistics\n",
    "    print(\"Embedding Statistics:\")\n",
    "    print(f\"Shape: {all_embeddings.shape}\")\n",
    "    print(f\"Mean norm: {np.mean(np.linalg.norm(all_embeddings, axis=1)):.4f}\")\n",
    "    print(f\"Std norm: {np.std(np.linalg.norm(all_embeddings, axis=1)):.4f}\")\n",
    "    \n",
    "    # Find most similar word pairs\n",
    "    print(\"\\nMost similar word pairs:\")\n",
    "    similarities = cosine_similarity(all_embeddings)\n",
    "    \n",
    "    # Get top similar pairs (excluding self-similarity)\n",
    "    np.fill_diagonal(similarities, -1)  # Remove self-similarity\n",
    "    \n",
    "    top_pairs = []\n",
    "    for i in range(len(vocab)):\n",
    "        for j in range(i+1, len(vocab)):\n",
    "            top_pairs.append((similarities[i, j], labels[i], labels[j]))\n",
    "    \n",
    "    top_pairs.sort(reverse=True)\n",
    "    \n",
    "    for sim, word1, word2 in top_pairs[:10]:\n",
    "        print(f\"{word1} - {word2}: {sim:.4f}\")\n",
    "    \n",
    "    # Visualize with PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(all_embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.7)\n",
    "    \n",
    "    # Add labels\n",
    "    for i, label in enumerate(labels):\n",
    "        plt.annotate(label, (embeddings_2d[i, 0], embeddings_2d[i, 1]), \n",
    "                    fontsize=12, alpha=0.8)\n",
    "    \n",
    "    plt.title('PCA of Trained Word Embeddings')\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return all_embeddings, similarities, embeddings_2d\n",
    "\n",
    "# Analyze trained embeddings\n",
    "embeddings, similarities, embeddings_2d = analyze_embedding_properties(trained_model, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Semantic Relationships\n",
    "\n",
    "Let's see if our embeddings have captured semantic relationships by performing vector arithmetic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_analogy(word_a, word_b, word_c, vocab, model):\n",
    "    \"\"\"Solve analogy: a is to b as c is to ?\"\"\"\n",
    "    # Get word IDs\n",
    "    if word_a not in vocab or word_b not in vocab or word_c not in vocab:\n",
    "        return \"One or more words not in vocabulary\"\n",
    "    \n",
    "    a_id = vocab[word_a]\n",
    "    b_id = vocab[word_b]\n",
    "    c_id = vocab[word_c]\n",
    "    \n",
    "    # Get embeddings\n",
    "    a_emb = model.get_embedding(a_id)\n",
    "    b_emb = model.get_embedding(b_id)\n",
    "    c_emb = model.get_embedding(c_id)\n",
    "    \n",
    "    # Vector arithmetic: b - a + c\n",
    "    target_vector = b_emb - a_emb + c_emb\n",
    "    \n",
    "    # Find most similar word\n",
    "    vocab_reverse = {v: k for k, v in vocab.items()}\n",
    "    all_embeddings = np.array([model.get_embedding(i) for i in range(len(vocab))])\n",
    "    \n",
    "    similarities = cosine_similarity([target_vector], all_embeddings)[0]\n",
    "    \n",
    "    # Exclude input words\n",
    "    for word_id in [a_id, b_id, c_id]:\n",
    "        similarities[word_id] = -float('inf')\n",
    "    \n",
    "    # Get top results\n",
    "    top_indices = np.argsort(similarities)[::-1][:5]\n",
    "    results = [(vocab_reverse[idx], similarities[idx]) for idx in top_indices]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Try some analogies if we have the right words in our vocabulary\n",
    "analogies = [\n",
    "    ('cat', 'cats', 'dog'),  # singular:plural\n",
    "    ('dog', 'park', 'fish'),  # animal:habitat\n",
    "    ('cat', 'pets', 'tree')   # random test\n",
    "]\n",
    "\n",
    "for a, b, c in analogies:\n",
    "    if a in vocab and b in vocab and c in vocab:\n",
    "        results = vector_analogy(a, b, c, vocab, trained_model)\n",
    "        print(f\"{a} : {b} :: {c} : ?\")\n",
    "        for word, sim in results:\n",
    "            print(f\"  {word}: {sim:.4f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Context is Key**: Embeddings become meaningful when trained with contextual information\n",
    "2. **Skip-gram Model**: Predicts context words from target words, capturing semantic relationships\n",
    "3. **Vector Arithmetic**: Semantic relationships can be expressed through vector operations\n",
    "4. **Visualization**: PCA helps understand the structure of the embedding space\n",
    "5. **Training Data**: The quality and quantity of training data significantly impacts embedding quality\n",
    "\n",
    "In our small example, we may not see perfect analogies due to the limited training data. In practice, embeddings are trained on billions of words to capture rich semantic relationships."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
