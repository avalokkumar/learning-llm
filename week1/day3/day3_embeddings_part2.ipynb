{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3: Embeddings - Part 2: Cosine Similarity\n",
    "\n",
    "In this notebook, we'll explore cosine similarity in depth and how it's used to measure semantic relationships between word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cosine Similarity Deep Dive\n",
    "\n",
    "Cosine similarity measures the cosine of the angle between two vectors, focusing on direction rather than magnitude. It's a key metric for comparing embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cosine_similarity():\n",
    "    \"\"\"Visualize cosine similarity with 2D examples.\"\"\"\n",
    "    \n",
    "    # Create some 2D vectors for visualization\n",
    "    vectors = {\n",
    "        'A': np.array([1, 0]),\n",
    "        'B': np.array([1, 1]),\n",
    "        'C': np.array([0, 1]),\n",
    "        'D': np.array([-1, 0]),\n",
    "        'E': np.array([2, 0])  # Same direction as A, different magnitude\n",
    "    }\n",
    "    \n",
    "    # Compute all pairwise similarities\n",
    "    similarities = {}\n",
    "    for name1, vec1 in vectors.items():\n",
    "        for name2, vec2 in vectors.items():\n",
    "            sim = cosine_similarity([vec1], [vec2])[0][0]\n",
    "            similarities[(name1, name2)] = sim\n",
    "    \n",
    "    # Print similarity matrix\n",
    "    print(\"Cosine Similarity Matrix:\")\n",
    "    print(\"    \", end=\"\")\n",
    "    for name in vectors.keys():\n",
    "        print(f\"{name:6s}\", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    for name1 in vectors.keys():\n",
    "        print(f\"{name1}: \", end=\"\")\n",
    "        for name2 in vectors.keys():\n",
    "            sim = similarities[(name1, name2)]\n",
    "            print(f\"{sim:6.2f}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    # Visualize vectors\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "    \n",
    "    for i, (name, vec) in enumerate(vectors.items()):\n",
    "        plt.arrow(0, 0, vec[0], vec[1], head_width=0.1, head_length=0.1, \n",
    "                 fc=colors[i], ec=colors[i], label=f'Vector {name}')\n",
    "        plt.text(vec[0]*1.1, vec[1]*1.1, name, fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add angle annotations\n",
    "    def add_angle_annotation(vec1_name, vec2_name, radius=0.5):\n",
    "        vec1 = vectors[vec1_name]\n",
    "        vec2 = vectors[vec2_name]\n",
    "        \n",
    "        # Normalize vectors\n",
    "        vec1_norm = vec1 / np.linalg.norm(vec1)\n",
    "        vec2_norm = vec2 / np.linalg.norm(vec2)\n",
    "        \n",
    "        # Calculate angle\n",
    "        angle = np.arccos(np.clip(np.dot(vec1_norm, vec2_norm), -1.0, 1.0))\n",
    "        angle_deg = np.degrees(angle)\n",
    "        \n",
    "        # Draw arc\n",
    "        theta1 = np.arctan2(vec1[1], vec1[0])\n",
    "        theta2 = np.arctan2(vec2[1], vec2[0])\n",
    "        \n",
    "        # Ensure we draw the smaller angle\n",
    "        if abs(theta2 - theta1) > np.pi:\n",
    "            if theta1 < theta2:\n",
    "                theta1 += 2 * np.pi\n",
    "            else:\n",
    "                theta2 += 2 * np.pi\n",
    "                \n",
    "        # Draw arc\n",
    "        theta = np.linspace(min(theta1, theta2), max(theta1, theta2), 100)\n",
    "        x = radius * np.cos(theta)\n",
    "        y = radius * np.sin(theta)\n",
    "        plt.plot(x, y, 'k--', alpha=0.3)\n",
    "        \n",
    "        # Add angle text at midpoint\n",
    "        mid_theta = (theta1 + theta2) / 2\n",
    "        plt.text(radius * 1.2 * np.cos(mid_theta), \n",
    "                 radius * 1.2 * np.sin(mid_theta), \n",
    "                 f\"{angle_deg:.1f}Â°\", fontsize=10)\n",
    "        \n",
    "        return angle_deg\n",
    "    \n",
    "    # Add some angle annotations\n",
    "    add_angle_annotation('A', 'B')\n",
    "    add_angle_annotation('B', 'C')\n",
    "    add_angle_annotation('A', 'C')\n",
    "    add_angle_annotation('A', 'D')\n",
    "    \n",
    "    plt.xlim(-1.5, 2.5)\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "    plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "    plt.title('Vector Visualization for Cosine Similarity')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.axis('equal')  # Equal scale\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a heatmap of similarities\n",
    "    sim_matrix = np.zeros((len(vectors), len(vectors)))\n",
    "    for i, name1 in enumerate(vectors.keys()):\n",
    "        for j, name2 in enumerate(vectors.keys()):\n",
    "            sim_matrix[i, j] = similarities[(name1, name2)]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(sim_matrix, annot=True, fmt='.2f', cmap='YlGnBu',\n",
    "                xticklabels=vectors.keys(), yticklabels=vectors.keys())\n",
    "    plt.title('Cosine Similarity Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "# Run visualization\n",
    "similarities = visualize_cosine_similarity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Similarity Measures\n",
    "\n",
    "Let's compare cosine similarity with other similarity measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_measures(emb1, emb2):\n",
    "    \"\"\"Compute various similarity measures between embeddings.\"\"\"\n",
    "    \n",
    "    # Cosine similarity\n",
    "    cos_sim = cosine_similarity([emb1], [emb2])[0][0]\n",
    "    \n",
    "    # Euclidean distance (lower = more similar)\n",
    "    euclidean_dist = np.linalg.norm(emb1 - emb2)\n",
    "    \n",
    "    # Manhattan distance\n",
    "    manhattan_dist = np.sum(np.abs(emb1 - emb2))\n",
    "    \n",
    "    # Dot product (unnormalized)\n",
    "    dot_product = np.dot(emb1, emb2)\n",
    "    \n",
    "    return {\n",
    "        'cosine_similarity': cos_sim,\n",
    "        'euclidean_distance': euclidean_dist,\n",
    "        'manhattan_distance': manhattan_dist,\n",
    "        'dot_product': dot_product\n",
    "    }\n",
    "\n",
    "# Generate random embeddings for demonstration\n",
    "np.random.seed(42)  # For reproducibility\n",
    "emb_dim = 50\n",
    "\n",
    "# Create random embeddings\n",
    "emb1 = np.random.normal(0, 1, emb_dim)\n",
    "emb2 = np.random.normal(0, 1, emb_dim)  # Completely different\n",
    "emb3 = emb1 + 0.1 * np.random.normal(0, 1, emb_dim)  # Similar to emb1\n",
    "emb4 = -emb1 + 0.1 * np.random.normal(0, 1, emb_dim)  # Opposite to emb1\n",
    "\n",
    "# Compare similarity measures\n",
    "print(\"Random vs Random:\")\n",
    "sim_random = compute_similarity_measures(emb1, emb2)\n",
    "for measure, value in sim_random.items():\n",
    "    print(f\"{measure}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nSimilar Embeddings:\")\n",
    "sim_similar = compute_similarity_measures(emb1, emb3)\n",
    "for measure, value in sim_similar.items():\n",
    "    print(f\"{measure}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nOpposite Embeddings:\")\n",
    "sim_opposite = compute_similarity_measures(emb1, emb4)\n",
    "for measure, value in sim_opposite.items():\n",
    "    print(f\"{measure}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Similarity in High Dimensions\n",
    "\n",
    "Let's create a visualization to understand how similarity behaves in high-dimensional spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_high_dim_similarity(dimensions, num_vectors=1000):\n",
    "    \"\"\"Visualize how similarity behaves in high-dimensional spaces.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for dim in dimensions:\n",
    "        # Generate random unit vectors\n",
    "        vectors = np.random.normal(0, 1, (num_vectors, dim))\n",
    "        \n",
    "        # Normalize to unit length\n",
    "        norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "        vectors = vectors / norms\n",
    "        \n",
    "        # Compute pairwise similarities for a subset\n",
    "        subset_size = min(100, num_vectors)  # Limit computation for large sets\n",
    "        subset = vectors[:subset_size]\n",
    "        similarities = cosine_similarity(subset)\n",
    "        \n",
    "        # Remove self-similarities\n",
    "        np.fill_diagonal(similarities, np.nan)\n",
    "        similarities = similarities.flatten()\n",
    "        similarities = similarities[~np.isnan(similarities)]\n",
    "        \n",
    "        # Compute statistics\n",
    "        results.append({\n",
    "            'dimension': dim,\n",
    "            'mean_similarity': np.mean(similarities),\n",
    "            'std_similarity': np.std(similarities),\n",
    "            'min_similarity': np.min(similarities),\n",
    "            'max_similarity': np.max(similarities),\n",
    "            'similarities': similarities\n",
    "        })\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot mean similarity vs dimension\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot([r['dimension'] for r in results], \n",
    "             [r['mean_similarity'] for r in results], \n",
    "             'o-', linewidth=2)\n",
    "    plt.fill_between([r['dimension'] for r in results],\n",
    "                     [r['mean_similarity'] - r['std_similarity'] for r in results],\n",
    "                     [r['mean_similarity'] + r['std_similarity'] for r in results],\n",
    "                     alpha=0.2)\n",
    "    plt.title('Mean Cosine Similarity vs Dimension')\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('Mean Similarity')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot distribution for selected dimensions\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i, dim in enumerate([dimensions[0], dimensions[-1]]):\n",
    "        result = next(r for r in results if r['dimension'] == dim)\n",
    "        sns.histplot(result['similarities'], kde=True, \n",
    "                    label=f'Dim={dim}', alpha=0.5)\n",
    "    \n",
    "    plt.title('Distribution of Cosine Similarities')\n",
    "    plt.xlabel('Cosine Similarity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Visualize similarity in different dimensions\n",
    "dimensions = [2, 5, 10, 50, 100, 300, 1000]\n",
    "high_dim_results = visualize_high_dim_similarity(dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights about Cosine Similarity\n",
    "\n",
    "1. **Direction vs. Magnitude**: Cosine similarity focuses on the angle between vectors, ignoring their magnitudes\n",
    "2. **Range**: Values range from -1 (opposite directions) to 1 (same direction), with 0 indicating orthogonality\n",
    "3. **High Dimensions**: In high-dimensional spaces, random vectors tend to be nearly orthogonal\n",
    "4. **Semantic Meaning**: In word embeddings, cosine similarity captures semantic relationships\n",
    "5. **Efficiency**: Computationally efficient for sparse high-dimensional vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
