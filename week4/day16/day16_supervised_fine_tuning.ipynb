{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 16: Supervised Fine-Tuning (SFT) Implementation\n",
    "\n",
    "In this notebook, we'll implement Supervised Fine-Tuning (SFT) on a small pre-trained language model. We'll use a subset of instruction data to fine-tune the model to follow instructions and generate helpful responses.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Load a small pre-trained model\n",
    "2. Prepare instruction dataset\n",
    "3. Format data for training\n",
    "4. Implement SFT training loop\n",
    "5. Evaluate the fine-tuned model\n",
    "6. Compare pre-trained vs. fine-tuned outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load a Small Pre-trained Model\n",
    "\n",
    "We'll use a small pre-trained model for this demonstration. In a real-world scenario, you might use larger models like Llama, Mistral, or GPT-J."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, we'll use a small model to make it runnable on most hardware\n",
    "model_name = \"gpt2\"  # ~124M parameters\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Number of parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Instruction Dataset\n",
    "\n",
    "We'll create a small synthetic instruction dataset for demonstration purposes. In practice, you would use datasets like Stanford Alpaca, Dolly, or FLAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small synthetic instruction dataset\n",
    "synthetic_instructions = [\n",
    "    {\n",
    "        \"instruction\": \"Explain the concept of machine learning in simple terms.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Machine learning is when computers learn from examples instead of being explicitly programmed. It's like teaching a child to recognize dogs by showing them many pictures of dogs, rather than listing all the features that make a dog a dog.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Write a short poem about the ocean.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Vast blue expanse beneath the sky,\\nWaves dance and crash as time goes by.\\nSecrets deep in waters cold,\\nOcean stories, forever told.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Summarize the following paragraph.\",\n",
    "        \"input\": \"Artificial intelligence has made significant strides in recent years, transforming industries from healthcare to finance. Machine learning algorithms now power recommendation systems, autonomous vehicles, and medical diagnostic tools. Despite these advances, challenges remain in areas such as bias mitigation, explainability, and ensuring AI systems align with human values.\",\n",
    "        \"output\": \"AI has advanced rapidly, impacting healthcare and finance through applications like recommendation systems and autonomous vehicles, though challenges persist in bias, explainability, and alignment with human values.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Translate the following English text to French.\",\n",
    "        \"input\": \"Hello, how are you today? I hope you're doing well.\",\n",
    "        \"output\": \"Bonjour, comment allez-vous aujourd'hui ? J'espère que vous allez bien.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain the difference between RAM and ROM.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"RAM (Random Access Memory) is temporary, volatile memory that stores data a computer is actively using. It's erased when the computer is turned off. ROM (Read-Only Memory) is permanent, non-volatile memory that stores instructions needed to start up a computer. It retains data even when the computer is turned off.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Generate a list of 5 creative uses for a paperclip.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"1. Use as a tiny bookmark for specific lines in a book\\n2. Create miniature sculptures or art pieces\\n3. Use as a makeshift phone stand by bending it appropriately\\n4. Make a simple hook for hanging small items\\n5. Use as an emergency zipper pull replacement\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain the concept of photosynthesis.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Photosynthesis is the process by which plants, algae, and some bacteria convert sunlight, water, and carbon dioxide into glucose (sugar) and oxygen. The plants use the glucose as food for energy and growth, while the oxygen is released into the atmosphere. This process is essential for life on Earth as it produces oxygen and forms the base of most food chains.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Provide three tips for effective time management.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"1. Prioritize tasks using methods like the Eisenhower Matrix (urgent/important)\\n2. Break large projects into smaller, manageable tasks with specific deadlines\\n3. Use the Pomodoro Technique: work in focused 25-minute intervals followed by short breaks\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Calculate the area of a circle with radius 5 cm.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"To calculate the area of a circle, I use the formula: Area = π × r². With a radius of 5 cm:\\nArea = π × 5² = π × 25 = 78.54 cm²\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain the difference between weather and climate.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Weather refers to short-term atmospheric conditions in a specific place and time, such as temperature, humidity, precipitation, and wind. Climate, on the other hand, describes the long-term patterns of weather in a particular region, typically averaged over a period of 30 years or more. Simply put, weather is what you experience day to day, while climate is the average weather pattern of a region over many years.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "df = pd.DataFrame(synthetic_instructions)\n",
    "print(f\"Dataset size: {len(df)} examples\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option: Load a Real Instruction Dataset\n",
    "\n",
    "Alternatively, you can load a real instruction dataset from the Hugging Face Hub. Uncomment the code below to use the Stanford Alpaca dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load a subset of the Stanford Alpaca dataset\n",
    "# try:\n",
    "#     dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:100]\")\n",
    "#     df = pd.DataFrame({\n",
    "#         \"instruction\": dataset[\"instruction\"],\n",
    "#         \"input\": dataset[\"input\"],\n",
    "#         \"output\": dataset[\"output\"]\n",
    "#     })\n",
    "#     print(f\"Loaded {len(df)} examples from Stanford Alpaca dataset\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading dataset: {e}\")\n",
    "#     print(\"Using synthetic dataset instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Format Data for Training\n",
    "\n",
    "We need to format our instruction data for the model. We'll use a template that combines the instruction, input, and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(row):\n",
    "    \"\"\"Format instruction data into a single string.\"\"\"\n",
    "    instruction = row[\"instruction\"]\n",
    "    input_text = row[\"input\"]\n",
    "    output = row[\"output\"]\n",
    "    \n",
    "    if input_text:\n",
    "        formatted = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output}\"\n",
    "    else:\n",
    "        formatted = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}\"\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "# Apply formatting to our dataset\n",
    "df[\"formatted_text\"] = df.apply(format_instruction, axis=1)\n",
    "\n",
    "# Display an example\n",
    "print(\"Example of formatted instruction:\")\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "print(df[\"formatted_text\"].iloc[2])\n",
    "print(\"\\n\" + \"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenize and Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation sets\n",
    "train_df = df.sample(frac=0.8, random_state=42)\n",
    "val_df = df.drop(train_df.index)\n",
    "\n",
    "print(f\"Training examples: {len(train_df)}\")\n",
    "print(f\"Validation examples: {len(val_df)}\")\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"formatted_text\"]])\n",
    "val_dataset = Dataset.from_pandas(val_df[[\"formatted_text\"]])\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"formatted_text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"formatted_text\"]\n",
    ")\n",
    "\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"formatted_text\"]\n",
    ")\n",
    "\n",
    "# Add labels for causal language modeling (labels are the same as input_ids)\n",
    "tokenized_train_dataset = tokenized_train_dataset.map(lambda examples: {\"labels\": examples[\"input_ids\"]})\n",
    "tokenized_val_dataset = tokenized_val_dataset.map(lambda examples: {\"labels\": examples[\"input_ids\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Set Up LoRA for Parameter-Efficient Fine-Tuning\n",
    "\n",
    "Instead of fine-tuning all parameters, we'll use LoRA (Low-Rank Adaptation) to make the process more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,  # Rank of the update matrices\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    lora_dropout=0.1,  # Dropout probability for LoRA layers\n",
    "    target_modules=[\"c_attn\", \"c_proj\"]  # Layers to apply LoRA to (specific to GPT-2)\n",
    ")\n",
    "\n",
    "# Create LoRA model\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Set Up Training Arguments and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if available\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients over 2 steps\n",
    "    report_to=\"none\"  # Disable reporting to avoid dependencies\n",
    ")\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # We're doing causal language modeling, not masked language modeling\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Model\n",
    "\n",
    "Now we'll fine-tune the model on our instruction dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate the Model\n",
    "\n",
    "Let's compare the outputs of the pre-trained model and our fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model_path = \"./fine_tuned_model\"\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# Load the original pre-trained model for comparison\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "original_model.eval()\n",
    "\n",
    "# Load the fine-tuned model\n",
    "fine_tuned_model = model.to(device)\n",
    "fine_tuned_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, instruction, input_text=\"\"):\n",
    "    \"\"\"Generate a response from the model given an instruction and optional input.\"\"\"\n",
    "    # Format the prompt\n",
    "    if input_text:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the response part\n",
    "    response = response.split(\"### Response:\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test instructions\n",
    "test_instructions = [\n",
    "    {\"instruction\": \"Explain what a neural network is in simple terms.\", \"input\": \"\"},\n",
    "    {\"instruction\": \"Write a short poem about mountains.\", \"input\": \"\"},\n",
    "    {\"instruction\": \"Summarize the following text.\", \"input\": \"Climate change is the long-term alteration of temperature and typical weather patterns in a place. It could refer to a particular location or the planet as a whole. Climate change has been connected to damaging weather events such as more frequent and more intense hurricanes, floods, downpours, and winter storms.\"}\n",
    "]\n",
    "\n",
    "# Compare original and fine-tuned model outputs\n",
    "for test in test_instructions:\n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "    print(f\"Instruction: {test['instruction']}\")\n",
    "    if test['input']:\n",
    "        print(f\"Input: {test['input']}\")\n",
    "    \n",
    "    print(\"\\nOriginal Model Response:\")\n",
    "    original_response = generate_response(original_model, test['instruction'], test['input'])\n",
    "    print(original_response)\n",
    "    \n",
    "    print(\"\\nFine-tuned Model Response:\")\n",
    "    fine_tuned_response = generate_response(fine_tuned_model, test['instruction'], test['input'])\n",
    "    print(fine_tuned_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced: Evaluating Instruction Following\n",
    "\n",
    "Let's evaluate how well our model follows instructions using a simple scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_instruction_following(model, test_cases):\n",
    "    \"\"\"Evaluate how well the model follows instructions.\"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for i, test in enumerate(test_cases):\n",
    "        instruction = test[\"instruction\"]\n",
    "        input_text = test.get(\"input\", \"\")\n",
    "        expected_type = test[\"expected_type\"]\n",
    "        \n",
    "        # Generate response\n",
    "        response = generate_response(model, instruction, input_text)\n",
    "        \n",
    "        # Score based on expected response type\n",
    "        score = 0\n",
    "        if expected_type == \"explanation\" and len(response.split()) > 20:\n",
    "            score = 1  # Basic check for explanation length\n",
    "        elif expected_type == \"poem\" and \"\\n\" in response:\n",
    "            score = 1  # Basic check for poem format (line breaks)\n",
    "        elif expected_type == \"summary\" and len(response.split()) < len(input_text.split()) * 0.7:\n",
    "            score = 1  # Basic check for summary (shorter than input)\n",
    "        \n",
    "        scores.append(score)\n",
    "        \n",
    "        print(f\"Test {i+1}: {'Passed' if score == 1 else 'Failed'}\")\n",
    "        print(f\"Instruction: {instruction}\")\n",
    "        if input_text:\n",
    "            print(f\"Input: {input_text[:50]}...\")\n",
    "        print(f\"Response: {response[:100]}...\" if len(response) > 100 else f\"Response: {response}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    avg_score = sum(scores) / len(scores) if scores else 0\n",
    "    print(f\"\\nAverage instruction following score: {avg_score:.2f} ({sum(scores)}/{len(scores)} tests passed)\")\n",
    "    \n",
    "    return avg_score\n",
    "\n",
    "# Test cases with expected response types\n",
    "test_cases = [\n",
    "    {\"instruction\": \"Explain what a neural network is in simple terms.\", \"input\": \"\", \"expected_type\": \"explanation\"},\n",
    "    {\"instruction\": \"Write a short poem about mountains.\", \"input\": \"\", \"expected_type\": \"poem\"},\n",
    "    {\"instruction\": \"Summarize the following text.\", \"input\": \"Climate change is the long-term alteration of temperature and typical weather patterns in a place. It could refer to a particular location or the planet as a whole. Climate change has been connected to damaging weather events such as more frequent and more intense hurricanes, floods, downpours, and winter storms.\", \"expected_type\": \"summary\"}\n",
    "]\n",
    "\n",
    "print(\"Evaluating original model:\")\n",
    "original_score = evaluate_instruction_following(original_model, test_cases)\n",
    "\n",
    "print(\"\\nEvaluating fine-tuned model:\")\n",
    "fine_tuned_score = evaluate_instruction_following(fine_tuned_model, test_cases)\n",
    "\n",
    "print(f\"\\nImprovement: {(fine_tuned_score - original_score) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "In this notebook, we've implemented Supervised Fine-Tuning (SFT) on a small pre-trained language model. We've seen how SFT can improve a model's ability to follow instructions and generate helpful responses.\n",
    "\n",
    "Key takeaways:\n",
    "1. SFT is a straightforward but powerful technique for aligning language models with human intent\n",
    "2. Even with a small dataset, we can see improvements in instruction following\n",
    "3. Parameter-efficient fine-tuning methods like LoRA make SFT feasible on consumer hardware\n",
    "4. The quality of the instruction dataset is crucial for effective SFT\n",
    "\n",
    "Next steps:\n",
    "- Experiment with larger instruction datasets\n",
    "- Try different prompt formats\n",
    "- Explore more advanced alignment techniques like RLHF and DPO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
