{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 17: RLHF Components - Part 2\n",
    "\n",
    "This notebook continues our exploration of RLHF components. In this part, we'll focus on:\n",
    "\n",
    "1. Training the reward model\n",
    "2. Implementing a simplified PPO algorithm\n",
    "3. Optimizing our policy model using PPO\n",
    "4. Evaluating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries (if running this notebook independently)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Reward Model\n",
    "\n",
    "Now we'll train our reward model to predict human preferences. The model should assign higher rewards to preferred responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reward model training loop\n",
    "def train_reward_model(model, train_dataset, val_dataset, epochs=3, batch_size=4, lr=1e-5):\n",
    "    \"\"\"Train the reward model on preference data.\"\"\"\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            # Get batch data\n",
    "            chosen_input_ids = batch[\"chosen_input_ids\"].to(device)\n",
    "            chosen_attention_mask = batch[\"chosen_attention_mask\"].to(device)\n",
    "            rejected_input_ids = batch[\"rejected_input_ids\"].to(device)\n",
    "            rejected_attention_mask = batch[\"rejected_attention_mask\"].to(device)\n",
    "            \n",
    "            # Forward pass for chosen responses\n",
    "            chosen_rewards = model(chosen_input_ids, chosen_attention_mask)\n",
    "            \n",
    "            # Forward pass for rejected responses\n",
    "            rejected_rewards = model(rejected_input_ids, rejected_attention_mask)\n",
    "            \n",
    "            # Compute loss (chosen should have higher reward than rejected)\n",
    "            loss = -torch.log(torch.sigmoid(chosen_rewards - rejected_rewards)).mean()\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Get batch data\n",
    "                chosen_input_ids = batch[\"chosen_input_ids\"].to(device)\n",
    "                chosen_attention_mask = batch[\"chosen_attention_mask\"].to(device)\n",
    "                rejected_input_ids = batch[\"rejected_input_ids\"].to(device)\n",
    "                rejected_attention_mask = batch[\"rejected_attention_mask\"].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                chosen_rewards = model(chosen_input_ids, chosen_attention_mask)\n",
    "                rejected_rewards = model(rejected_input_ids, rejected_attention_mask)\n",
    "                \n",
    "                # Count correct predictions (chosen should have higher reward)\n",
    "                correct += (chosen_rewards > rejected_rewards).sum().item()\n",
    "                total += chosen_rewards.size(0)\n",
    "        \n",
    "        val_accuracy = correct / total\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    return train_losses, val_accuracies\n",
    "\n",
    "# Train the reward model\n",
    "# Note: In a real implementation, you would train for more epochs and with more data\n",
    "train_losses, val_accuracies = train_reward_model(\n",
    "    reward_model, \n",
    "    tokenized_train_dataset, \n",
    "    tokenized_val_dataset,\n",
    "    epochs=2,  # Using a small number for demonstration\n",
    "    batch_size=2,\n",
    "    lr=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_accuracies)\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing the Reward Model\n",
    "\n",
    "Let's test our trained reward model on some examples to see if it can predict human preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(model, prompt, response):\n",
    "    \"\"\"Get reward for a prompt-response pair.\"\"\"\n",
    "    input_text = f\"Prompt: {prompt}\\nResponse: {response}\"\n",
    "    inputs = reward_tokenizer(\n",
    "        input_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        reward = model(inputs[\"input_ids\"], inputs[\"attention_mask\"]).item()\n",
    "    \n",
    "    return reward\n",
    "\n",
    "# Test the reward model on some examples\n",
    "test_examples = [\n",
    "    {\n",
    "        \"prompt\": \"Explain the concept of machine learning.\",\n",
    "        \"good_response\": \"Machine learning is a branch of artificial intelligence that enables computers to learn from data and improve their performance on a task without being explicitly programmed. It works by identifying patterns in data and using those patterns to make predictions or decisions.\",\n",
    "        \"bad_response\": \"Machine learning is when computers do stuff with data.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Write a poem about the moon.\",\n",
    "        \"good_response\": \"Silver orb in midnight sky,\\nSilent guardian watching high.\\nCasting light on dreams below,\\nAncient wisdom you bestow.\\nCraters tell your timeless tale,\\nAs you wax and as you wane.\",\n",
    "        \"bad_response\": \"Moon is big. Moon is bright. Moon is in the sky at night.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Evaluate the reward model\n",
    "reward_model.eval()\n",
    "for example in test_examples:\n",
    "    prompt = example[\"prompt\"]\n",
    "    good_response = example[\"good_response\"]\n",
    "    bad_response = example[\"bad_response\"]\n",
    "    \n",
    "    good_reward = get_reward(reward_model, prompt, good_response)\n",
    "    bad_reward = get_reward(reward_model, prompt, bad_response)\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Good response reward: {good_reward:.4f}\")\n",
    "    print(f\"Bad response reward: {bad_reward:.4f}\")\n",
    "    print(f\"Correctly identified better response: {good_reward > bad_reward}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implementing a Simplified PPO Algorithm\n",
    "\n",
    "Now we'll implement a simplified version of the PPO algorithm for language models. In a real RLHF pipeline, this would be much more complex, but this simplified version will help us understand the key components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePPO:\n",
    "    \"\"\"Simplified PPO implementation for language models.\"\"\"\n",
    "    \n",
    "    def __init__(self, policy_model, ref_model, reward_model, tokenizer, device):\n",
    "        self.policy_model = policy_model\n",
    "        self.ref_model = ref_model\n",
    "        self.reward_model = reward_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        \n",
    "        # PPO hyperparameters\n",
    "        self.kl_coef = 0.1  # KL penalty coefficient\n",
    "        self.clip_param = 0.2  # PPO clip parameter\n",
    "        \n",
    "    def generate_responses(self, prompts, max_length=100):\n",
    "        \"\"\"Generate responses from the policy model.\"\"\"\n",
    "        responses = []\n",
    "        log_probs = []\n",
    "        \n",
    "        for prompt in prompts:\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            \n",
    "            # Generate from policy model\n",
    "            with torch.no_grad():\n",
    "                output = self.policy_model.generate(\n",
    "                    input_ids,\n",
    "                    max_length=max_length,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True\n",
    "                )\n",
    "            \n",
    "            # Extract generated tokens and scores\n",
    "            generated_ids = output.sequences[0]\n",
    "            scores = output.scores\n",
    "            \n",
    "            # Compute log probabilities (simplified)\n",
    "            response_log_prob = 0.0\n",
    "            for i, score in enumerate(scores):\n",
    "                token_id = generated_ids[input_ids.size(1) + i].item()\n",
    "                token_log_prob = F.log_softmax(score[0], dim=-1)[token_id].item()\n",
    "                response_log_prob += token_log_prob\n",
    "            \n",
    "            # Decode response (remove prompt)\n",
    "            response = self.tokenizer.decode(generated_ids[input_ids.size(1):], skip_special_tokens=True)\n",
    "            \n",
    "            responses.append(response)\n",
    "            log_probs.append(response_log_prob)\n",
    "        \n",
    "        return responses, log_probs\n",
    "    \n",
    "    def compute_rewards(self, prompts, responses):\n",
    "        \"\"\"Compute rewards for prompt-response pairs.\"\"\"\n",
    "        rewards = []\n",
    "        \n",
    "        for prompt, response in zip(prompts, responses):\n",
    "            reward = get_reward(self.reward_model, prompt, response)\n",
    "            rewards.append(reward)\n",
    "        \n",
    "        return rewards\n",
    "    \n",
    "    def compute_kl_penalty(self, prompts, responses):\n",
    "        \"\"\"Compute KL divergence between policy and reference model (simplified).\"\"\"\n",
    "        kl_penalties = []\n",
    "        \n",
    "        for prompt, response in zip(prompts, responses):\n",
    "            # This is a very simplified KL calculation\n",
    "            # In a real implementation, you would compute token-by-token KL\n",
    "            input_text = prompt + response\n",
    "            inputs = self.tokenizer(input_text, return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Get logits from both models\n",
    "                policy_outputs = self.policy_model(inputs[\"input_ids\"])\n",
    "                ref_outputs = self.ref_model(inputs[\"input_ids\"])\n",
    "                \n",
    "                policy_logits = policy_outputs.logits\n",
    "                ref_logits = ref_outputs.logits\n",
    "                \n",
    "                # Compute KL divergence (simplified)\n",
    "                policy_probs = F.softmax(policy_logits, dim=-1)\n",
    "                ref_probs = F.softmax(ref_logits, dim=-1)\n",
    "                \n",
    "                kl = F.kl_div(\n",
    "                    F.log_softmax(policy_logits, dim=-1),\n",
    "                    ref_probs,\n",
    "                    reduction=\"batchmean\"\n",
    "                )\n",
    "                \n",
    "                kl_penalties.append(kl.item())\n",
    "        \n",
    "        return kl_penalties\n",
    "    \n",
    "    def ppo_step(self, prompts, batch_size=4, lr=1e-6):\n",
    "        \"\"\"Perform one PPO update step.\"\"\"\n",
    "        # Set policy model to training mode\n",
    "        self.policy_model.train()\n",
    "        \n",
    "        # Create optimizer\n",
    "        optimizer = torch.optim.AdamW(self.policy_model.parameters(), lr=lr)\n",
    "        \n",
    "        # Generate responses and compute rewards\n",
    "        responses, old_log_probs = self.generate_responses(prompts)\n",
    "        rewards = self.compute_rewards(prompts, responses)\n",
    "        kl_penalties = self.compute_kl_penalty(prompts, responses)\n",
    "        \n",
    "        # Compute advantages (reward - KL penalty)\n",
    "        advantages = [r - self.kl_coef * kl for r, kl in zip(rewards, kl_penalties)]\n",
    "        \n",
    "        # PPO update (simplified)\n",
    "        for i in range(0, len(prompts), batch_size):\n",
    "            batch_prompts = prompts[i:i+batch_size]\n",
    "            batch_responses = responses[i:i+batch_size]\n",
    "            batch_old_log_probs = old_log_probs[i:i+batch_size]\n",
    "            batch_advantages = advantages[i:i+batch_size]\n",
    "            \n",
    "            # This is a simplified PPO update\n",
    "            # In a real implementation, you would compute token-by-token probabilities\n",
    "            # and apply the PPO objective properly\n",
    "            \n",
    "            # For demonstration purposes, we'll just print the values\n",
    "            for j in range(len(batch_prompts)):\n",
    "                print(f\"Prompt: {batch_prompts[j]}\")\n",
    "                print(f\"Response: {batch_responses[j]}\")\n",
    "                print(f\"Reward: {rewards[i+j]:.4f}\")\n",
    "                print(f\"KL Penalty: {kl_penalties[i+j]:.4f}\")\n",
    "                print(f\"Advantage: {batch_advantages[j]:.4f}\")\n",
    "                print(\"-\" * 50)\n",
    "        \n",
    "        # In a real implementation, you would:\n",
    "        # 1. Compute new log probs for the same responses\n",
    "        # 2. Compute ratio = exp(new_log_probs - old_log_probs)\n",
    "        # 3. Compute clipped objective: min(ratio * advantage, clip(ratio, 1-ε, 1+ε) * advantage)\n",
    "        # 4. Optimize this objective\n",
    "        \n",
    "        # For demonstration, we'll just return the average reward\n",
    "        return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Running the PPO Algorithm\n",
    "\n",
    "Now let's run our simplified PPO algorithm to optimize the policy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the SFT model to serve as the reference model\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "ref_model.eval()  # Reference model is frozen\n",
    "\n",
    "# Initialize PPO\n",
    "ppo = SimplePPO(\n",
    "    policy_model=sft_model,\n",
    "    ref_model=ref_model,\n",
    "    reward_model=reward_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Sample prompts for PPO training\n",
    "ppo_prompts = [\n",
    "    \"Explain the concept of reinforcement learning.\",\n",
    "    \"Write a short story about a robot learning to feel emotions.\"\n",
    "]\n",
    "\n",
    "# Run one PPO step (in a real implementation, you would run many steps)\n",
    "avg_reward = ppo.ppo_step(ppo_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparing SFT and RLHF Models\n",
    "\n",
    "Finally, let's compare the outputs of our original SFT model and the RLHF-optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, prompt, max_length=100):\n",
    "    \"\"\"Generate a response from the model given a prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    response = response[len(prompt):].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Explain quantum computing to a high school student.\",\n",
    "    \"Write a poem about artificial intelligence.\",\n",
    "    \"What are three ways to improve productivity?\"\n",
    "]\n",
    "\n",
    "# Compare SFT and RLHF models\n",
    "for prompt in test_prompts:\n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    \n",
    "    # Generate from SFT model (reference model)\n",
    "    ref_response = generate_response(ref_model, prompt)\n",
    "    print(f\"SFT Model Response:\\n{ref_response}\\n\")\n",
    "    \n",
    "    # Generate from RLHF model (policy model)\n",
    "    policy_response = generate_response(sft_model, prompt)\n",
    "    print(f\"RLHF Model Response:\\n{policy_response}\\n\")\n",
    "    \n",
    "    # Get rewards\n",
    "    ref_reward = get_reward(reward_model, prompt, ref_response)\n",
    "    policy_reward = get_reward(reward_model, prompt, policy_response)\n",
    "    \n",
    "    print(f\"SFT Model Reward: {ref_reward:.4f}\")\n",
    "    print(f\"RLHF Model Reward: {policy_reward:.4f}\")\n",
    "    print(f\"Improvement: {policy_reward - ref_reward:.4f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "In this notebook, we've implemented simplified versions of the key components of the RLHF pipeline:\n",
    "\n",
    "1. **Reward Model Training**: We trained a model to predict human preferences between pairs of responses.\n",
    "2. **PPO Algorithm**: We implemented a simplified version of PPO for language models.\n",
    "3. **Policy Optimization**: We used PPO to optimize our policy model based on the reward model's feedback.\n",
    "\n",
    "In a real RLHF implementation, each of these components would be much more complex and would require more data, compute resources, and careful tuning. However, this simplified implementation helps us understand the core concepts and how they fit together.\n",
    "\n",
    "Key takeaways:\n",
    "- RLHF builds on SFT by optimizing for human preferences rather than just imitating examples\n",
    "- The reward model is crucial for guiding the policy optimization\n",
    "- PPO helps balance between maximizing reward and staying close to the original model\n",
    "- RLHF can lead to more helpful, harmless, and honest responses compared to SFT alone\n",
    "\n",
    "Next steps:\n",
    "- Explore more efficient alternatives to RLHF, such as Direct Preference Optimization (DPO)\n",
    "- Learn about safety considerations and evaluation methods for aligned models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
