{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 17: Reinforcement Learning from Human Feedback (RLHF) Components\n",
    "\n",
    "In this notebook, we'll explore the key components of the RLHF pipeline and implement simplified versions to understand how they work together. We'll focus on:\n",
    "\n",
    "1. Generating response pairs for comparison\n",
    "2. Training a reward model from human preferences\n",
    "3. Implementing a simplified PPO algorithm for language models\n",
    "4. Comparing RLHF results with SFT\n",
    "\n",
    "## Overview\n",
    "\n",
    "RLHF consists of three main stages:\n",
    "1. Start with an SFT model\n",
    "2. Train a reward model on human preferences\n",
    "3. Optimize the model with PPO using the reward model\n",
    "\n",
    "Let's implement simplified versions of each component to understand the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulating an SFT Model\n",
    "\n",
    "In a real RLHF pipeline, we would start with a model that has already been fine-tuned with SFT. For this demonstration, we'll use a small pre-trained model and simulate SFT behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small pre-trained model\n",
    "model_name = \"gpt2\"  # Using a small model for demonstration\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    sft_model.config.pad_token_id = sft_model.config.eos_token_id\n",
    "\n",
    "# Move model to device\n",
    "sft_model = sft_model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Number of parameters: {sft_model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating Response Pairs for Comparison\n",
    "\n",
    "In RLHF, we need pairs of responses to the same prompt, where humans can express a preference. Let's generate some response pairs using our simulated SFT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(model, prompt, num_responses=2, max_length=100, temperature=1.0):\n",
    "    \"\"\"Generate multiple responses to the same prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    responses = []\n",
    "    \n",
    "    for _ in range(num_responses):\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_length=max_length,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        # Extract only the generated part (remove the prompt)\n",
    "        response = response[len(prompt):].strip()\n",
    "        responses.append(response)\n",
    "    \n",
    "    return responses\n",
    "\n",
    "# Example prompts\n",
    "prompts = [\n",
    "    \"Explain the concept of artificial intelligence to a 5-year-old.\",\n",
    "    \"Write a short poem about nature.\",\n",
    "    \"What are three ways to reduce stress?\",\n",
    "    \"Describe the process of photosynthesis briefly.\",\n",
    "    \"Give me a recipe for chocolate chip cookies.\"\n",
    "]\n",
    "\n",
    "# Generate response pairs\n",
    "response_pairs = []\n",
    "for prompt in prompts:\n",
    "    # Generate with different temperatures to get diverse responses\n",
    "    responses_1 = generate_responses(sft_model, prompt, num_responses=1, temperature=0.7)[0]\n",
    "    responses_2 = generate_responses(sft_model, prompt, num_responses=1, temperature=1.0)[0]\n",
    "    \n",
    "    response_pairs.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response_a\": responses_1,\n",
    "        \"response_b\": responses_2\n",
    "    })\n",
    "\n",
    "# Display an example pair\n",
    "example = response_pairs[0]\n",
    "print(f\"Prompt: {example['prompt']}\\n\")\n",
    "print(f\"Response A:\\n{example['response_a']}\\n\")\n",
    "print(f\"Response B:\\n{example['response_b']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulating Human Preferences\n",
    "\n",
    "In a real RLHF pipeline, human annotators would provide preferences between response pairs. For this demonstration, we'll simulate human preferences using some heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_human_preference(prompt, response_a, response_b):\n",
    "    \"\"\"Simulate human preference between two responses.\n",
    "    \n",
    "    Returns:\n",
    "    - 0 if response_a is preferred\n",
    "    - 1 if response_b is preferred\n",
    "    \"\"\"\n",
    "    # Simple heuristics for preference simulation\n",
    "    score_a = 0\n",
    "    score_b = 0\n",
    "    \n",
    "    # Length preference (not too short, not too long)\n",
    "    len_a = len(response_a.split())\n",
    "    len_b = len(response_b.split())\n",
    "    \n",
    "    # Prefer responses between 20 and 100 words\n",
    "    if 20 <= len_a <= 100:\n",
    "        score_a += 1\n",
    "    if 20 <= len_b <= 100:\n",
    "        score_b += 1\n",
    "    \n",
    "    # Relevance to prompt (simple keyword matching)\n",
    "    prompt_keywords = set(prompt.lower().split())\n",
    "    response_a_keywords = set(response_a.lower().split())\n",
    "    response_b_keywords = set(response_b.lower().split())\n",
    "    \n",
    "    overlap_a = len(prompt_keywords.intersection(response_a_keywords))\n",
    "    overlap_b = len(prompt_keywords.intersection(response_b_keywords))\n",
    "    \n",
    "    score_a += overlap_a\n",
    "    score_b += overlap_b\n",
    "    \n",
    "    # Coherence (simple proxy: fewer very short sentences)\n",
    "    short_sentences_a = sum(1 for s in response_a.split(\".\") if len(s.split()) < 3)\n",
    "    short_sentences_b = sum(1 for s in response_b.split(\".\") if len(s.split()) < 3)\n",
    "    \n",
    "    score_a -= short_sentences_a\n",
    "    score_b -= short_sentences_b\n",
    "    \n",
    "    # Return preference\n",
    "    if score_a > score_b:\n",
    "        return 0  # Prefer response_a\n",
    "    elif score_b > score_a:\n",
    "        return 1  # Prefer response_b\n",
    "    else:\n",
    "        # If tied, randomly choose\n",
    "        return random.randint(0, 1)\n",
    "\n",
    "# Add simulated preferences to our response pairs\n",
    "for pair in response_pairs:\n",
    "    preference = simulate_human_preference(\n",
    "        pair[\"prompt\"], \n",
    "        pair[\"response_a\"], \n",
    "        pair[\"response_b\"]\n",
    "    )\n",
    "    pair[\"preference\"] = preference\n",
    "\n",
    "# Display preferences\n",
    "for i, pair in enumerate(response_pairs):\n",
    "    preferred = \"A\" if pair[\"preference\"] == 0 else \"B\"\n",
    "    print(f\"Prompt {i+1}: Preferred response {preferred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training a Reward Model\n",
    "\n",
    "Now we'll train a reward model to predict human preferences between response pairs. The reward model takes a prompt and response as input and outputs a scalar reward value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create more synthetic preference data for training\n",
    "additional_prompts = [\n",
    "    \"What are the benefits of exercise?\",\n",
    "    \"Explain how the internet works.\",\n",
    "    \"Give me tips for learning a new language.\",\n",
    "    \"What causes climate change?\",\n",
    "    \"How do airplanes fly?\",\n",
    "    \"Describe the water cycle.\",\n",
    "    \"What are the main features of democracy?\",\n",
    "    \"How does the human digestive system work?\",\n",
    "    \"What are the benefits of meditation?\",\n",
    "    \"Explain the concept of supply and demand.\"\n",
    "]\n",
    "\n",
    "# Generate more response pairs\n",
    "for prompt in additional_prompts:\n",
    "    responses_1 = generate_responses(sft_model, prompt, num_responses=1, temperature=0.7)[0]\n",
    "    responses_2 = generate_responses(sft_model, prompt, num_responses=1, temperature=1.0)[0]\n",
    "    \n",
    "    preference = simulate_human_preference(prompt, responses_1, responses_2)\n",
    "    \n",
    "    response_pairs.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response_a\": responses_1,\n",
    "        \"response_b\": responses_2,\n",
    "        \"preference\": preference\n",
    "    })\n",
    "\n",
    "print(f\"Total preference pairs: {len(response_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for reward model training\n",
    "def prepare_reward_data(response_pairs):\n",
    "    \"\"\"Prepare data for reward model training.\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for pair in response_pairs:\n",
    "        prompt = pair[\"prompt\"]\n",
    "        response_a = pair[\"response_a\"]\n",
    "        response_b = pair[\"response_b\"]\n",
    "        preference = pair[\"preference\"]\n",
    "        \n",
    "        # Add chosen and rejected responses\n",
    "        if preference == 0:  # Response A preferred\n",
    "            chosen = response_a\n",
    "            rejected = response_b\n",
    "        else:  # Response B preferred\n",
    "            chosen = response_b\n",
    "            rejected = response_a\n",
    "        \n",
    "        # Add to data\n",
    "        data.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"chosen\": chosen,\n",
    "            \"rejected\": rejected\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "reward_data = prepare_reward_data(response_pairs)\n",
    "\n",
    "# Split into train and validation sets\n",
    "random.shuffle(reward_data)\n",
    "train_size = int(0.8 * len(reward_data))\n",
    "train_data = reward_data[:train_size]\n",
    "val_data = reward_data[train_size:]\n",
    "\n",
    "print(f\"Training examples: {len(train_data)}\")\n",
    "print(f\"Validation examples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple reward model\n",
    "class RewardModel(nn.Module):\n",
    "    \"\"\"Simple reward model for demonstration.\"\"\"\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            num_labels=1  # Single scalar output\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits.squeeze(-1)  # Return scalar rewards\n",
    "\n",
    "# Initialize reward model\n",
    "reward_model_name = \"distilbert-base-uncased\"  # Using a smaller model for demonstration\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(reward_model_name)\n",
    "reward_model = RewardModel(reward_model_name).to(device)\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if reward_tokenizer.pad_token is None:\n",
    "    reward_tokenizer.pad_token = reward_tokenizer.eos_token\n",
    "\n",
    "print(f\"Reward model initialized: {reward_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for reward model training\n",
    "def tokenize_for_reward_model(examples):\n",
    "    \"\"\"Tokenize data for reward model training.\"\"\"\n",
    "    chosen_inputs = [f\"Prompt: {p}\\nResponse: {c}\" for p, c in zip(examples[\"prompt\"], examples[\"chosen\"])]\n",
    "    rejected_inputs = [f\"Prompt: {p}\\nResponse: {r}\" for p, r in zip(examples[\"prompt\"], examples[\"rejected\"])]\n",
    "    \n",
    "    chosen_tokens = reward_tokenizer(\n",
    "        chosen_inputs,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    rejected_tokens = reward_tokenizer(\n",
    "        rejected_inputs,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"chosen_input_ids\": chosen_tokens[\"input_ids\"],\n",
    "        \"chosen_attention_mask\": chosen_tokens[\"attention_mask\"],\n",
    "        \"rejected_input_ids\": rejected_tokens[\"input_ids\"],\n",
    "        \"rejected_attention_mask\": rejected_tokens[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(pd.DataFrame(train_data))\n",
    "val_dataset = Dataset.from_pandas(pd.DataFrame(val_data))\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_for_reward_model,\n",
    "    batched=True,\n",
    "    remove_columns=[\"prompt\", \"chosen\", \"rejected\"]\n",
    ")\n",
    "\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    tokenize_for_reward_model,\n",
    "    batched=True,\n",
    "    remove_columns=[\"prompt\", \"chosen\", \"rejected\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
