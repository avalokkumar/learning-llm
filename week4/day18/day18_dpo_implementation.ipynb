{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 18: Direct Preference Optimization (DPO) Implementation\n",
    "\n",
    "In this notebook, we'll implement Direct Preference Optimization (DPO), a more efficient alternative to RLHF for aligning language models with human preferences. We'll focus on:\n",
    "\n",
    "1. Understanding the DPO algorithm\n",
    "2. Preparing preference data\n",
    "3. Implementing the DPO loss function\n",
    "4. Training a model with DPO\n",
    "5. Comparing DPO results with SFT and RLHF\n",
    "\n",
    "## Overview\n",
    "\n",
    "DPO simplifies the RLHF pipeline by eliminating the need for a separate reward model and the complex RL optimization step. Instead, it directly optimizes a policy to align with human preferences using a simple classification-like objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the DPO Algorithm\n",
    "\n",
    "DPO is based on the insight that the optimal policy in RLHF can be expressed in terms of the reference policy (SFT model) and the reward function:\n",
    "\n",
    "$$\\pi^*(x|y) \\propto \\pi_{\\text{ref}}(x|y) \\exp(\\beta r(x, y))$$\n",
    "\n",
    "DPO rearranges this to express the reward function in terms of the optimal policy and reference policy:\n",
    "\n",
    "$$r(x, y) = \\frac{1}{\\beta} \\log \\frac{\\pi^*(x|y)}{\\pi_{\\text{ref}}(x|y)} + Z(y)$$\n",
    "\n",
    "Using this relationship, DPO derives a loss function that directly optimizes the policy to match human preferences:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{DPO}}(\\pi_\\theta; \\pi_{\\text{ref}}) = -\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)} \\right) \\right]$$\n",
    "\n",
    "Where:\n",
    "- $(x, y_w, y_l)$ is a preference pair with prompt $x$, preferred response $y_w$, and less preferred response $y_l$\n",
    "- $\\pi_\\theta$ is the policy being trained\n",
    "- $\\pi_{\\text{ref}}$ is the reference policy (SFT model)\n",
    "- $\\beta$ is a hyperparameter controlling the strength of the preference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Models\n",
    "\n",
    "We'll start by loading a pre-trained language model to serve as our reference model (which would typically be an SFT model in a real pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small pre-trained model\n",
    "model_name = \"gpt2\"  # Using a small model for demonstration\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "reference_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    reference_model.config.pad_token_id = reference_model.config.eos_token_id\n",
    "\n",
    "# Create a copy of the reference model to be optimized with DPO\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "policy_model.config.pad_token_id = reference_model.config.pad_token_id\n",
    "\n",
    "# Move models to device\n",
    "reference_model = reference_model.to(device)\n",
    "policy_model = policy_model.to(device)\n",
    "\n",
    "print(f\"Models loaded: {model_name}\")\n",
    "print(f\"Number of parameters: {reference_model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing Preference Data\n",
    "\n",
    "DPO requires preference data in the form of (prompt, chosen_response, rejected_response) triples. Let's create a synthetic dataset for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic preference dataset\n",
    "preference_data = [\n",
    "    {\n",
    "        \"prompt\": \"Explain the concept of machine learning.\",\n",
    "        \"chosen\": \"Machine learning is a branch of artificial intelligence that enables computers to learn from data and improve their performance on a task without being explicitly programmed. It works by identifying patterns in data and using those patterns to make predictions or decisions.\",\n",
    "        \"rejected\": \"Machine learning is when computers do stuff with data.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Write a poem about the ocean.\",\n",
    "        \"chosen\": \"Vast blue expanse beneath the sky,\\nWaves dance and crash as time goes by.\\nSecrets deep in waters cold,\\nOcean stories, forever told.\\nSunlight sparkles on the foam,\\nEndless waters, the sailor's home.\",\n",
    "        \"rejected\": \"Ocean big. Ocean blue. Fish swim there. The end.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Summarize the theory of relativity.\",\n",
    "        \"chosen\": \"Einstein's theory of relativity consists of two parts: special relativity and general relativity. Special relativity states that the laws of physics are the same for all non-accelerating observers and that the speed of light is constant regardless of the observer's motion. General relativity extends this to include gravity, describing it as a curvature of spacetime caused by mass and energy.\",\n",
    "        \"rejected\": \"Einstein said E=mcÂ² and stuff moves weird when it's fast or heavy.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Provide three tips for effective time management.\",\n",
    "        \"chosen\": \"1. Prioritize tasks using methods like the Eisenhower Matrix, which categorizes tasks by urgency and importance.\\n2. Break large projects into smaller, manageable tasks with specific deadlines.\\n3. Use the Pomodoro Technique: work in focused 25-minute intervals followed by short breaks to maintain productivity and prevent burnout.\",\n",
    "        \"rejected\": \"1. Don't waste time.\\n2. Do important stuff first.\\n3. Use a calendar I guess.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain how photosynthesis works.\",\n",
    "        \"chosen\": \"Photosynthesis is the process by which plants, algae, and some bacteria convert sunlight, water, and carbon dioxide into glucose (sugar) and oxygen. The process occurs in chloroplasts, specifically in the chlorophyll-containing thylakoids. It consists of light-dependent reactions, which capture energy from sunlight to generate ATP and NADPH, and the Calvin cycle, which uses this energy to fix carbon dioxide into glucose.\",\n",
    "        \"rejected\": \"Plants use sunlight to make food. They take in CO2 and release oxygen. It happens in the green parts.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add more examples\n",
    "additional_examples = [\n",
    "    {\n",
    "        \"prompt\": \"Describe the water cycle.\",\n",
    "        \"chosen\": \"The water cycle, or hydrologic cycle, is the continuous movement of water on, above, and below Earth's surface. It begins with evaporation, where water from oceans, lakes, and rivers turns into water vapor due to solar energy. This vapor rises, cools, and condenses into clouds (condensation). When the clouds become saturated, precipitation occurs as rain, snow, or hail. The water then either infiltrates the ground, becoming groundwater, or flows as surface runoff back to bodies of water, completing the cycle.\",\n",
    "        \"rejected\": \"Water evaporates, forms clouds, then rains down. Then it happens again.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain the concept of supply and demand.\",\n",
    "        \"chosen\": \"Supply and demand is a fundamental economic principle that describes how the price of a good or service is determined in a free market. Supply represents how much of a product producers are willing to offer at different prices, while demand represents how much consumers are willing to purchase at those prices. When supply exceeds demand, prices tend to fall; when demand exceeds supply, prices tend to rise. The point where supply and demand curves intersect is called the equilibrium, representing the optimal price and quantity for the market.\",\n",
    "        \"rejected\": \"Supply is what sellers have. Demand is what buyers want. When there's more demand, prices go up. When there's more supply, prices go down.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"How do airplanes fly?\",\n",
    "        \"chosen\": \"Airplanes fly due to the principles of aerodynamics. The wings of an aircraft are shaped with a curved top and flatter bottom (airfoil), creating a pressure difference when air flows around them. As air moves faster over the curved top surface, it creates lower pressure compared to the higher pressure under the wing, generating lift according to Bernoulli's principle. Additionally, the angle of the wings (angle of attack) deflects air downward, creating an upward force according to Newton's third law. These forces, combined with thrust from engines to overcome drag, allow the airplane to overcome gravity and achieve flight.\",\n",
    "        \"rejected\": \"Airplanes fly because their wings push air down and the engines push them forward. The shape of the wings helps them stay up.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What causes climate change?\",\n",
    "        \"chosen\": \"Climate change is primarily caused by the enhanced greenhouse effect due to human activities. When we burn fossil fuels like coal, oil, and natural gas, we release greenhouse gases, particularly carbon dioxide (CO2), into the atmosphere. These gases trap heat from the sun that would otherwise escape into space, causing global temperatures to rise. Deforestation reduces the Earth's capacity to absorb CO2, exacerbating the problem. Other contributing factors include industrial processes, agriculture (especially livestock production and rice farming, which release methane), and certain land-use changes. The scientific consensus is that these anthropogenic factors are the dominant cause of observed warming since the mid-20th century.\",\n",
    "        \"rejected\": \"Climate change happens because of pollution and greenhouse gases. People burn too much fossil fuels and cut down too many trees. This makes the Earth get warmer.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain the concept of artificial intelligence.\",\n",
    "        \"chosen\": \"Artificial Intelligence (AI) refers to the simulation of human intelligence in machines programmed to think and learn like humans. It encompasses various techniques including machine learning, where algorithms improve through experience; deep learning, which uses neural networks with many layers; natural language processing, enabling computers to understand and generate human language; computer vision for image and video analysis; and reinforcement learning, where agents learn optimal behaviors through trial and error. AI systems can perform tasks that typically require human intelligence such as visual perception, speech recognition, decision-making, and language translation. The field ranges from narrow AI designed for specific tasks to the theoretical goal of artificial general intelligence that could potentially perform any intellectual task a human can do.\",\n",
    "        \"rejected\": \"AI is when computers act smart and do things humans can do. They use algorithms and data to make decisions and solve problems. Some people think AI might take over the world someday.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "preference_data.extend(additional_examples)\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "df = pd.DataFrame(preference_data)\n",
    "print(f\"Dataset size: {len(df)} examples\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing the DPO Loss Function\n",
    "\n",
    "Now we'll implement the DPO loss function, which is the core of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPOTrainer:\n",
    "    \"\"\"Trainer for Direct Preference Optimization.\"\"\"\n",
    "    \n",
    "    def __init__(self, policy_model, reference_model, tokenizer, beta=0.1):\n",
    "        self.policy_model = policy_model\n",
    "        self.reference_model = reference_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.beta = beta  # Controls the strength of the preference\n",
    "    \n",
    "    def get_log_probs(self, model, prompt, response):\n",
    "        \"\"\"Compute log probabilities of a response given a prompt.\"\"\"\n",
    "        # Tokenize prompt and response\n",
    "        prompt_tokens = self.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        prompt_len = prompt_tokens.input_ids.size(1)\n",
    "        \n",
    "        # Tokenize the full sequence (prompt + response)\n",
    "        full_tokens = self.tokenizer(prompt + response, return_tensors=\"pt\").to(device)\n",
    "        full_ids = full_tokens.input_ids\n",
    "        \n",
    "        # Get the response part only\n",
    "        response_ids = full_ids[:, prompt_len:]\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(full_ids)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        # Get logits for the response tokens\n",
    "        response_logits = logits[:, prompt_len-1:-1, :]  # Shift by 1 for next-token prediction\n",
    "        \n",
    "        # Compute log probabilities\n",
    "        log_probs = F.log_softmax(response_logits, dim=-1)\n",
    "        \n",
    "        # Gather the log probs for the actual response tokens\n",
    "        token_log_probs = torch.gather(log_probs, 2, response_ids.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # Sum the log probs\n",
    "        return token_log_probs.sum().item()\n",
    "    \n",
    "    def compute_dpo_loss(self, prompt, chosen, rejected):\n",
    "        \"\"\"Compute the DPO loss for a single preference pair.\"\"\"\n",
    "        # Get log probs from policy model\n",
    "        policy_chosen_log_prob = self.get_log_probs(self.policy_model, prompt, chosen)\n",
    "        policy_rejected_log_prob = self.get_log_probs(self.policy_model, prompt, rejected)\n",
    "        \n",
    "        # Get log probs from reference model\n",
    "        ref_chosen_log_prob = self.get_log_probs(self.reference_model, prompt, chosen)\n",
    "        ref_rejected_log_prob = self.get_log_probs(self.reference_model, prompt, rejected)\n",
    "        \n",
    "        # Compute log ratios\n",
    "        chosen_log_ratio = policy_chosen_log_prob - ref_chosen_log_prob\n",
    "        rejected_log_ratio = policy_rejected_log_prob - ref_rejected_log_prob\n",
    "        \n",
    "        # Compute DPO loss\n",
    "        loss = -torch.log(torch.sigmoid(self.beta * (chosen_log_ratio - rejected_log_ratio)))\n",
    "        \n",
    "        return loss, chosen_log_ratio, rejected_log_ratio\n",
    "    \n",
    "    def train_step(self, prompt, chosen, rejected, optimizer):\n",
    "        \"\"\"Perform one training step.\"\"\"\n",
    "        # Enable gradients for policy model\n",
    "        for param in self.policy_model.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, chosen_log_ratio, rejected_log_ratio = self.compute_dpo_loss(prompt, chosen, rejected)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss.item(), chosen_log_ratio, rejected_log_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training with DPO\n",
    "\n",
    "Now let's train our policy model using DPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DPO trainer\n",
    "dpo_trainer = DPOTrainer(\n",
    "    policy_model=policy_model,\n",
    "    reference_model=reference_model,\n",
    "    tokenizer=tokenizer,\n",
    "    beta=0.1\n",
    ")\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = torch.optim.AdamW(policy_model.parameters(), lr=1e-5)\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_df = df.sample(frac=0.8, random_state=42)\n",
    "val_df = df.drop(train_df.index)\n",
    "\n",
    "print(f\"Training examples: {len(train_df)}\")\n",
    "print(f\"Validation examples: {len(val_df)}\")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Training\n",
    "    policy_model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Training\"):\n",
    "        prompt = row[\"prompt\"]\n",
    "        chosen = row[\"chosen\"]\n",
    "        rejected = row[\"rejected\"]\n",
    "        \n",
    "        loss, chosen_ratio, rejected_ratio = dpo_trainer.train_step(prompt, chosen, rejected, optimizer)\n",
    "        epoch_loss += loss\n",
    "    \n",
    "    avg_train_loss = epoch_loss / len(train_df)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    policy_model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    for i, row in tqdm(val_df.iterrows(), total=len(val_df), desc=\"Validation\"):\n",
    "        prompt = row[\"prompt\"]\n",
    "        chosen = row[\"chosen\"]\n",
    "        rejected = row[\"rejected\"]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss, _, _ = dpo_trainer.compute_dpo_loss(prompt, chosen, rejected)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_df)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Plot training results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('DPO Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating the DPO Model\n",
    "\n",
    "Let's compare the outputs of the reference model and the DPO-optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, prompt, max_length=100):\n",
    "    \"\"\"Generate a response from the model given a prompt.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=inputs[\"input_ids\"].size(1) + max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    response = response[len(prompt):].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Explain the concept of neural networks.\",\n",
    "    \"Write a short poem about technology.\",\n",
    "    \"What are three benefits of regular exercise?\"\n",
    "]\n",
    "\n",
    "# Compare reference and DPO models\n",
    "for prompt in test_prompts:\n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    \n",
    "    # Generate from reference model\n",
    "    ref_response = generate_response(reference_model, prompt)\n",
    "    print(f\"Reference Model Response:\\n{ref_response}\\n\")\n",
    "    \n",
    "    # Generate from DPO model\n",
    "    dpo_response = generate_response(policy_model, prompt)\n",
    "    print(f\"DPO Model Response:\\n{dpo_response}\\n\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing DPO and RLHF\n",
    "\n",
    "Let's analyze the differences between DPO and RLHF in terms of implementation complexity and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Complexity\n",
    "\n",
    "**RLHF Pipeline:**\n",
    "1. Train an SFT model\n",
    "2. Generate response pairs\n",
    "3. Collect human preferences\n",
    "4. Train a reward model\n",
    "5. Optimize policy with PPO\n",
    "\n",
    "**DPO Pipeline:**\n",
    "1. Train an SFT model\n",
    "2. Collect human preferences\n",
    "3. Optimize policy with DPO\n",
    "\n",
    "DPO eliminates the need for a separate reward model and the complex PPO optimization, making it significantly simpler to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Analysis\n",
    "\n",
    "In practice, DPO has been shown to achieve comparable or better results than RLHF in many cases, while being more computationally efficient. The key advantages of DPO include:\n",
    "\n",
    "1. **Simplicity**: Fewer components and hyperparameters to tune\n",
    "2. **Stability**: More stable training without the complexities of RL\n",
    "3. **Efficiency**: Lower computational requirements\n",
    "4. **Performance**: Comparable or better alignment with human preferences\n",
    "\n",
    "However, RLHF may still be preferred in some cases:\n",
    "\n",
    "1. When fine-grained control over the KL divergence is needed\n",
    "2. When incorporating additional reward components beyond human preferences\n",
    "3. When working with more complex preference structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this notebook, we've implemented Direct Preference Optimization (DPO), a more efficient alternative to RLHF for aligning language models with human preferences. We've seen how DPO simplifies the alignment pipeline by eliminating the need for a separate reward model and the complex RL optimization step.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. DPO directly optimizes a policy to align with human preferences using a simple classification-like objective.\n",
    "2. The DPO loss is derived from the same preference data used in RLHF, but avoids explicitly modeling the reward function.\n",
    "3. DPO is more computationally efficient and easier to implement than RLHF, while achieving comparable or better results.\n",
    "4. The choice between DPO and RLHF depends on the specific requirements of the alignment task.\n",
    "\n",
    "As the field of language model alignment continues to evolve, we're likely to see further refinements and variations of DPO, as well as hybrid approaches that combine the strengths of different alignment techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
