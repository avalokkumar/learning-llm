{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 29: Validation Strategies and Early Stopping\n",
    "\n",
    "In this notebook, we'll focus on implementing effective validation strategies and early stopping techniques for fine-tuning large language models.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Setup and dependencies\n",
    "2. Implementing K-fold cross-validation\n",
    "3. Early stopping implementation\n",
    "4. Analyzing learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft evaluate accelerate scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType\n",
    ")\n",
    "import evaluate\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base model\n",
    "base_model_name = \"roberta-base\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# Load a small dataset for demonstration\n",
    "# We'll use a subset of SST-2 for faster execution\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "train_dataset = dataset[\"train\"].select(range(1000))  # Use 1000 examples for training\n",
    "eval_dataset = dataset[\"validation\"].select(range(200))  # Use 200 examples for validation\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Create a data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing K-Fold Cross-Validation\n",
    "\n",
    "K-fold cross-validation helps ensure our model's performance is robust and not dependent on a specific train-validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics for evaluation\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a model with LoRA adapters\n",
    "def create_lora_model():\n",
    "    # Load the base model\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model_name,\n",
    "        num_labels=2,  # Binary classification for sentiment\n",
    "        return_dict=True\n",
    "    )\n",
    "    \n",
    "    # Define LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"query\", \"key\", \"value\"],\n",
    "        bias=\"none\"\n",
    "    )\n",
    "    \n",
    "    # Create the PEFT model\n",
    "    peft_model = get_peft_model(base_model, lora_config)\n",
    "    \n",
    "    return peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement K-fold cross-validation\n",
    "def k_fold_cross_validation(dataset, k=5, num_epochs=2):\n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Store results for each fold\n",
    "    fold_results = []\n",
    "    \n",
    "    # Get indices of the dataset\n",
    "    indices = list(range(len(dataset)))\n",
    "    \n",
    "    # Perform K-fold cross-validation\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(indices)):\n",
    "        print(f\"\\nTraining fold {fold+1}/{k}\")\n",
    "        \n",
    "        # Create train and validation subsets\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "        \n",
    "        # Create a new model for each fold\n",
    "        model = create_lora_model().to(device)\n",
    "        \n",
    "        # Define training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./results/fold_{fold+1}\",\n",
    "            learning_rate=3e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=32,\n",
    "            num_train_epochs=num_epochs,\n",
    "            weight_decay=0.01,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            push_to_hub=False,\n",
    "            report_to=\"none\",\n",
    "            logging_steps=10\n",
    "        )\n",
    "        \n",
    "        # Create the trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_subset,\n",
    "            eval_dataset=val_subset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        \n",
    "        # Evaluate the model\n",
    "        eval_results = trainer.evaluate()\n",
    "        \n",
    "        # Store results\n",
    "        fold_results.append({\n",
    "            \"fold\": fold + 1,\n",
    "            \"accuracy\": eval_results[\"eval_accuracy\"],\n",
    "            \"loss\": eval_results[\"eval_loss\"]\n",
    "        })\n",
    "        \n",
    "        print(f\"Fold {fold+1} accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "    \n",
    "    return fold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run K-fold cross-validation with a small K for demonstration\n",
    "# In practice, you would use K=5 or K=10\n",
    "k_fold_results = k_fold_cross_validation(tokenized_train, k=3, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze K-fold results\n",
    "k_fold_df = pd.DataFrame(k_fold_results)\n",
    "print(\"K-fold Cross-Validation Results:\")\n",
    "print(k_fold_df)\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean_accuracy = k_fold_df[\"accuracy\"].mean()\n",
    "std_accuracy = k_fold_df[\"accuracy\"].std()\n",
    "\n",
    "print(f\"\\nMean accuracy: {mean_accuracy:.4f} Â± {std_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Early Stopping Implementation\n",
    "\n",
    "Early stopping prevents overfitting by monitoring validation performance and stopping training when performance plateaus or degrades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train with early stopping and track metrics\n",
    "def train_with_early_stopping(patience=3, num_epochs=10):\n",
    "    # Create a new model\n",
    "    model = create_lora_model().to(device)\n",
    "    \n",
    "    # Define training arguments with early stopping\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results/early_stopping\",\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=num_epochs,  # Maximum number of epochs\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\",\n",
    "        logging_steps=10,\n",
    "        metric_for_best_model=\"accuracy\",  # Monitor accuracy for early stopping\n",
    "    )\n",
    "    \n",
    "    # Create the trainer with early stopping callback\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_eval,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=patience)]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Get training history\n",
    "    training_history = trainer.state.log_history\n",
    "    \n",
    "    # Extract metrics from history\n",
    "    train_metrics = []\n",
    "    eval_metrics = []\n",
    "    \n",
    "    for entry in training_history:\n",
    "        if \"loss\" in entry and \"epoch\" in entry and \"eval_accuracy\" not in entry:\n",
    "            train_metrics.append({\n",
    "                \"epoch\": entry[\"epoch\"],\n",
    "                \"loss\": entry[\"loss\"]\n",
    "            })\n",
    "        elif \"eval_accuracy\" in entry and \"epoch\" in entry:\n",
    "            eval_metrics.append({\n",
    "                \"epoch\": entry[\"epoch\"],\n",
    "                \"accuracy\": entry[\"eval_accuracy\"],\n",
    "                \"loss\": entry[\"eval_loss\"]\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        \"train_metrics\": train_metrics,\n",
    "        \"eval_metrics\": eval_metrics,\n",
    "        \"model\": model,\n",
    "        \"trainer\": trainer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with early stopping\n",
    "print(\"Training with early stopping...\")\n",
    "early_stopping_results = train_with_early_stopping(patience=2, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing Learning Curves\n",
    "\n",
    "Learning curves help us understand the training dynamics and identify potential issues like overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert metrics to DataFrames\n",
    "train_df = pd.DataFrame(early_stopping_results[\"train_metrics\"])\n",
    "eval_df = pd.DataFrame(early_stopping_results[\"eval_metrics\"])\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot loss curves\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_df[\"epoch\"], train_df[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(eval_df[\"epoch\"], eval_df[\"loss\"], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot accuracy curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(eval_df[\"epoch\"], eval_df[\"accuracy\"], label=\"Validation Accuracy\", marker=\"o\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze when early stopping occurred\n",
    "best_epoch = eval_df.loc[eval_df[\"accuracy\"].idxmax()][\"epoch\"]\n",
    "best_accuracy = eval_df[\"accuracy\"].max()\n",
    "\n",
    "print(f\"Best epoch: {best_epoch}\")\n",
    "print(f\"Best validation accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"Total epochs trained: {len(eval_df)}\")\n",
    "\n",
    "if len(eval_df) < 10:  # If we trained fewer than the maximum epochs\n",
    "    print(f\"Early stopping occurred after epoch {len(eval_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating the Final Model\n",
    "\n",
    "Let's evaluate our final model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "trainer = early_stopping_results[\"trainer\"]\n",
    "test_results = trainer.evaluate(eval_dataset)\n",
    "\n",
    "print(\"Test Results:\")\n",
    "for key, value in test_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've implemented and explored validation strategies and early stopping techniques for fine-tuning large language models. We've seen how to:\n",
    "\n",
    "1. Implement K-fold cross-validation to get a robust estimate of model performance\n",
    "2. Use early stopping to prevent overfitting and reduce training time\n",
    "3. Analyze learning curves to understand training dynamics\n",
    "\n",
    "These techniques are essential for developing high-performing models and ensuring they generalize well to unseen data. By carefully validating our models and stopping training at the right time, we can create more efficient and effective language models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
