{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 29: Mitigating Catastrophic Forgetting\n",
    "\n",
    "In this notebook, we'll explore techniques to mitigate catastrophic forgetting when fine-tuning large language models. Catastrophic forgetting occurs when a model loses previously learned knowledge during fine-tuning on a new task.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Setup and dependencies\n",
    "2. Demonstrating catastrophic forgetting\n",
    "3. Implementing regularization-based methods\n",
    "4. Comparing model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft evaluate accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType\n",
    ")\n",
    "import evaluate\n",
    "import copy\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and Preparing Datasets\n",
    "\n",
    "We'll use two different tasks to demonstrate catastrophic forgetting: sentiment analysis (SST-2) and natural language inference (MNLI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base model\n",
    "base_model_name = \"roberta-base\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# Load datasets for two different tasks\n",
    "# Task 1: Sentiment Analysis (SST-2)\n",
    "sst2_dataset = load_dataset(\"glue\", \"sst2\")\n",
    "# Use smaller subsets for demonstration\n",
    "sst2_train = sst2_dataset[\"train\"].select(range(1000))\n",
    "sst2_eval = sst2_dataset[\"validation\"].select(range(200))\n",
    "\n",
    "# Task 2: Natural Language Inference (MNLI)\n",
    "mnli_dataset = load_dataset(\"glue\", \"mnli\")\n",
    "mnli_train = mnli_dataset[\"train\"].select(range(1000))\n",
    "mnli_eval = mnli_dataset[\"validation_matched\"].select(range(200))\n",
    "\n",
    "print(\"SST-2 dataset (Sentiment Analysis):\")\n",
    "print(f\"  Train: {len(sst2_train)} examples\")\n",
    "print(f\"  Validation: {len(sst2_eval)} examples\")\n",
    "print(f\"  Labels: {sst2_dataset['train'].features['label'].names}\")\n",
    "\n",
    "print(\"\\nMNLI dataset (Natural Language Inference):\")\n",
    "print(f\"  Train: {len(mnli_train)} examples\")\n",
    "print(f\"  Validation: {len(mnli_eval)} examples\")\n",
    "print(f\"  Labels: {mnli_dataset['train'].features['label'].names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize SST-2 dataset\n",
    "def tokenize_sst2(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_sst2_train = sst2_train.map(tokenize_sst2, batched=True)\n",
    "tokenized_sst2_eval = sst2_eval.map(tokenize_sst2, batched=True)\n",
    "\n",
    "# Tokenize MNLI dataset\n",
    "def tokenize_mnli(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"premise\"],\n",
    "        examples[\"hypothesis\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_mnli_train = mnli_train.map(tokenize_mnli, batched=True)\n",
    "tokenized_mnli_eval = mnli_eval.map(tokenize_mnli, batched=True)\n",
    "\n",
    "# Create data collators\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Demonstrating Catastrophic Forgetting\n",
    "\n",
    "We'll first train a model on Task 1 (SST-2), then fine-tune it on Task 2 (MNLI), and observe how performance on Task 1 degrades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics for evaluation\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train on a task\n",
    "def train_on_task(model, train_dataset, eval_dataset, num_labels, output_dir, num_epochs=3):\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    # Create the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    return model, eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on Task 1 (SST-2)\n",
    "print(\"Training on Task 1 (SST-2)...\")\n",
    "model_task1 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    num_labels=2,  # Binary classification for SST-2\n",
    "    return_dict=True\n",
    ").to(device)\n",
    "\n",
    "model_task1, task1_results = train_on_task(\n",
    "    model_task1,\n",
    "    tokenized_sst2_train,\n",
    "    tokenized_sst2_eval,\n",
    "    num_labels=2,\n",
    "    output_dir=\"./results/task1\",\n",
    "    num_epochs=3\n",
    ")\n",
    "\n",
    "print(f\"Task 1 (SST-2) accuracy: {task1_results['eval_accuracy']:.4f}\")\n",
    "\n",
    "# Save a copy of the Task 1 model for later comparison\n",
    "task1_model_copy = copy.deepcopy(model_task1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune on Task 2 (MNLI) using the Task 1 model\n",
    "print(\"\\nFine-tuning on Task 2 (MNLI)...\")\n",
    "\n",
    "# We need to resize the classification head for the new task\n",
    "model_task2 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    num_labels=3,  # MNLI has 3 classes\n",
    "    return_dict=True\n",
    ").to(device)\n",
    "\n",
    "# Copy weights from task1 model to task2 model (except for the classification head)\n",
    "for name, param in model_task1.named_parameters():\n",
    "    if \"classifier\" not in name:  # Skip the classification head\n",
    "        model_task2.state_dict()[name].copy_(param)\n",
    "\n",
    "model_task2, task2_results = train_on_task(\n",
    "    model_task2,\n",
    "    tokenized_mnli_train,\n",
    "    tokenized_mnli_eval,\n",
    "    num_labels=3,\n",
    "    output_dir=\"./results/task2\",\n",
    "    num_epochs=3\n",
    ")\n",
    "\n",
    "print(f\"Task 2 (MNLI) accuracy: {task2_results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Task 1 performance after fine-tuning on Task 2\n",
    "# We need to create a new model with the Task 1 classification head\n",
    "model_task1_after = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    num_labels=2,  # Binary classification for SST-2\n",
    "    return_dict=True\n",
    ").to(device)\n",
    "\n",
    "# Copy weights from task2 model to task1_after model (except for the classification head)\n",
    "for name, param in model_task2.named_parameters():\n",
    "    if \"classifier\" not in name:  # Skip the classification head\n",
    "        model_task1_after.state_dict()[name].copy_(param)\n",
    "\n",
    "# Copy the original task1 classification head\n",
    "for name, param in task1_model_copy.named_parameters():\n",
    "    if \"classifier\" in name:\n",
    "        model_task1_after.state_dict()[name].copy_(param)\n",
    "\n",
    "# Evaluate on Task 1\n",
    "trainer = Trainer(\n",
    "    model=model_task1_after,\n",
    "    eval_dataset=tokenized_sst2_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "task1_after_results = trainer.evaluate()\n",
    "print(f\"\\nTask 1 (SST-2) accuracy after fine-tuning on Task 2: {task1_after_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Performance drop: {task1_results['eval_accuracy'] - task1_after_results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mitigating Catastrophic Forgetting with LoRA\n",
    "\n",
    "Now, let's use LoRA to mitigate catastrophic forgetting by only updating a small subset of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on Task 1 (SST-2) with LoRA\n",
    "print(\"Training on Task 1 (SST-2) with LoRA...\")\n",
    "\n",
    "# Load the base model\n",
    "base_model_task1 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    num_labels=2,  # Binary classification for SST-2\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config_task1 = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"key\", \"value\"],\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Create the PEFT model\n",
    "lora_model_task1 = get_peft_model(base_model_task1, lora_config_task1).to(device)\n",
    "\n",
    "# Train on Task 1\n",
    "lora_model_task1, lora_task1_results = train_on_task(\n",
    "    lora_model_task1,\n",
    "    tokenized_sst2_train,\n",
    "    tokenized_sst2_eval,\n",
    "    num_labels=2,\n",
    "    output_dir=\"./results/lora_task1\",\n",
    "    num_epochs=3\n",
    ")\n",
    "\n",
    "print(f\"Task 1 (SST-2) accuracy with LoRA: {lora_task1_results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune on Task 2 (MNLI) with a new LoRA adapter\n",
    "print(\"\\nFine-tuning on Task 2 (MNLI) with a new LoRA adapter...\")\n",
    "\n",
    "# Load the base model for Task 2\n",
    "base_model_task2 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    num_labels=3,  # MNLI has 3 classes\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "# Define LoRA configuration for Task 2\n",
    "lora_config_task2 = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"key\", \"value\"],\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Create the PEFT model\n",
    "lora_model_task2 = get_peft_model(base_model_task2, lora_config_task2).to(device)\n",
    "\n",
    "# Train on Task 2\n",
    "lora_model_task2, lora_task2_results = train_on_task(\n",
    "    lora_model_task2,\n",
    "    tokenized_mnli_train,\n",
    "    tokenized_mnli_eval,\n",
    "    num_labels=3,\n",
    "    output_dir=\"./results/lora_task2\",\n",
    "    num_epochs=3\n",
    ")\n",
    "\n",
    "print(f\"Task 2 (MNLI) accuracy with LoRA: {lora_task2_results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-evaluate Task 1 with its LoRA adapter\n",
    "trainer = Trainer(\n",
    "    model=lora_model_task1,\n",
    "    eval_dataset=tokenized_sst2_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "lora_task1_after_results = trainer.evaluate()\n",
    "print(f\"\\nTask 1 (SST-2) accuracy with LoRA after training Task 2: {lora_task1_after_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Performance change: {lora_task1_after_results['eval_accuracy'] - lora_task1_results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing Results\n",
    "\n",
    "Let's compare the results of the standard fine-tuning approach and the LoRA approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results\n",
    "results = {\n",
    "    \"Standard Fine-tuning\": {\n",
    "        \"Task 1 (Initial)\": task1_results[\"eval_accuracy\"],\n",
    "        \"Task 2\": task2_results[\"eval_accuracy\"],\n",
    "        \"Task 1 (After Task 2)\": task1_after_results[\"eval_accuracy\"],\n",
    "        \"Task 1 Performance Drop\": task1_results[\"eval_accuracy\"] - task1_after_results[\"eval_accuracy\"]\n",
    "    },\n",
    "    \"LoRA\": {\n",
    "        \"Task 1 (Initial)\": lora_task1_results[\"eval_accuracy\"],\n",
    "        \"Task 2\": lora_task2_results[\"eval_accuracy\"],\n",
    "        \"Task 1 (After Task 2)\": lora_task1_after_results[\"eval_accuracy\"],\n",
    "        \"Task 1 Performance Drop\": lora_task1_results[\"eval_accuracy\"] - lora_task1_after_results[\"eval_accuracy\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Comparison of Standard Fine-tuning vs. LoRA:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Task 1 performance before and after Task 2\n",
    "x = np.arange(2)\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, \n",
    "        [results[\"Standard Fine-tuning\"][\"Task 1 (Initial)\"], results[\"Standard Fine-tuning\"][\"Task 1 (After Task 2)\"]], \n",
    "        width, label=\"Standard Fine-tuning\")\n",
    "plt.bar(x + width/2, \n",
    "        [results[\"LoRA\"][\"Task 1 (Initial)\"], results[\"LoRA\"][\"Task 1 (After Task 2)\"]], \n",
    "        width, label=\"LoRA\")\n",
    "\n",
    "plt.xlabel(\"Training Stage\")\n",
    "plt.ylabel(\"Task 1 Accuracy\")\n",
    "plt.title(\"Impact of Task 2 Training on Task 1 Performance\")\n",
    "plt.xticks(x, [\"Before Task 2\", \"After Task 2\"])\n",
    "plt.legend()\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Other Techniques to Mitigate Catastrophic Forgetting\n",
    "\n",
    "Let's briefly discuss other techniques that can be used to mitigate catastrophic forgetting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Elastic Weight Consolidation (EWC)\n",
    "\n",
    "EWC adds a regularization term that penalizes changes to parameters that are important for the previous task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode for EWC implementation\n",
    "'''\n",
    "# Calculate Fisher Information Matrix for Task 1\n",
    "fisher_information = calculate_fisher_information(model, task1_data)\n",
    "\n",
    "# Define EWC loss\n",
    "def ewc_loss(model, old_model, fisher, lambda_ewc):\n",
    "    loss = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in fisher:\n",
    "            loss += lambda_ewc * torch.sum(fisher[name] * (param - old_model[name])**2)\n",
    "    return loss\n",
    "\n",
    "# Add EWC loss to the standard loss during Task 2 training\n",
    "total_loss = task2_loss + ewc_loss(model, task1_model, fisher_information, lambda_ewc=1000)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Replay Methods\n",
    "\n",
    "Replay methods involve mixing in samples from previous tasks during training on new tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode for replay implementation\n",
    "'''\n",
    "# Create a combined dataset with samples from both tasks\n",
    "combined_dataset = concatenate_datasets([task1_dataset.select(indices), task2_dataset])\n",
    "\n",
    "# Train on the combined dataset\n",
    "model = train_model(model, combined_dataset)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Knowledge Distillation\n",
    "\n",
    "Knowledge distillation involves using the outputs of the original model as soft targets when training on the new task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode for knowledge distillation implementation\n",
    "'''\n",
    "# Get predictions from the original model on Task 2 data\n",
    "with torch.no_grad():\n",
    "    task1_model_outputs = task1_model(task2_inputs)\n",
    "\n",
    "# Define distillation loss\n",
    "def distillation_loss(student_logits, teacher_logits, temperature=2.0):\n",
    "    soft_targets = F.softmax(teacher_logits / temperature, dim=-1)\n",
    "    soft_predictions = F.log_softmax(student_logits / temperature, dim=-1)\n",
    "    return F.kl_div(soft_predictions, soft_targets, reduction=\"batchmean\") * (temperature ** 2)\n",
    "\n",
    "# Combine task loss and distillation loss\n",
    "total_loss = task2_loss + alpha * distillation_loss(model_outputs, task1_model_outputs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored catastrophic forgetting in large language models and techniques to mitigate it:\n",
    "\n",
    "1. We demonstrated catastrophic forgetting by fine-tuning a model on two sequential tasks and observing the performance drop on the first task.\n",
    "\n",
    "2. We showed how Parameter-Efficient Fine-Tuning (PEFT) with LoRA can mitigate catastrophic forgetting by only updating a small subset of parameters for each task.\n",
    "\n",
    "3. We discussed other techniques like Elastic Weight Consolidation (EWC), replay methods, and knowledge distillation.\n",
    "\n",
    "These techniques are essential for developing models that can learn multiple tasks sequentially without forgetting previously learned knowledge. By carefully selecting the right approach for your specific use case, you can create more versatile and robust language models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
