{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 29: Hyperparameter Optimization for LLMs - Part 1\n",
    "\n",
    "In this notebook, we'll implement hyperparameter optimization techniques for fine-tuning large language models. We'll focus on systematic approaches to hyperparameter search and validation strategies.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Setup and dependencies\n",
    "2. Defining the model and dataset\n",
    "3. Implementing a hyperparameter search framework\n",
    "4. Grid search implementation\n",
    "5. Random search implementation\n",
    "6. Analyzing search results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft evaluate accelerate optuna pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType\n",
    ")\n",
    "import evaluate\n",
    "import optuna\n",
    "from optuna.visualization import plot_param_importances, plot_optimization_history\n",
    "from functools import partial\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the Model and Dataset\n",
    "\n",
    "We'll use a RoBERTa model with LoRA adapters for a text classification task. For demonstration purposes, we'll use the SST-2 sentiment analysis dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base model\n",
    "base_model_name = \"roberta-base\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "print(dataset)\n",
    "\n",
    "# Look at a few examples\n",
    "for i in range(3):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Text: {dataset['train'][i]['sentence']}\")\n",
    "    print(f\"Label: {dataset['train'][i]['label']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Prepare the datasets for training\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "# Create a data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing a Hyperparameter Search Framework\n",
    "\n",
    "We'll create a framework for hyperparameter search that can be used with different search strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics for evaluation\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a model with LoRA adapters\n",
    "def create_lora_model(lora_r, lora_alpha, lora_dropout, target_modules):\n",
    "    # Load the base model\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model_name,\n",
    "        num_labels=2,  # Binary classification for sentiment\n",
    "        return_dict=True\n",
    "    )\n",
    "    \n",
    "    # Define LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        target_modules=target_modules,\n",
    "        bias=\"none\"\n",
    "    )\n",
    "    \n",
    "    # Create the PEFT model\n",
    "    peft_model = get_peft_model(base_model, lora_config)\n",
    "    \n",
    "    return peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate a model with given hyperparameters\n",
    "def train_and_evaluate(hyperparams, output_dir=\"./results/hp_search\", num_epochs=3):\n",
    "    # Extract hyperparameters\n",
    "    learning_rate = hyperparams[\"learning_rate\"]\n",
    "    batch_size = hyperparams[\"batch_size\"]\n",
    "    weight_decay = hyperparams[\"weight_decay\"]\n",
    "    lora_r = hyperparams[\"lora_r\"]\n",
    "    lora_alpha = hyperparams[\"lora_alpha\"]\n",
    "    lora_dropout = hyperparams[\"lora_dropout\"]\n",
    "    target_modules = hyperparams[\"target_modules\"]\n",
    "    \n",
    "    # Create a unique output directory for this run\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_output_dir = f\"{output_dir}/{timestamp}\"\n",
    "    os.makedirs(run_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save hyperparameters\n",
    "    with open(f\"{run_output_dir}/hyperparams.json\", \"w\") as f:\n",
    "        json.dump(hyperparams, f, indent=2)\n",
    "    \n",
    "    # Create the model\n",
    "    model = create_lora_model(lora_r, lora_alpha, lora_dropout, target_modules)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=run_output_dir,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size * 2,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=weight_decay,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        report_to=\"none\",\n",
    "        metric_for_best_model=\"accuracy\"\n",
    "    )\n",
    "    \n",
    "    # Create the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    trainer.train()\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate the model\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    # Add training time to results\n",
    "    eval_results[\"training_time\"] = training_time\n",
    "    \n",
    "    # Save evaluation results\n",
    "    with open(f\"{run_output_dir}/eval_results.json\", \"w\") as f:\n",
    "        json.dump(eval_results, f, indent=2)\n",
    "    \n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Grid Search Implementation\n",
    "\n",
    "Let's implement a grid search to systematically explore the hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    \"learning_rate\": [1e-5, 3e-5, 5e-5],\n",
    "    \"batch_size\": [16, 32],\n",
    "    \"weight_decay\": [0.01, 0.1],\n",
    "    \"lora_r\": [4, 8, 16],\n",
    "    \"lora_alpha\": [16, 32],\n",
    "    \"lora_dropout\": [0.05, 0.1],\n",
    "    \"target_modules\": [[\"query\", \"key\", \"value\"], [\"query\", \"key\", \"value\", \"dense\"]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate all combinations of hyperparameters\n",
    "def generate_grid_combinations(param_grid):\n",
    "    keys = list(param_grid.keys())\n",
    "    values = list(param_grid.values())\n",
    "    combinations = [{}]\n",
    "    \n",
    "    for i, key in enumerate(keys):\n",
    "        combinations = [dict(comb, **{key: val}) for comb in combinations for val in values[i]]\n",
    "    \n",
    "    return combinations\n",
    "\n",
    "# Calculate the number of combinations\n",
    "num_combinations = 1\n",
    "for values in param_grid.values():\n",
    "    num_combinations *= len(values)\n",
    "\n",
    "print(f\"Grid search will evaluate {num_combinations} combinations of hyperparameters.\")\n",
    "\n",
    "# For demonstration, we'll use a smaller subset\n",
    "# In practice, you would evaluate all combinations\n",
    "grid_combinations = generate_grid_combinations(param_grid)\n",
    "subset_size = min(3, len(grid_combinations))  # Just use 3 combinations for demonstration\n",
    "grid_subset = grid_combinations[:subset_size]\n",
    "\n",
    "print(f\"Using a subset of {subset_size} combinations for demonstration.\")\n",
    "for i, params in enumerate(grid_subset):\n",
    "    print(f\"\\nCombination {i+1}:\")\n",
    "    for key, value in params.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run grid search on the subset\n",
    "grid_search_results = []\n",
    "\n",
    "for i, hyperparams in enumerate(grid_subset):\n",
    "    print(f\"\\nEvaluating combination {i+1}/{len(grid_subset)}\")\n",
    "    \n",
    "    # Train and evaluate with these hyperparameters\n",
    "    eval_results = train_and_evaluate(\n",
    "        hyperparams,\n",
    "        output_dir=\"./results/grid_search\",\n",
    "        num_epochs=2  # Reduced for demonstration\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        \"hyperparams\": hyperparams,\n",
    "        \"eval_accuracy\": eval_results[\"eval_accuracy\"],\n",
    "        \"eval_loss\": eval_results[\"eval_loss\"],\n",
    "        \"training_time\": eval_results[\"training_time\"]\n",
    "    }\n",
    "    grid_search_results.append(result)\n",
    "    \n",
    "    print(f\"Accuracy: {eval_results['eval_accuracy']:.4f}, Loss: {eval_results['eval_loss']:.4f}, Time: {eval_results['training_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze grid search results\n",
    "grid_results_df = pd.DataFrame([\n",
    "    {\n",
    "        \"learning_rate\": r[\"hyperparams\"][\"learning_rate\"],\n",
    "        \"batch_size\": r[\"hyperparams\"][\"batch_size\"],\n",
    "        \"weight_decay\": r[\"hyperparams\"][\"weight_decay\"],\n",
    "        \"lora_r\": r[\"hyperparams\"][\"lora_r\"],\n",
    "        \"lora_alpha\": r[\"hyperparams\"][\"lora_alpha\"],\n",
    "        \"lora_dropout\": r[\"hyperparams\"][\"lora_dropout\"],\n",
    "        \"target_modules\": \"-\".join(r[\"hyperparams\"][\"target_modules\"]),\n",
    "        \"accuracy\": r[\"eval_accuracy\"],\n",
    "        \"loss\": r[\"eval_loss\"],\n",
    "        \"training_time\": r[\"training_time\"]\n",
    "    }\n",
    "    for r in grid_search_results\n",
    "])\n",
    "\n",
    "# Sort by accuracy\n",
    "grid_results_df = grid_results_df.sort_values(\"accuracy\", ascending=False)\n",
    "\n",
    "# Display results\n",
    "print(\"Grid Search Results (sorted by accuracy):\")\n",
    "grid_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Random Search Implementation\n",
    "\n",
    "Now, let's implement a random search using Optuna, which is more efficient for high-dimensional spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters\n",
    "    hyperparams = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-4, log=True),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [16, 32, 64]),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.01, 0.2),\n",
    "        \"lora_r\": trial.suggest_categorical(\"lora_r\", [4, 8, 16, 32]),\n",
    "        \"lora_alpha\": trial.suggest_categorical(\"lora_alpha\", [16, 32, 64]),\n",
    "        \"lora_dropout\": trial.suggest_float(\"lora_dropout\", 0.0, 0.2),\n",
    "        \"target_modules\": trial.suggest_categorical(\n",
    "            \"target_modules\",\n",
    "            [\n",
    "                [\"query\", \"key\", \"value\"],\n",
    "                [\"query\", \"key\", \"value\", \"dense\"],\n",
    "                [\"query\", \"key\", \"value\", \"output\"]\n",
    "            ]\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate with these hyperparameters\n",
    "    eval_results = train_and_evaluate(\n",
    "        hyperparams,\n",
    "        output_dir=\"./results/random_search\",\n",
    "        num_epochs=2  # Reduced for demonstration\n",
    "    )\n",
    "    \n",
    "    # Return the metric to optimize\n",
    "    return eval_results[\"eval_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"lora_hyperparameter_optimization\")\n",
    "\n",
    "# Run a few trials for demonstration\n",
    "# In practice, you would run more trials\n",
    "study.optimize(objective, n_trials=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best hyperparameters\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value:.4f}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyzing Search Results\n",
    "\n",
    "Let's visualize the results of our hyperparameter search to gain insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parameter importances\n",
    "try:\n",
    "    fig = plot_param_importances(study)\n",
    "    fig.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not plot parameter importances: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization history\n",
    "try:\n",
    "    fig = plot_optimization_history(study)\n",
    "    fig.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not plot optimization history: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Optuna trials to DataFrame for analysis\n",
    "optuna_results = []\n",
    "for trial in study.trials:\n",
    "    if trial.state == optuna.trial.TrialState.COMPLETE:\n",
    "        result = {\n",
    "            \"trial_number\": trial.number,\n",
    "            \"accuracy\": trial.value,\n",
    "            **trial.params\n",
    "        }\n",
    "        optuna_results.append(result)\n",
    "\n",
    "optuna_df = pd.DataFrame(optuna_results)\n",
    "\n",
    "# Display results\n",
    "print(\"Random Search Results (sorted by accuracy):\")\n",
    "optuna_df.sort_values(\"accuracy\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing Grid Search and Random Search\n",
    "\n",
    "Let's compare the results of our grid search and random search approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for comparison\n",
    "grid_best = grid_results_df.iloc[0] if not grid_results_df.empty else None\n",
    "random_best = optuna_df.sort_values(\"accuracy\", ascending=False).iloc[0] if not optuna_df.empty else None\n",
    "\n",
    "if grid_best is not None and random_best is not None:\n",
    "    print(\"Best Grid Search Configuration:\")\n",
    "    print(f\"  Accuracy: {grid_best['accuracy']:.4f}\")\n",
    "    print(f\"  Learning Rate: {grid_best['learning_rate']}\")\n",
    "    print(f\"  Batch Size: {grid_best['batch_size']}\")\n",
    "    print(f\"  LoRA Rank: {grid_best['lora_r']}\")\n",
    "    print(f\"  LoRA Alpha: {grid_best['lora_alpha']}\")\n",
    "    \n",
    "    print(\"\\nBest Random Search Configuration:\")\n",
    "    print(f\"  Accuracy: {random_best['accuracy']:.4f}\")\n",
    "    print(f\"  Learning Rate: {random_best['learning_rate']}\")\n",
    "    print(f\"  Batch Size: {random_best['batch_size']}\")\n",
    "    print(f\"  LoRA Rank: {random_best['lora_r']}\")\n",
    "    print(f\"  LoRA Alpha: {random_best['lora_alpha']}\")\n",
    "else:\n",
    "    print(\"Not enough data to compare grid search and random search.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've implemented and compared grid search and random search approaches for hyperparameter optimization of LLMs with LoRA adapters. We've seen how to:\n",
    "\n",
    "1. Define a hyperparameter search space for LoRA fine-tuning\n",
    "2. Implement grid search to systematically explore the hyperparameter space\n",
    "3. Use Optuna for more efficient random search\n",
    "4. Analyze and compare the results of different search strategies\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "- Grid search provides a comprehensive exploration but becomes impractical for high-dimensional spaces\n",
    "- Random search is more efficient and can find good configurations with fewer trials\n",
    "- Hyperparameter optimization is crucial for getting the best performance from LLMs\n",
    "- Different hyperparameters have varying levels of importance for model performance\n",
    "\n",
    "In Part 2, we'll explore more advanced techniques including early stopping, validation strategies, and preventing catastrophic forgetting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
