{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 30: Adapter Merging and Model Packaging - Part 1\n",
    "\n",
    "In this notebook, we'll explore different approaches to adapter management and model packaging for deployment. We'll focus on merging LoRA adapters with base models and preparing them for distribution.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Setup and dependencies\n",
    "2. Loading base models and LoRA adapters\n",
    "3. Merging adapters with base models\n",
    "4. Comparing merged models vs. on-the-fly adapters\n",
    "5. Saving and loading merged models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers peft datasets torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating a Base Model and LoRA Adapter\n",
    "\n",
    "First, let's create a simple base model and LoRA adapter for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base model\n",
    "base_model_name = \"gpt2\"  # Using a smaller model for demonstration\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token by default\n",
    "\n",
    "# Load the base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name).to(device)\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"c_attn\"],  # Target attention modules in GPT-2\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Create the PEFT model\n",
    "peft_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Print model information\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Base model parameters: {count_parameters(base_model):,}\")\n",
    "print(f\"LoRA trainable parameters: {count_parameters(peft_model):,}\")\n",
    "print(f\"Parameter efficiency: {count_parameters(peft_model) / count_parameters(base_model) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulating a Fine-tuned LoRA Adapter\n",
    "\n",
    "For demonstration purposes, we'll simulate a fine-tuned LoRA adapter by modifying some weights directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate fine-tuning by directly modifying some LoRA weights\n",
    "# In a real scenario, you would train the model on a dataset\n",
    "\n",
    "# Find LoRA modules\n",
    "lora_modules = []\n",
    "for name, module in peft_model.named_modules():\n",
    "    if \"lora\" in name and hasattr(module, \"weight\"):\n",
    "        lora_modules.append((name, module))\n",
    "\n",
    "# Modify some weights to simulate training\n",
    "with torch.no_grad():\n",
    "    for name, module in lora_modules:\n",
    "        if hasattr(module, \"weight\"):\n",
    "            # Add small random values to simulate training updates\n",
    "            module.weight.data += torch.randn_like(module.weight.data) * 0.01\n",
    "\n",
    "print(f\"Simulated fine-tuning on {len(lora_modules)} LoRA modules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Saving the LoRA Adapter\n",
    "\n",
    "Now, let's save the LoRA adapter weights separately from the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for the adapter\n",
    "adapter_path = \"./lora-adapter\"\n",
    "os.makedirs(adapter_path, exist_ok=True)\n",
    "\n",
    "# Save the adapter weights\n",
    "peft_model.save_pretrained(adapter_path)\n",
    "\n",
    "print(f\"LoRA adapter saved to {adapter_path}\")\n",
    "\n",
    "# Check the size of the saved adapter\n",
    "!du -sh {adapter_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loading the Base Model and Adapter Separately\n",
    "\n",
    "Let's load the base model and adapter separately, which is the on-the-fly approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model again\n",
    "fresh_base_model = AutoModelForCausalLM.from_pretrained(base_model_name).to(device)\n",
    "\n",
    "# Load the adapter configuration\n",
    "adapter_config = PeftConfig.from_pretrained(adapter_path)\n",
    "print(f\"Adapter config: {adapter_config}\")\n",
    "\n",
    "# Load the adapter with the base model\n",
    "adapter_model = PeftModel.from_pretrained(fresh_base_model, adapter_path)\n",
    "\n",
    "print(f\"Successfully loaded adapter model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Merging the Adapter with the Base Model\n",
    "\n",
    "Now, let's merge the adapter weights with the base model to create a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the adapter with the base model\n",
    "merged_model = adapter_model.merge_and_unload()\n",
    "\n",
    "print(f\"Successfully merged adapter with base model\")\n",
    "\n",
    "# Verify that the merged model has the same number of parameters as the base model\n",
    "print(f\"Base model parameters: {count_parameters(fresh_base_model):,}\")\n",
    "print(f\"Merged model parameters: {count_parameters(merged_model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing Inference Speed: On-the-fly vs. Merged\n",
    "\n",
    "Let's compare the inference speed of the on-the-fly adapter approach versus the merged model approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to measure inference time\n",
    "def measure_inference_time(model, tokenizer, prompt, num_runs=10):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Warm-up run\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(**inputs, max_length=50, num_return_sequences=1)\n",
    "    \n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            _ = model.generate(**inputs, max_length=50, num_return_sequences=1)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / num_runs\n",
    "    return avg_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare inference speed\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "# Measure on-the-fly adapter inference time\n",
    "adapter_time = measure_inference_time(adapter_model, tokenizer, prompt)\n",
    "print(f\"On-the-fly adapter inference time: {adapter_time:.4f} seconds per run\")\n",
    "\n",
    "# Measure merged model inference time\n",
    "merged_time = measure_inference_time(merged_model, tokenizer, prompt)\n",
    "print(f\"Merged model inference time: {merged_time:.4f} seconds per run\")\n",
    "\n",
    "# Calculate speedup\n",
    "speedup = adapter_time / merged_time\n",
    "print(f\"Speedup from merging: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparing Memory Usage: On-the-fly vs. Merged\n",
    "\n",
    "Let's also compare the memory usage of both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to measure peak memory usage\n",
    "def measure_peak_memory(model, tokenizer, prompt):\n",
    "    if torch.cuda.is_available():\n",
    "        # Reset peak memory stats\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Tokenize the input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(**inputs, max_length=50, num_return_sequences=1)\n",
    "        \n",
    "        # Get peak memory usage\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / (1024 * 1024)  # MB\n",
    "        return peak_memory\n",
    "    else:\n",
    "        return \"N/A (CUDA not available)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare memory usage\n",
    "if torch.cuda.is_available():\n",
    "    # Measure on-the-fly adapter memory usage\n",
    "    adapter_memory = measure_peak_memory(adapter_model, tokenizer, prompt)\n",
    "    print(f\"On-the-fly adapter peak memory: {adapter_memory:.2f} MB\")\n",
    "    \n",
    "    # Measure merged model memory usage\n",
    "    merged_memory = measure_peak_memory(merged_model, tokenizer, prompt)\n",
    "    print(f\"Merged model peak memory: {merged_memory:.2f} MB\")\n",
    "    \n",
    "    # Calculate memory difference\n",
    "    memory_diff = adapter_memory - merged_memory\n",
    "    print(f\"Memory difference: {memory_diff:.2f} MB ({memory_diff / adapter_memory * 100:.2f}%)\")\n",
    "else:\n",
    "    print(\"CUDA not available for memory measurement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Saving the Merged Model\n",
    "\n",
    "Let's save the merged model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for the merged model\n",
    "merged_model_path = \"./merged-model\"\n",
    "os.makedirs(merged_model_path, exist_ok=True)\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(merged_model_path)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(merged_model_path)\n",
    "\n",
    "print(f\"Merged model saved to {merged_model_path}\")\n",
    "\n",
    "# Check the size of the saved model\n",
    "!du -sh {merged_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Loading and Using the Merged Model\n",
    "\n",
    "Now, let's load the merged model and use it for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the merged model\n",
    "loaded_merged_model = AutoModelForCausalLM.from_pretrained(merged_model_path).to(device)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(merged_model_path)\n",
    "\n",
    "print(f\"Successfully loaded merged model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=loaded_merged_model,\n",
    "    tokenizer=loaded_tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Generate text\n",
    "prompts = [\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"Climate change will impact our planet by\",\n",
    "    \"The most important skill for the 21st century is\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    result = generator(prompt, max_length=50, num_return_sequences=1, do_sample=True, temperature=0.7)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {result[0]['generated_text']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Comparing File Sizes: Adapter vs. Merged Model\n",
    "\n",
    "Let's compare the file sizes of the adapter and the merged model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file sizes\n",
    "!echo \"Adapter size:\"\n",
    "!du -sh {adapter_path}\n",
    "\n",
    "!echo \"\\nMerged model size:\"\n",
    "!du -sh {merged_model_path}\n",
    "\n",
    "!echo \"\\nBase model size:\"\n",
    "!du -sh $(python -c \"import transformers; print(transformers.AutoModelForCausalLM.from_pretrained('{base_model_name}', local_files_only=False).config.name_or_path)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored two approaches to adapter management:\n",
    "\n",
    "1. **On-the-fly Adapter Composition**: Keeping the base model and adapters separate and applying the adapters during inference.\n",
    "   - Advantages: Smaller artifacts, flexibility to switch adapters\n",
    "   - Disadvantages: Slightly higher computational overhead, requires custom code\n",
    "\n",
    "2. **Adapter Merging**: Combining the weights of the base model and adapters into a single model.\n",
    "   - Advantages: Faster inference, standard pipelines work without modification\n",
    "   - Disadvantages: Larger artifact size, cannot switch adapters dynamically\n",
    "\n",
    "We've seen that merging adapters can provide a speed boost for inference while maintaining the same model quality. However, it comes at the cost of larger artifact sizes and loss of modularity.\n",
    "\n",
    "The choice between these approaches depends on your specific deployment needs:\n",
    "- For simple deployments where performance is critical, merged models are preferable.\n",
    "- For scenarios requiring flexibility to switch between tasks or where storage is limited, on-the-fly adapters may be better.\n",
    "\n",
    "In Part 2, we'll explore model documentation and creating comprehensive model cards for responsible sharing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
