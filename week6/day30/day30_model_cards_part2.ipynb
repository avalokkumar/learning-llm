{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 30: Model Cards and Documentation - Part 2\n",
    "\n",
    "In this notebook, we'll explore how to create comprehensive model cards and documentation for your fine-tuned models. Good documentation is essential for responsible sharing and use of machine learning models.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Understanding model cards\n",
    "2. Creating a model card template\n",
    "3. Documenting model performance\n",
    "4. Implementing a model card for our fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datetime import datetime\n",
    "\n",
    "# Set paths\n",
    "merged_model_path = \"./merged-model\"  # Path to our merged model from Part 1\n",
    "model_card_path = \"./model-card\"  # Path to save our model card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Model Cards\n",
    "\n",
    "Model cards are structured documentation that provide essential information about machine learning models. They were introduced by Mitchell et al. (2019) to promote transparency and accountability in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Components of a Model Card\n",
    "\n",
    "1. **Model Details**: Basic information about the model architecture, version, and creators\n",
    "2. **Intended Use**: Primary use cases and out-of-scope uses\n",
    "3. **Training Data**: Information about the data used to train the model\n",
    "4. **Performance**: Evaluation metrics and benchmarks\n",
    "5. **Limitations**: Known limitations and biases\n",
    "6. **Ethical Considerations**: Potential risks and mitigations\n",
    "7. **Technical Specifications**: Hardware requirements and inference details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a Model Card Template\n",
    "\n",
    "Let's create a template for our model card based on the Hugging Face model card specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model card template\n",
    "def create_model_card_template(model_name, model_description, base_model, creators, license_info):\n",
    "    template = f\"\"\"---\n",
    "language: en\n",
    "license: {license_info}\n",
    "library_name: transformers\n",
    "tags:\n",
    "- lora\n",
    "- adapter\n",
    "- fine-tuned\n",
    "- text-generation\n",
    "---\n",
    "\n",
    "# {model_name}\n",
    "\n",
    "## Model Description\n",
    "\n",
    "{model_description}\n",
    "\n",
    "## Model Details\n",
    "\n",
    "- **Developed by:** {creators}\n",
    "- **Base Model:** {base_model}\n",
    "- **Model type:** Fine-tuned language model with LoRA adapters\n",
    "- **Language(s):** English\n",
    "- **License:** {license_info}\n",
    "- **Last updated:** {datetime.now().strftime('%Y-%m-%d')}\n",
    "\n",
    "## Intended Use\n",
    "\n",
    "### Primary Intended Uses\n",
    "\n",
    "- Text generation for [specific use case]\n",
    "- [Other intended uses]\n",
    "\n",
    "### Out-of-Scope Uses\n",
    "\n",
    "- Production systems without human review\n",
    "- Critical decision-making without oversight\n",
    "- [Other out-of-scope uses]\n",
    "\n",
    "## Training Data\n",
    "\n",
    "- **Training Dataset:** [Dataset name and description]\n",
    "- **Preprocessing:** [Description of preprocessing steps]\n",
    "- **Training-Validation Split:** [Split details]\n",
    "\n",
    "## Performance and Limitations\n",
    "\n",
    "### Performance Measures\n",
    "\n",
    "- [Key metrics and results]\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- [Known limitations of the model]\n",
    "- [Potential biases]\n",
    "\n",
    "## Ethical Considerations\n",
    "\n",
    "- [Potential risks and ethical concerns]\n",
    "- [Mitigation strategies]\n",
    "\n",
    "## Technical Specifications\n",
    "\n",
    "- **Hardware Requirements:** [Minimum hardware requirements]\n",
    "- **Inference Speed:** [Inference benchmarks]\n",
    "- **Model Size:** [Size of model artifacts]\n",
    "\n",
    "## How to Use\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"[model_path]\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"[model_path]\")\n",
    "\n",
    "# Generate text\n",
    "inputs = tokenizer(\"Your prompt here\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "## Citation and Contact\n",
    "\n",
    "- **Citation:** [How to cite the model]\n",
    "- **Contact:** [Contact information]\n",
    "\"\"\"\n",
    "    return template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Documenting Model Performance\n",
    "\n",
    "Let's create some example performance metrics for our model. In a real scenario, you would use actual evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example performance metrics\n",
    "performance_metrics = {\n",
    "    \"perplexity\": 15.3,\n",
    "    \"accuracy\": 0.87,\n",
    "    \"f1_score\": 0.85,\n",
    "    \"inference_time_ms\": 120,\n",
    "    \"memory_usage_mb\": 512\n",
    "}\n",
    "\n",
    "# Create a visualization of the metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot accuracy and F1 score\n",
    "plt.subplot(1, 2, 1)\n",
    "metrics = [\"accuracy\", \"f1_score\"]\n",
    "values = [performance_metrics[m] for m in metrics]\n",
    "plt.bar(metrics, values, color=[\"blue\", \"green\"])\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Performance Metrics\")\n",
    "\n",
    "# Plot perplexity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar([\"perplexity\"], [performance_metrics[\"perplexity\"]], color=\"red\")\n",
    "plt.title(\"Perplexity (lower is better)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"performance_metrics.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementing a Model Card for Our Fine-tuned Model\n",
    "\n",
    "Now, let's create a specific model card for our fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our model to get details\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(merged_model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(merged_model_path)\n",
    "    \n",
    "    # Get model details\n",
    "    model_name = \"GPT-2 with LoRA Fine-tuning\"\n",
    "    base_model = \"gpt2\"\n",
    "    model_size = sum(p.numel() for p in model.parameters()) / 1_000_000  # in millions\n",
    "    \n",
    "    print(f\"Model loaded: {model_name}\")\n",
    "    print(f\"Base model: {base_model}\")\n",
    "    print(f\"Model size: {model_size:.2f}M parameters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    # Use default values if model loading fails\n",
    "    model_name = \"GPT-2 with LoRA Fine-tuning\"\n",
    "    base_model = \"gpt2\"\n",
    "    model_size = 124  # GPT-2 has approximately 124M parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our specific model card\n",
    "model_description = f\"\"\"This is a fine-tuned version of {base_model} using LoRA adapters. The model was trained to improve text generation capabilities for specific topics. The LoRA adapters were merged with the base model for efficient inference.\n",
    "\n",
    "The model uses a rank-8 LoRA configuration targeting attention modules, which allows for efficient fine-tuning while maintaining most of the base model's general knowledge.\"\"\"\n",
    "\n",
    "creators = \"Your Name\"\n",
    "license_info = \"MIT\"\n",
    "\n",
    "model_card = create_model_card_template(\n",
    "    model_name=model_name,\n",
    "    model_description=model_description,\n",
    "    base_model=base_model,\n",
    "    creators=creators,\n",
    "    license_info=license_info\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add performance metrics to the model card\n",
    "performance_section = f\"\"\"\n",
    "## Performance Measures\n",
    "\n",
    "- **Perplexity:** {performance_metrics['perplexity']:.2f}\n",
    "- **Accuracy:** {performance_metrics['accuracy']:.2f}\n",
    "- **F1 Score:** {performance_metrics['f1_score']:.2f}\n",
    "- **Inference Time:** {performance_metrics['inference_time_ms']} ms per generation\n",
    "- **Memory Usage:** {performance_metrics['memory_usage_mb']} MB\n",
    "\n",
    "![Performance Metrics](performance_metrics.png)\n",
    "\"\"\"\n",
    "\n",
    "# Replace the placeholder performance section\n",
    "model_card = model_card.replace(\"- [Key metrics and results]\", performance_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add technical specifications\n",
    "technical_specs = f\"\"\"\n",
    "- **Model Type:** Transformer-based language model\n",
    "- **Model Size:** {model_size:.2f}M parameters\n",
    "- **Hardware Requirements:** GPU with at least 4GB VRAM for inference\n",
    "- **Inference Speed:** {performance_metrics['inference_time_ms']} ms per generation (average)\n",
    "- **Model Size on Disk:** Approximately 500MB\n",
    "\"\"\"\n",
    "\n",
    "# Replace the placeholder technical specifications\n",
    "model_card = model_card.replace(\"- **Hardware Requirements:** [Minimum hardware requirements]\\n- **Inference Speed:** [Inference benchmarks]\\n- **Model Size:** [Size of model artifacts]\", technical_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add example usage specific to our model\n",
    "usage_example = f\"\"\"\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"{model_name.lower().replace(' ', '-')}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{model_name.lower().replace(' ', '-')}\")\n",
    "\n",
    "# Generate text\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100, do_sample=True, temperature=0.7)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# Replace the placeholder usage example\n",
    "model_card = model_card.replace(\"```python\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n# Load model and tokenizer\\nmodel = AutoModelForCausalLM.from_pretrained(\\\"[model_path]\\\")\\ntokenizer = AutoTokenizer.from_pretrained(\\\"[model_path]\\\")\\n\\n# Generate text\\ninputs = tokenizer(\\\"Your prompt here\\\", return_tensors=\\\"pt\\\")\\noutputs = model.generate(**inputs, max_length=100)\\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\\n```\", usage_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model card\n",
    "os.makedirs(model_card_path, exist_ok=True)\n",
    "with open(f\"{model_card_path}/README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "# Copy the performance metrics image to the model card directory\n",
    "import shutil\n",
    "shutil.copy(\"performance_metrics.png\", f\"{model_card_path}/performance_metrics.png\")\n",
    "\n",
    "print(f\"Model card saved to {model_card_path}/README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Viewing the Model Card\n",
    "\n",
    "Let's take a look at our model card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the model card\n",
    "with open(f\"{model_card_path}/README.md\", \"r\") as f:\n",
    "    model_card_content = f.read()\n",
    "\n",
    "print(model_card_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Adding Metadata for Model Registries\n",
    "\n",
    "Many model registries (like Hugging Face Hub) use metadata files to provide additional information about the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metadata for Hugging Face Hub\n",
    "metadata = {\n",
    "    \"language\": \"en\",\n",
    "    \"license\": license_info,\n",
    "    \"library_name\": \"transformers\",\n",
    "    \"tags\": [\"lora\", \"adapter\", \"fine-tuned\", \"text-generation\"],\n",
    "    \"datasets\": [\"custom\"],\n",
    "    \"metrics\": [\"perplexity\", \"accuracy\"],\n",
    "    \"model-index\": [\n",
    "        {\n",
    "            \"name\": model_name,\n",
    "            \"results\": [\n",
    "                {\n",
    "                    \"task\": {\n",
    "                        \"type\": \"text-generation\"\n",
    "                    },\n",
    "                    \"dataset\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"name\": \"Custom Dataset\"\n",
    "                    },\n",
    "                    \"metrics\": [\n",
    "                        {\n",
    "                            \"type\": \"perplexity\",\n",
    "                            \"value\": performance_metrics[\"perplexity\"],\n",
    "                            \"name\": \"Perplexity\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"accuracy\",\n",
    "                            \"value\": performance_metrics[\"accuracy\"],\n",
    "                            \"name\": \"Accuracy\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "with open(f\"{model_card_path}/metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Metadata saved to {model_card_path}/metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Creating a License File\n",
    "\n",
    "Including a license file is important for clarifying how others can use your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an MIT license file\n",
    "mit_license = f\"\"\"MIT License\n",
    "\n",
    "Copyright (c) {datetime.now().year} {creators}\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\"\"\"\n",
    "\n",
    "# Save the license file\n",
    "with open(f\"{model_card_path}/LICENSE\", \"w\") as f:\n",
    "    f.write(mit_license)\n",
    "\n",
    "print(f\"License file saved to {model_card_path}/LICENSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored how to create comprehensive documentation for fine-tuned models:\n",
    "\n",
    "1. We created a detailed model card following best practices\n",
    "2. We added performance metrics and visualizations\n",
    "3. We included metadata for model registries\n",
    "4. We added a license file to clarify usage rights\n",
    "\n",
    "Good documentation is essential for responsible model sharing and use. By providing clear information about your model's capabilities, limitations, and intended uses, you help others use your model appropriately and build upon your work.\n",
    "\n",
    "In a real-world scenario, you would include more detailed information about:\n",
    "- The specific dataset used for fine-tuning\n",
    "- Comprehensive evaluation results across different metrics and datasets\n",
    "- Known biases and limitations based on thorough testing\n",
    "- Specific ethical considerations relevant to your model's domain\n",
    "\n",
    "This documentation, combined with the adapter management techniques from Part 1, provides a solid foundation for packaging and sharing your fine-tuned models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
