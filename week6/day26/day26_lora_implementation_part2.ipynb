{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 26: LoRA Implementation - Part 2\n",
    "\n",
    "In this notebook, we'll explore more advanced LoRA techniques, including:\n",
    "\n",
    "1. Adapter merging\n",
    "2. Multi-task adaptation\n",
    "3. Different target modules and freezing strategies\n",
    "4. LoRA for generative models\n",
    "\n",
    "We'll build on the concepts from Part 1 and see how LoRA can be applied in more complex scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft evaluate accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    PeftModel,\n",
    "    PeftConfig\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LoRA for Generative Models\n",
    "\n",
    "Let's apply LoRA to a generative model like GPT-2 for a text generation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained generative model\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token by default\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Define a function to count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure LoRA for the Generative Model\n",
    "\n",
    "For generative models, we'll target both attention and MLP layers for more comprehensive adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA configuration for generative model\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,  # Causal language modeling\n",
    "    r=16,                          # Higher rank for more capacity\n",
    "    lora_alpha=32,                 # Alpha scaling factor\n",
    "    lora_dropout=0.05,             # Dropout probability\n",
    "    # Target both attention and MLP layers\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# Create the PEFT model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model = peft_model.to(device)\n",
    "\n",
    "# Print trainable parameters\n",
    "print(f\"Full model parameters: {count_parameters(model):,}\")\n",
    "print(f\"LoRA model trainable parameters: {count_parameters(peft_model):,}\")\n",
    "print(f\"Parameter efficiency: {count_parameters(peft_model) / count_parameters(model) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a Dataset for Fine-tuning\n",
    "\n",
    "We'll use a small dataset of poetry to fine-tune our generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a poetry dataset\n",
    "dataset = load_dataset(\"merve/poetry\")\n",
    "print(dataset)\n",
    "\n",
    "# Look at a few examples\n",
    "for i in range(3):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Title: {dataset['train'][i]['title']}\")\n",
    "    print(f\"Content:\\n{dataset['train'][i]['content'][:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    # Combine title and content\n",
    "    texts = [f\"Title: {title}\\n\\n{content}\" for title, content in zip(examples[\"title\"], examples[\"content\"])]\n",
    "    return tokenizer(texts, truncation=True, max_length=512)\n",
    "\n",
    "# Process the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"title\", \"content\", \"author\"])\n",
    "\n",
    "# Create smaller datasets for demonstration\n",
    "train_dataset = tokenized_datasets[\"train\"].select(range(1000))  # Use 1000 examples for training\n",
    "eval_dataset = tokenized_datasets[\"train\"].select(range(1000, 1100))  # Use 100 examples for evaluation\n",
    "\n",
    "# Create a data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Generative Model with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/gpt2-poetry-lora\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Create the trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text with the LoRA-adapted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapter\n",
    "peft_model_path = \"./lora-gpt2-poetry\"\n",
    "peft_model.save_pretrained(peft_model_path)\n",
    "\n",
    "# Function for text generation\n",
    "def generate_text(model, tokenizer, prompt, max_length=200):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Test with the fine-tuned model\n",
    "prompts = [\n",
    "    \"Title: Sunset\\n\\n\",\n",
    "    \"Title: The Ocean's Whisper\\n\\n\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated_text = generate_text(peft_model, tokenizer, prompt)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated:\\n{generated_text}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adapter Merging\n",
    "\n",
    "One of the powerful features of LoRA is the ability to merge adapters with the base model or with other adapters. Let's explore this capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Load the LoRA adapter\n",
    "peft_model_loaded = PeftModel.from_pretrained(base_model, peft_model_path)\n",
    "\n",
    "# Merge the adapter with the base model\n",
    "merged_model = peft_model_loaded.merge_and_unload()\n",
    "\n",
    "# Check that the merged model has the same number of parameters as the base model\n",
    "print(f\"Base model parameters: {sum(p.numel() for p in base_model.parameters()):,}\")\n",
    "print(f\"Merged model parameters: {sum(p.numel() for p in merged_model.parameters()):,}\")\n",
    "\n",
    "# Test the merged model\n",
    "merged_model = merged_model.to(device)\n",
    "for prompt in prompts:\n",
    "    generated_text = generate_text(merged_model, tokenizer, prompt)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated (Merged Model):\\n{generated_text}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Task Adaptation with LoRA\n",
    "\n",
    "Let's create a second adapter for a different task and see how we can switch between adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, we'll create a simplified second task (technical writing)\n",
    "# In a real scenario, you would train this on a different dataset\n",
    "\n",
    "# Create a mock technical writing adapter\n",
    "technical_lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# For demonstration purposes, we'll just save a copy of our poetry adapter with a different name\n",
    "# In practice, you would train this on technical writing data\n",
    "technical_adapter_path = \"./lora-gpt2-technical\"\n",
    "peft_model.save_pretrained(technical_adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model again\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Function to load an adapter and generate text\n",
    "def generate_with_adapter(adapter_path, prompt):\n",
    "    # Load the adapter\n",
    "    adapted_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    \n",
    "    # Generate text\n",
    "    return generate_text(adapted_model, tokenizer, prompt)\n",
    "\n",
    "# Test with different adapters\n",
    "prompt = \"Title: The Future of AI\\n\\n\"\n",
    "\n",
    "print(\"Using Poetry Adapter:\")\n",
    "poetry_output = generate_with_adapter(peft_model_path, prompt)\n",
    "print(poetry_output)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"Using Technical Adapter:\")\n",
    "technical_output = generate_with_adapter(technical_adapter_path, prompt)\n",
    "print(technical_output)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Different Target Modules and Freezing Strategies\n",
    "\n",
    "Let's explore how targeting different layers affects the adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different LoRA configurations\n",
    "lora_configs = {\n",
    "    \"attention_only\": LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"c_attn\"],  # Only query, key, value projections\n",
    "        bias=\"none\",\n",
    "    ),\n",
    "    \"attention_output\": LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"c_proj\"],  # Only attention output projection\n",
    "        bias=\"none\",\n",
    "    ),\n",
    "    \"mlp_only\": LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"c_fc\"],  # Only MLP layers\n",
    "        bias=\"none\",\n",
    "    ),\n",
    "    \"comprehensive\": LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],  # All layers\n",
    "        bias=\"none\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Compare the number of trainable parameters for each configuration\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "for name, config in lora_configs.items():\n",
    "    model_with_lora = get_peft_model(base_model, config)\n",
    "    trainable_params = count_parameters(model_with_lora)\n",
    "    print(f\"{name}: {trainable_params:,} trainable parameters ({trainable_params / count_parameters(base_model) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored advanced LoRA techniques:\n",
    "\n",
    "1. **LoRA for Generative Models**: We applied LoRA to GPT-2 for poetry generation, showing how it works for generative tasks.\n",
    "\n",
    "2. **Adapter Merging**: We demonstrated how to merge LoRA adapters with the base model for deployment.\n",
    "\n",
    "3. **Multi-Task Adaptation**: We showed how multiple adapters can be used with a single base model for different tasks.\n",
    "\n",
    "4. **Different Target Modules**: We explored how targeting different layers affects the parameter count and adaptation capabilities.\n",
    "\n",
    "These techniques make LoRA a versatile and powerful tool for efficient fine-tuning of large language models. By strategically applying LoRA to specific layers and tasks, we can achieve excellent performance with minimal computational resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
