{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 26: LoRA Implementation - Part 1\n",
    "\n",
    "In this notebook, we'll implement Low-Rank Adaptation (LoRA) to fine-tune a pre-trained language model on a specific task. We'll use the Hugging Face PEFT library to apply LoRA to a base model and train it efficiently.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Setup and dependencies\n",
    "2. Loading a pre-trained model\n",
    "3. Configuring LoRA adapters\n",
    "4. Preparing a dataset for fine-tuning\n",
    "5. Training the model with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, let's install the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q transformers datasets peft evaluate accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    PeftModel,\n",
    "    PeftConfig\n",
    ")\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading a Pre-trained Model\n",
    "\n",
    "For this example, we'll use a RoBERTa model for sentiment analysis. We'll fine-tune it on the SST-2 (Stanford Sentiment Treebank) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model name\n",
    "model_name = \"roberta-base\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  # Binary classification for sentiment\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model size\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuring LoRA Adapters\n",
    "\n",
    "Now, let's configure LoRA to adapt only specific layers of the model. We'll target the attention layers in the transformer blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Sequence classification task\n",
    "    r=8,                         # Rank of the update matrices\n",
    "    lora_alpha=16,               # Alpha parameter for scaling\n",
    "    lora_dropout=0.1,            # Dropout probability for LoRA layers\n",
    "    target_modules=[\"query\", \"key\", \"value\"],  # Apply LoRA to attention layers\n",
    "    bias=\"none\",                 # Don't train bias parameters\n",
    ")\n",
    "\n",
    "# Create the PEFT model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "print(f\"Full model parameters: {count_parameters(model):,}\")\n",
    "print(f\"LoRA model trainable parameters: {count_parameters(peft_model):,}\")\n",
    "print(f\"Parameter efficiency: {count_parameters(peft_model) / count_parameters(model) * 100:.2f}%\")\n",
    "\n",
    "# Print the model architecture with LoRA adapters\n",
    "print(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing a Dataset for Fine-tuning\n",
    "\n",
    "We'll use the SST-2 dataset, which contains movie reviews labeled as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SST-2 dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "print(dataset)\n",
    "\n",
    "# Look at a few examples\n",
    "for i in range(3):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Text: {dataset['train'][i]['sentence']}\")\n",
    "    print(f\"Label: {dataset['train'][i]['label']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Prepare the datasets for training\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "# Create a data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Model with LoRA\n",
    "\n",
    "Now, let's set up the training arguments and train our model with LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics for evaluation\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/roberta-sst2-lora\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",  # Disable wandb, tensorboard, etc.\n",
    ")\n",
    "\n",
    "# Create the trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating the LoRA-adapted Model\n",
    "\n",
    "Let's evaluate our fine-tuned model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Saving the LoRA Adapter\n",
    "\n",
    "One of the key benefits of LoRA is that we only need to save the adapter weights, not the entire model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapter weights\n",
    "peft_model_path = \"./lora-roberta-sst2\"\n",
    "peft_model.save_pretrained(peft_model_path)\n",
    "\n",
    "print(f\"LoRA adapter saved to {peft_model_path}\")\n",
    "\n",
    "# Check the size of the saved adapter\n",
    "!du -sh {peft_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Loading and Using the LoRA Adapter\n",
    "\n",
    "Let's see how to load and use our trained LoRA adapter with the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model again\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "# Load the LoRA adapter\n",
    "peft_model_loaded = PeftModel.from_pretrained(base_model, peft_model_path)\n",
    "\n",
    "# Test the model on a sample input\n",
    "test_texts = [\n",
    "    \"This movie was fantastic! I really enjoyed it.\",\n",
    "    \"What a terrible waste of time. I hated every minute.\"\n",
    "]\n",
    "\n",
    "# Tokenize the inputs\n",
    "inputs = tokenizer(test_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    outputs = peft_model_loaded(**inputs)\n",
    "    predictions = torch.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Print results\n",
    "for i, text in enumerate(test_texts):\n",
    "    sentiment = \"positive\" if predictions[i][1] > predictions[i][0] else \"negative\"\n",
    "    confidence = predictions[i][1] if sentiment == \"positive\" else predictions[i][0]\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {sentiment} (confidence: {confidence:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've successfully implemented LoRA fine-tuning on a pre-trained RoBERTa model for sentiment analysis. We've seen how LoRA allows us to adapt a large model with only a small number of trainable parameters, making fine-tuning more efficient.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. LoRA significantly reduces the number of trainable parameters (typically <1% of the full model)\n",
    "2. The adapter weights are small and easy to store/distribute\n",
    "3. We can achieve competitive performance with much less computational resources\n",
    "4. The base model remains unchanged, allowing for multiple task adaptations\n",
    "\n",
    "In Part 2, we'll explore more advanced LoRA techniques, including adapter merging and multi-task adaptation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
