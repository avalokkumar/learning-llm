{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 28: Knowledge Distillation Implementation - Part 1\n",
    "\n",
    "In this notebook, we'll implement knowledge distillation for language models. We'll focus on response-based distillation, where a smaller student model learns to mimic the outputs of a larger teacher model.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Setup and dependencies\n",
    "2. Loading teacher and student models\n",
    "3. Preparing a dataset for distillation\n",
    "4. Implementing the distillation loss\n",
    "5. Training the student model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, let's install the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets torch evaluate accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Teacher and Student Models\n",
    "\n",
    "For this example, we'll use BERT-base as our teacher model and DistilBERT as our student model. We'll fine-tune them on a sentiment analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model names\n",
    "teacher_model_name = \"bert-base-uncased\"\n",
    "student_model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# Load tokenizers\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(student_model_name)\n",
    "\n",
    "# Load models for sequence classification (sentiment analysis)\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    teacher_model_name,\n",
    "    num_labels=2  # Binary classification for sentiment\n",
    ")\n",
    "\n",
    "student_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    student_model_name,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Move models to device\n",
    "teacher_model = teacher_model.to(device)\n",
    "student_model = student_model.to(device)\n",
    "\n",
    "# Print model sizes\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Teacher model ({teacher_model_name}) has {count_parameters(teacher_model):,} parameters\")\n",
    "print(f\"Student model ({student_model_name}) has {count_parameters(student_model):,} parameters\")\n",
    "print(f\"Size reduction: {count_parameters(teacher_model) / count_parameters(student_model):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing a Dataset for Distillation\n",
    "\n",
    "We'll use the SST-2 (Stanford Sentiment Treebank) dataset for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SST-2 dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "print(dataset)\n",
    "\n",
    "# Look at a few examples\n",
    "for i in range(3):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Text: {dataset['train'][i]['sentence']}\")\n",
    "    print(f\"Label: {dataset['train'][i]['label']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    # We'll use the student's tokenizer for both models to ensure compatibility\n",
    "    return student_tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Prepare the datasets for training\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "# Create a data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=student_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-tuning the Teacher Model\n",
    "\n",
    "Before we can distill knowledge from the teacher, we need to fine-tune it on our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics for evaluation\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments for the teacher\n",
    "teacher_training_args = TrainingArguments(\n",
    "    output_dir=\"./results/teacher-sst2\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",  # Disable wandb, tensorboard, etc.\n",
    ")\n",
    "\n",
    "# Create the trainer for the teacher\n",
    "teacher_trainer = Trainer(\n",
    "    model=teacher_model,\n",
    "    args=teacher_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=teacher_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the teacher model\n",
    "teacher_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the teacher model\n",
    "teacher_eval_results = teacher_trainer.evaluate()\n",
    "print(f\"Teacher model evaluation results: {teacher_eval_results}\")\n",
    "\n",
    "# Save the fine-tuned teacher model\n",
    "teacher_model_path = \"./teacher-sst2\"\n",
    "teacher_model.save_pretrained(teacher_model_path)\n",
    "teacher_tokenizer.save_pretrained(teacher_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generating Soft Labels from the Teacher\n",
    "\n",
    "Now, we'll use the fine-tuned teacher model to generate soft labels (logits) for our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate soft labels (logits) from the teacher\n",
    "def generate_soft_labels(model, dataset, batch_size=32):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_logits = []\n",
    "    \n",
    "    # Create a dataloader\n",
    "    from torch.utils.data import DataLoader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    \n",
    "    # Generate logits batch by batch\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for batch in dataloader:\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Store logits\n",
    "            all_logits.append(logits.cpu())\n",
    "    \n",
    "    # Concatenate all logits\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    \n",
    "    return all_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate soft labels for the training dataset\n",
    "print(\"Generating soft labels from the teacher model...\")\n",
    "teacher_logits = generate_soft_labels(teacher_model, train_dataset)\n",
    "print(f\"Generated logits shape: {teacher_logits.shape}\")\n",
    "\n",
    "# Look at a few examples of soft labels\n",
    "for i in range(3):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Text: {train_dataset[i]['sentence']}\")\n",
    "    print(f\"Hard label: {train_dataset[i]['label']}\")\n",
    "    print(f\"Soft logits: {teacher_logits[i]}\")\n",
    "    print(f\"Soft probabilities: {F.softmax(teacher_logits[i], dim=0)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementing the Distillation Loss\n",
    "\n",
    "Now, let's implement a custom trainer that combines the standard cross-entropy loss with the distillation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom trainer for knowledge distillation\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_logits=None, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_logits = teacher_logits\n",
    "        self.alpha = alpha  # Weight for the distillation loss\n",
    "        self.temperature = temperature  # Temperature for softening the distributions\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Get the index in the dataset\n",
    "        if hasattr(inputs, \"idx\"):\n",
    "            idx = inputs.pop(\"idx\")\n",
    "        else:\n",
    "            # If idx is not provided, try to infer it from the input_ids\n",
    "            # This is a simplification and might not work in all cases\n",
    "            idx = torch.arange(inputs[\"input_ids\"].shape[0])\n",
    "        \n",
    "        # Standard forward pass\n",
    "        outputs = model(**inputs)\n",
    "        student_logits = outputs.logits\n",
    "        \n",
    "        # Get the teacher's logits for this batch\n",
    "        teacher_logits_batch = self.teacher_logits[idx].to(device)\n",
    "        \n",
    "        # Standard cross-entropy loss\n",
    "        hard_loss = outputs.loss\n",
    "        \n",
    "        # Distillation loss (KL divergence)\n",
    "        soft_targets = F.softmax(teacher_logits_batch / self.temperature, dim=-1)\n",
    "        soft_predictions = F.log_softmax(student_logits / self.temperature, dim=-1)\n",
    "        distillation_loss = F.kl_div(soft_predictions, soft_targets, reduction=\"batchmean\") * (self.temperature ** 2)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = self.alpha * hard_loss + (1 - self.alpha) * distillation_loss\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training the Student Model with Distillation\n",
    "\n",
    "Now, let's train the student model using our custom distillation trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments for the student\n",
    "student_training_args = TrainingArguments(\n",
    "    output_dir=\"./results/student-sst2-distilled\",\n",
    "    learning_rate=5e-5,  # Slightly higher learning rate for the student\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,  # More epochs for the student\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Create the distillation trainer\n",
    "distillation_trainer = DistillationTrainer(\n",
    "    model=student_model,\n",
    "    args=student_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=student_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    teacher_logits=teacher_logits,\n",
    "    alpha=0.5,  # Equal weight to hard and soft targets\n",
    "    temperature=2.0,  # Temperature for softening\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the student model with distillation\n",
    "distillation_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the distilled student model\n",
    "student_eval_results = distillation_trainer.evaluate()\n",
    "print(f\"Distilled student model evaluation results: {student_eval_results}\")\n",
    "\n",
    "# Save the distilled student model\n",
    "student_model_path = \"./student-sst2-distilled\"\n",
    "student_model.save_pretrained(student_model_path)\n",
    "student_tokenizer.save_pretrained(student_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparing Teacher and Student Performance\n",
    "\n",
    "Let's compare the performance of the teacher and student models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the results\n",
    "print(f\"Teacher model accuracy: {teacher_eval_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Distilled student model accuracy: {student_eval_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Performance retention: {student_eval_results['eval_accuracy'] / teacher_eval_results['eval_accuracy'] * 100:.2f}%\")\n",
    "print(f\"Size reduction: {count_parameters(teacher_model) / count_parameters(student_model):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Testing on Individual Examples\n",
    "\n",
    "Let's test both models on some individual examples to see how their predictions compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get predictions from a model\n",
    "def get_prediction(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "    prediction = torch.argmax(probabilities, dim=-1).item()\n",
    "    confidence = probabilities[0][prediction].item()\n",
    "    \n",
    "    return prediction, confidence, probabilities[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test examples\n",
    "test_examples = [\n",
    "    \"This movie was fantastic! I really enjoyed it.\",\n",
    "    \"What a terrible waste of time. I hated every minute.\",\n",
    "    \"The film was neither good nor bad, just mediocre.\",\n",
    "    \"While it had some flaws, overall I'd recommend seeing it.\"\n",
    "]\n",
    "\n",
    "# Compare predictions\n",
    "for text in test_examples:\n",
    "    print(f\"Text: {text}\")\n",
    "    \n",
    "    # Teacher prediction\n",
    "    teacher_pred, teacher_conf, teacher_probs = get_prediction(teacher_model, teacher_tokenizer, text)\n",
    "    teacher_sentiment = \"positive\" if teacher_pred == 1 else \"negative\"\n",
    "    print(f\"Teacher: {teacher_sentiment} (confidence: {teacher_conf:.4f}, probs: {teacher_probs})\")\n",
    "    \n",
    "    # Student prediction\n",
    "    student_pred, student_conf, student_probs = get_prediction(student_model, student_tokenizer, text)\n",
    "    student_sentiment = \"positive\" if student_pred == 1 else \"negative\"\n",
    "    print(f\"Student: {student_sentiment} (confidence: {student_conf:.4f}, probs: {student_probs})\")\n",
    "    \n",
    "    # Check if they agree\n",
    "    agreement = \"✓\" if teacher_pred == student_pred else \"✗\"\n",
    "    print(f\"Agreement: {agreement}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've implemented knowledge distillation for language models. We've seen how to:\n",
    "\n",
    "1. Fine-tune a teacher model on a specific task\n",
    "2. Generate soft labels from the teacher model\n",
    "3. Implement a custom distillation loss that combines hard and soft targets\n",
    "4. Train a smaller student model using knowledge distillation\n",
    "5. Compare the performance of the teacher and student models\n",
    "\n",
    "The distilled student model achieves comparable performance to the teacher model while being significantly smaller. This demonstrates the power of knowledge distillation for creating efficient models that can be deployed in resource-constrained environments.\n",
    "\n",
    "In Part 2, we'll explore more advanced distillation techniques, including feature-based distillation and multi-task distillation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
