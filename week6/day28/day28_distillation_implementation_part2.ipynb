{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 28: Knowledge Distillation Implementation - Part 2\n",
    "\n",
    "In this notebook, we'll explore advanced knowledge distillation techniques for language models, focusing on feature-based distillation and generating synthetic data for distillation.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Setup and dependencies\n",
    "2. Feature-based distillation (hidden states)\n",
    "3. Generating synthetic data for distillation\n",
    "4. Evaluating the distilled model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature-Based Distillation\n",
    "\n",
    "In feature-based distillation, the student learns to mimic the internal representations (hidden states) of the teacher model, not just the final outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load teacher and student models for causal language modeling\n",
    "teacher_model_name = \"gpt2-medium\"  # 345M parameters\n",
    "student_model_name = \"gpt2\"         # 124M parameters\n",
    "\n",
    "# Load tokenizers\n",
    "tokenizer = AutoTokenizer.from_pretrained(student_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load models with output_hidden_states=True to access internal representations\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "    teacher_model_name,\n",
    "    output_hidden_states=True\n",
    ")\n",
    "\n",
    "student_model = AutoModelForCausalLM.from_pretrained(\n",
    "    student_model_name,\n",
    "    output_hidden_states=True\n",
    ")\n",
    "\n",
    "# Move models to device\n",
    "teacher_model = teacher_model.to(device)\n",
    "student_model = student_model.to(device)\n",
    "\n",
    "# Print model sizes\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Teacher model ({teacher_model_name}) has {count_parameters(teacher_model):,} parameters\")\n",
    "print(f\"Student model ({student_model_name}) has {count_parameters(student_model):,} parameters\")\n",
    "print(f\"Size reduction: {count_parameters(teacher_model) / count_parameters(student_model):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Implementing Feature-Based Distillation Loss\n",
    "\n",
    "We'll create a custom loss function that combines:\n",
    "1. Language modeling loss (next token prediction)\n",
    "2. Hidden state distillation loss (MSE between teacher and student hidden states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom model for feature-based distillation\n",
    "class FeatureDistillationModel(nn.Module):\n",
    "    def __init__(self, student_model):\n",
    "        super().__init__()\n",
    "        self.student = student_model\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, teacher_hidden_states=None):\n",
    "        # Forward pass through student model\n",
    "        outputs = self.student(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "        # Standard language modeling loss\n",
    "        lm_loss = outputs.loss\n",
    "        \n",
    "        # Feature distillation loss\n",
    "        feature_loss = 0\n",
    "        if teacher_hidden_states is not None:\n",
    "            student_hidden_states = outputs.hidden_states\n",
    "            \n",
    "            # We'll use the last hidden state for distillation\n",
    "            # For more comprehensive distillation, you could use multiple layers\n",
    "            teacher_last_hidden = teacher_hidden_states[-1]\n",
    "            student_last_hidden = student_hidden_states[-1]\n",
    "            \n",
    "            # MSE loss between hidden states\n",
    "            feature_loss = F.mse_loss(student_last_hidden, teacher_last_hidden)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = lm_loss + feature_loss\n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"lm_loss\": lm_loss,\n",
    "            \"feature_loss\": feature_loss,\n",
    "            \"logits\": outputs.logits,\n",
    "            \"hidden_states\": outputs.hidden_states\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generating Synthetic Data for Distillation\n",
    "\n",
    "One powerful approach to distillation is to generate synthetic data using the teacher model. This allows us to create a large, diverse dataset tailored to our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate synthetic data using the teacher model\n",
    "def generate_synthetic_data(model, tokenizer, prompts, num_samples=100, max_length=128):\n",
    "    model.eval()\n",
    "    generated_texts = []\n",
    "    \n",
    "    for prompt in tqdm(prompts * (num_samples // len(prompts) + 1)):\n",
    "        if len(generated_texts) >= num_samples:\n",
    "            break\n",
    "            \n",
    "        # Tokenize the prompt\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Generate text\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=max_length,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                temperature=0.7,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "        \n",
    "        # Decode the generated text\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_texts.append(generated_text)\n",
    "    \n",
    "    return generated_texts[:num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some diverse prompts for generation\n",
    "prompts = [\n",
    "    \"The best way to learn a new language is\",\n",
    "    \"In the future, artificial intelligence will\",\n",
    "    \"Climate change is affecting our planet by\",\n",
    "    \"The most important scientific discovery of the last century was\",\n",
    "    \"When it comes to healthy eating habits,\"\n",
    "]\n",
    "\n",
    "# Generate synthetic data (small sample for demonstration)\n",
    "print(\"Generating synthetic data...\")\n",
    "synthetic_texts = generate_synthetic_data(teacher_model, tokenizer, prompts, num_samples=10)\n",
    "\n",
    "# Display some examples\n",
    "for i, text in enumerate(synthetic_texts[:3]):\n",
    "    print(f\"Example {i+1}:\\n{text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from the synthetic texts\n",
    "from datasets import Dataset\n",
    "\n",
    "synthetic_dataset = Dataset.from_dict({\"text\": synthetic_texts})\n",
    "print(synthetic_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Preparing the Dataset for Feature-Based Distillation\n",
    "\n",
    "Now, we'll tokenize the synthetic data and extract the teacher's hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the synthetic dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = synthetic_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Create a data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # We're doing causal language modeling, not masked language modeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract teacher hidden states\n",
    "def extract_teacher_hidden_states(model, dataset, batch_size=2):\n",
    "    model.eval()\n",
    "    all_hidden_states = []\n",
    "    \n",
    "    # Create a dataloader\n",
    "    from torch.utils.data import DataLoader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    \n",
    "    # Extract hidden states batch by batch\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch, output_hidden_states=True)\n",
    "            \n",
    "            # Store hidden states\n",
    "            hidden_states = outputs.hidden_states\n",
    "            all_hidden_states.append([h.cpu() for h in hidden_states])\n",
    "    \n",
    "    return all_hidden_states\n",
    "\n",
    "# Extract teacher hidden states (this would be done for the full dataset in practice)\n",
    "print(\"Extracting teacher hidden states...\")\n",
    "teacher_hidden_states = extract_teacher_hidden_states(teacher_model, tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Trainer for Feature-Based Distillation\n",
    "\n",
    "Now, let's create a custom trainer that incorporates the teacher's hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom trainer for feature-based distillation\n",
    "class FeatureDistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_hidden_states=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_hidden_states = teacher_hidden_states\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Get the batch index\n",
    "        if hasattr(self, \"_current_batch_idx\"):\n",
    "            batch_idx = self._current_batch_idx\n",
    "        else:\n",
    "            batch_idx = 0\n",
    "            self._current_batch_idx = 0\n",
    "        \n",
    "        # Get teacher hidden states for this batch\n",
    "        if self.teacher_hidden_states and batch_idx < len(self.teacher_hidden_states):\n",
    "            teacher_hidden = self.teacher_hidden_states[batch_idx]\n",
    "            teacher_hidden = [h.to(device) for h in teacher_hidden]\n",
    "        else:\n",
    "            teacher_hidden = None\n",
    "        \n",
    "        # Increment batch index for next call\n",
    "        self._current_batch_idx = (batch_idx + 1) % len(self.teacher_hidden_states) if self.teacher_hidden_states else 0\n",
    "        \n",
    "        # Forward pass with feature distillation\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs.get(\"attention_mask\"),\n",
    "            labels=inputs.get(\"labels\"),\n",
    "            teacher_hidden_states=teacher_hidden\n",
    "        )\n",
    "        \n",
    "        loss = outputs[\"loss\"]\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training with Feature-Based Distillation\n",
    "\n",
    "Now, let's train the student model using feature-based distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the student model in our feature distillation model\n",
    "feature_distillation_model = FeatureDistillationModel(student_model)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/gpt2-feature-distilled\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,  # Small batch size for demonstration\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Create the feature distillation trainer\n",
    "feature_trainer = FeatureDistillationTrainer(\n",
    "    model=feature_distillation_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    teacher_hidden_states=teacher_hidden_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with feature distillation\n",
    "# Note: In practice, you would use a larger dataset and more epochs\n",
    "feature_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating the Distilled Model\n",
    "\n",
    "Let's compare the teacher and student models on text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text with a model\n",
    "def generate_text(model, tokenizer, prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"The most effective way to combat climate change is\",\n",
    "    \"When considering the ethics of technology, it's important to\"\n",
    "]\n",
    "\n",
    "# Compare teacher and student outputs\n",
    "for prompt in test_prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    \n",
    "    # Teacher output\n",
    "    teacher_output = generate_text(teacher_model, tokenizer, prompt)\n",
    "    print(f\"\\nTeacher:\\n{teacher_output}\")\n",
    "    \n",
    "    # Student output\n",
    "    student_output = generate_text(student_model, tokenizer, prompt)\n",
    "    print(f\"\\nStudent:\\n{student_output}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Measuring Efficiency Gains\n",
    "\n",
    "Let's measure the inference speed and memory usage of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Function to measure inference time\n",
    "def measure_inference_time(model, tokenizer, prompt, num_runs=10):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Warm-up run\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(inputs.input_ids, max_length=50)\n",
    "    \n",
    "    # Timed runs\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            _ = model.generate(inputs.input_ids, max_length=50)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / num_runs\n",
    "    return avg_time\n",
    "\n",
    "# Measure inference time for both models\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "teacher_time = measure_inference_time(teacher_model, tokenizer, prompt)\n",
    "student_time = measure_inference_time(student_model, tokenizer, prompt)\n",
    "\n",
    "print(f\"Teacher inference time: {teacher_time:.4f} seconds\")\n",
    "print(f\"Student inference time: {student_time:.4f} seconds\")\n",
    "print(f\"Speedup: {teacher_time / student_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure memory usage\n",
    "def measure_memory_usage(model, tokenizer, prompt):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            _ = model.generate(inputs.input_ids, max_length=50)\n",
    "        \n",
    "        memory_usage = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "        return memory_usage\n",
    "    else:\n",
    "        return \"N/A (CUDA not available)\"\n",
    "\n",
    "# Measure memory usage for both models\n",
    "if torch.cuda.is_available():\n",
    "    teacher_memory = measure_memory_usage(teacher_model, tokenizer, prompt)\n",
    "    student_memory = measure_memory_usage(student_model, tokenizer, prompt)\n",
    "    \n",
    "    print(f\"Teacher memory usage: {teacher_memory:.2f} MB\")\n",
    "    print(f\"Student memory usage: {student_memory:.2f} MB\")\n",
    "    print(f\"Memory reduction: {teacher_memory / student_memory:.2f}x\")\n",
    "else:\n",
    "    print(\"Memory usage measurement requires CUDA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored advanced knowledge distillation techniques for language models:\n",
    "\n",
    "1. **Feature-Based Distillation**: We implemented distillation based on the internal representations (hidden states) of the teacher model, not just the final outputs.\n",
    "\n",
    "2. **Synthetic Data Generation**: We used the teacher model to generate synthetic data for distillation, which can be particularly useful when labeled data is scarce.\n",
    "\n",
    "3. **Efficiency Evaluation**: We measured the inference speed and memory usage improvements achieved through distillation.\n",
    "\n",
    "The distilled student model is significantly smaller and faster than the teacher model while maintaining much of its generation capability. This demonstrates the power of knowledge distillation for creating efficient models that can be deployed in resource-constrained environments.\n",
    "\n",
    "In practice, you would want to:\n",
    "- Use a larger synthetic dataset\n",
    "- Train for more epochs\n",
    "- Experiment with different distillation objectives and hyperparameters\n",
    "- Evaluate on standardized benchmarks\n",
    "\n",
    "Knowledge distillation is an active area of research, and new techniques are constantly being developed to improve the efficiency and performance of language models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
