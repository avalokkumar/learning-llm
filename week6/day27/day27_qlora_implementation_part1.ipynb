{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 27: QLoRA Implementation - Part 1\n",
    "\n",
    "In this notebook, we'll implement QLoRA (Quantized Low-Rank Adaptation) to fine-tune a large language model on consumer hardware. We'll focus on the setup, quantization, and basic QLoRA configuration.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Setup and dependencies\n",
    "2. Loading and quantizing a pre-trained model\n",
    "3. Configuring QLoRA adapters\n",
    "4. Preparing a dataset for fine-tuning\n",
    "5. Setting up the training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, let's install the necessary libraries. We'll need `bitsandbytes` for quantization, `peft` for LoRA, and `transformers` for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft evaluate accelerate bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging\n",
    ")\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel,\n",
    "    TaskType\n",
    ")\n",
    "import bitsandbytes as bnb\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Set logging level\n",
    "logging.set_verbosity_info()\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and Quantizing a Pre-trained Model\n",
    "\n",
    "We'll use a smaller model for demonstration purposes, but the same principles apply to larger models like Llama 2 or Falcon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model name\n",
    "model_name = \"facebook/opt-1.3b\"  # Using OPT-1.3B as an example\n",
    "\n",
    "# Configure quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,              # Load model in 4-bit precision\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Compute in fp16\n",
    "    bnb_4bit_use_double_quant=True,  # Use double quantization\n",
    "    bnb_4bit_quant_type=\"nf4\"        # Use NF4 quantization\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "\n",
    "# Load the model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",  # Automatically determine device mapping\n",
    ")\n",
    "\n",
    "# Print model size information\n",
    "def print_model_size(model):\n",
    "    \"\"\"Print model size information\"\"\"\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    size_mb = (param_size + buffer_size) / 1024**2\n",
    "    print(f\"Model size: {size_mb:.2f} MB\")\n",
    "    \n",
    "    # Count parameters by dtype\n",
    "    dtypes = {}\n",
    "    for param in model.parameters():\n",
    "        dtype = param.dtype\n",
    "        if dtype not in dtypes:\n",
    "            dtypes[dtype] = 0\n",
    "        dtypes[dtype] += param.nelement()\n",
    "    \n",
    "    for dtype, count in dtypes.items():\n",
    "        print(f\"{dtype}: {count:,} parameters\")\n",
    "\n",
    "print_model_size(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuring QLoRA Adapters\n",
    "\n",
    "Now, let's prepare the model for QLoRA fine-tuning by adding LoRA adapters to the quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                    # Rank of the update matrices\n",
    "    lora_alpha=32,           # Alpha parameter for scaling\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Target attention modules\n",
    "    lora_dropout=0.05,       # Dropout probability for LoRA layers\n",
    "    bias=\"none\",             # Don't train bias parameters\n",
    "    task_type=TaskType.CAUSAL_LM  # Task type (causal language modeling)\n",
    ")\n",
    "\n",
    "# Create the PEFT model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"Prints the number of trainable parameters in the model.\"\"\"\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params:,} || all params: {all_params:,} || \"\n",
    "        f\"trainable%: {100 * trainable_params / all_params:.2f}%\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing a Dataset for Fine-tuning\n",
    "\n",
    "We'll use a subset of the Alpaca dataset for instruction fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Alpaca dataset\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
    "print(dataset)\n",
    "\n",
    "# Look at a few examples\n",
    "for i in range(3):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Instruction: {dataset['train'][i]['instruction']}\")\n",
    "    print(f\"Input: {dataset['train'][i]['input']}\")\n",
    "    print(f\"Output: {dataset['train'][i]['output']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the dataset for instruction fine-tuning\n",
    "def format_instruction(example):\n",
    "    \"\"\"Format the instruction, input, and output into a single text.\"\"\"\n",
    "    instruction = example[\"instruction\"]\n",
    "    input_text = example[\"input\"]\n",
    "    output = example[\"output\"]\n",
    "    \n",
    "    if input_text:\n",
    "        formatted_text = f\"### Instruction: {instruction}\\n\\n### Input: {input_text}\\n\\n### Response: {output}\"\n",
    "    else:\n",
    "        formatted_text = f\"### Instruction: {instruction}\\n\\n### Response: {output}\"\n",
    "    \n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "# Apply the formatting function\n",
    "formatted_dataset = dataset.map(format_instruction, remove_columns=[\"instruction\", \"input\", \"output\"])\n",
    "\n",
    "# Create a smaller dataset for demonstration\n",
    "train_dataset = formatted_dataset[\"train\"].select(range(1000))  # Use 1000 examples for training\n",
    "\n",
    "# Show a formatted example\n",
    "print(train_dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setting Up the Training Pipeline\n",
    "\n",
    "We'll use the `SFTTrainer` from the TRL library to simplify the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/opt-qlora-alpaca\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=500,\n",
    "    warmup_steps=50,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    optim=\"paged_adamw_8bit\",  # Use 8-bit optimizer to save memory\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    "    report_to=\"none\",  # Disable wandb, tensorboard, etc.\n",
    ")\n",
    "\n",
    "# Create the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=512,\n",
    "    packing=True,  # Pack multiple examples into one sequence\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Memory Optimization Techniques\n",
    "\n",
    "Let's explore the memory optimization techniques used in QLoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if gradient checkpointing is enabled\n",
    "print(f\"Gradient checkpointing enabled: {peft_model.gradient_checkpointing}\")\n",
    "\n",
    "# Check optimizer type\n",
    "print(f\"Optimizer: {training_args.optim}\")\n",
    "\n",
    "# Check quantization settings\n",
    "print(f\"4-bit quantization: {quantization_config.load_in_4bit}\")\n",
    "print(f\"Double quantization: {quantization_config.bnb_4bit_use_double_quant}\")\n",
    "print(f\"Quantization type: {quantization_config.bnb_4bit_quant_type}\")\n",
    "\n",
    "# Check memory usage before training\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training the Model\n",
    "\n",
    "Now, let's start the training process. Note that this will take some time, even with the optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Saving the QLoRA Adapter\n",
    "\n",
    "After training, we'll save the LoRA adapter weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapter weights\n",
    "peft_model_path = \"./qlora-opt-alpaca\"\n",
    "peft_model.save_pretrained(peft_model_path)\n",
    "\n",
    "print(f\"QLoRA adapter saved to {peft_model_path}\")\n",
    "\n",
    "# Check the size of the saved adapter\n",
    "!du -sh {peft_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've implemented QLoRA to fine-tune a language model with 4-bit quantization. We've seen how to:\n",
    "\n",
    "1. Load a model with 4-bit quantization using BitsAndBytes\n",
    "2. Configure LoRA adapters for the quantized model\n",
    "3. Prepare a dataset for instruction fine-tuning\n",
    "4. Set up memory-efficient training with gradient checkpointing and 8-bit optimizers\n",
    "5. Train and save the QLoRA adapter\n",
    "\n",
    "This approach allows us to fine-tune large language models on consumer hardware that would otherwise be impossible to train. In Part 2, we'll explore how to use the fine-tuned model for inference and evaluate its performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
