{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 27: QLoRA Implementation - Part 2\n",
    "\n",
    "In this notebook, we'll focus on using our QLoRA-trained model for inference and evaluating its performance. We'll also explore advanced techniques for memory optimization and model deployment.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Loading the QLoRA adapter and base model\n",
    "2. Inference with the fine-tuned model\n",
    "3. Evaluating model performance\n",
    "4. Advanced memory optimization techniques\n",
    "5. Merging adapters for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from peft import PeftModel, PeftConfig\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the QLoRA Adapter and Base Model\n",
    "\n",
    "Let's load our fine-tuned QLoRA adapter and the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the saved adapter\n",
    "peft_model_path = \"./qlora-opt-alpaca\"\n",
    "\n",
    "# Load the PEFT configuration\n",
    "config = PeftConfig.from_pretrained(peft_model_path)\n",
    "print(f\"Base model: {config.base_model_name_or_path}\")\n",
    "\n",
    "# Configure quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# Load the base model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the LoRA adapter\n",
    "model = PeftModel.from_pretrained(model, peft_model_path)\n",
    "\n",
    "print(\"Model and adapter loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference with the Fine-tuned Model\n",
    "\n",
    "Now, let's use our fine-tuned model to generate responses to instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "# Function to generate responses\n",
    "def generate_response(instruction, input_text=None):\n",
    "    # Format the prompt\n",
    "    if input_text:\n",
    "        prompt = f\"### Instruction: {instruction}\\n\\n### Input: {input_text}\\n\\n### Response:\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction: {instruction}\\n\\n### Response:\"\n",
    "    \n",
    "    # Generate the response\n",
    "    result = generator(prompt, max_new_tokens=256)[0][\"generated_text\"]\n",
    "    \n",
    "    # Extract just the response part\n",
    "    response = result.split(\"### Response:\")[-1].strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with some instructions\n",
    "test_instructions = [\n",
    "    \"Write a short poem about artificial intelligence.\",\n",
    "    \"Explain the concept of quantum computing to a 10-year-old.\",\n",
    "    \"List five ways to reduce carbon emissions in daily life.\"\n",
    "]\n",
    "\n",
    "for instruction in test_instructions:\n",
    "    print(f\"Instruction: {instruction}\")\n",
    "    response = generate_response(instruction)\n",
    "    print(f\"Response:\\n{response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating Model Performance\n",
    "\n",
    "Let's evaluate our model on a few examples from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a small evaluation dataset\n",
    "eval_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[1000:1020]\")  # Just 20 examples for demonstration\n",
    "\n",
    "# Evaluate on a few examples\n",
    "for i in range(5):  # Evaluate on 5 examples\n",
    "    example = eval_dataset[i]\n",
    "    instruction = example[\"instruction\"]\n",
    "    input_text = example[\"input\"]\n",
    "    reference_output = example[\"output\"]\n",
    "    \n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Instruction: {instruction}\")\n",
    "    if input_text:\n",
    "        print(f\"Input: {input_text}\")\n",
    "    print(f\"Reference: {reference_output}\")\n",
    "    \n",
    "    # Generate response\n",
    "    model_output = generate_response(instruction, input_text)\n",
    "    print(f\"Model Output: {model_output}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Memory Optimization Techniques\n",
    "\n",
    "Let's explore some advanced memory optimization techniques for working with large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check GPU memory usage\n",
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "        print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "        print(f\"Max GPU memory allocated: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "# Check current memory usage\n",
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 CPU Offloading\n",
    "\n",
    "For extremely large models, we can offload some layers to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This is just a demonstration of the concept\n",
    "# In practice, you would use device_map to specify which layers go where\n",
    "\n",
    "# Example of device map for CPU offloading\n",
    "device_map_example = {\n",
    "    \"model.embed_tokens\": \"cpu\",\n",
    "    \"model.layers.0\": \"cuda:0\",\n",
    "    \"model.layers.1\": \"cuda:0\",\n",
    "    # ... more layers\n",
    "    \"model.layers.23\": \"cpu\",\n",
    "    \"model.norm\": \"cuda:0\",\n",
    "    \"lm_head\": \"cuda:0\"\n",
    "}\n",
    "\n",
    "print(\"Example device map for CPU offloading:\")\n",
    "for key, value in list(device_map_example.items())[:5]:\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"... (more layers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Flash Attention\n",
    "\n",
    "Flash Attention is a memory-efficient attention implementation that can significantly reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This is just a demonstration of the concept\n",
    "# In practice, you would use models that support Flash Attention\n",
    "\n",
    "print(\"Flash Attention benefits:\")\n",
    "print(\"1. O(n) memory complexity instead of O(nÂ²)\")\n",
    "print(\"2. Faster computation on GPUs\")\n",
    "print(\"3. Enables processing of longer sequences\")\n",
    "print(\"\\nTo use Flash Attention:\")\n",
    "print(\"model = AutoModelForCausalLM.from_pretrained('model_name', use_flash_attention=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Merging Adapters for Deployment\n",
    "\n",
    "For deployment, we can merge the LoRA adapter with the base model to eliminate the adapter overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the adapter with the base model\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save the merged model (optional - this would be a large file)\n",
    "# merged_model.save_pretrained(\"./merged-qlora-opt-alpaca\")\n",
    "\n",
    "# Test the merged model\n",
    "instruction = \"Explain the difference between machine learning and deep learning.\"\n",
    "print(f\"Instruction: {instruction}\")\n",
    "\n",
    "# Create a new pipeline with the merged model\n",
    "merged_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=merged_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# Generate response\n",
    "prompt = f\"### Instruction: {instruction}\\n\\n### Response:\"\n",
    "result = merged_generator(prompt, max_new_tokens=256)[0][\"generated_text\"]\n",
    "response = result.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Converting to Different Quantization Formats for Inference\n",
    "\n",
    "For deployment, we might want to convert the model to a different quantization format optimized for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This is just a demonstration of the concept\n",
    "# In practice, you would use libraries like ONNX or TensorRT for optimized inference\n",
    "\n",
    "print(\"Inference Optimization Options:\")\n",
    "print(\"1. ONNX Runtime: Convert model to ONNX format for optimized inference\")\n",
    "print(\"2. TensorRT: NVIDIA's deep learning inference optimizer\")\n",
    "print(\"3. INT8 Quantization: Further quantize to 8-bit integers for inference\")\n",
    "print(\"4. Model Pruning: Remove less important weights\")\n",
    "print(\"5. Knowledge Distillation: Train a smaller model to mimic the larger one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored how to use a QLoRA-trained model for inference and evaluated its performance. We've also discussed advanced memory optimization techniques and deployment strategies.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. QLoRA enables fine-tuning of large language models on consumer hardware\n",
    "2. The fine-tuned model can generate high-quality responses to instructions\n",
    "3. Advanced memory optimization techniques like CPU offloading and Flash Attention can further reduce memory requirements\n",
    "4. For deployment, adapters can be merged with the base model to eliminate overhead\n",
    "5. Additional optimizations like ONNX conversion or further quantization can improve inference performance\n",
    "\n",
    "QLoRA represents a significant advancement in democratizing access to large language model fine-tuning, making it possible for researchers and developers with limited resources to adapt state-of-the-art models to their specific needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
